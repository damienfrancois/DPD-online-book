{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Defensive Programming and Debugging This online book is a companion to the MOOC with the same name.","title":"Welcome to Defensive Programming and Debugging"},{"location":"#welcome-to-defensive-programming-and-debugging","text":"This online book is a companion to the MOOC with the same name.","title":"Welcome to Defensive Programming and Debugging"},{"location":"Appendices/getting_help_from_dev_env/","text":"Getting help from your development environment Although syntax errors are not really bugs, they are a nuisance nevertheless. For compiled languages such as Fortran, C and C++, you'd have to compile the code to spot the issues. Integrated development environments such as Eclipse can be a great help in this respect. They will perform a syntactic analysis in the background as you type, and warn you about syntax errors early, saving you the time to build your software. However, for those of you who prefer to keep things simple and use vim for software development, there are a number of plugins you can benefit from. Managing vim plugins is quite straightforward using vundle, and below you'll find my personal setup. \" setup for Vundle vim plugin manager set nocompatible \" required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'gmarik/Vundle.vim' \" add all your plugins here (note older versions of Vundle \" used Bundle instead of Plugin) Plugin 'vim-scripts/indentpython.vim' Plugin 'tmhedberg/SimpylFold' Plugin 'vim-syntastic/syntastic' Plugin 'nvie/vim-flake8' Plugin 'jnurmine/Zenburn' Plugin 'tpope/vim-surround' Plugin 'tpope/vim-commentary' \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" setup for Vundle vim plugin manager done When I save a file, its syntax is automatically checked, warnings, and errors are added as annotations. This often saves time by not having to build or run the software to catch mistakes. For interpreted languages such as Python a static code analysis is performed, and although this will not catch all errors, it is still worth the setup. A point to mention about your development environment: use a font for your editor that makes code easy to read. Some fonts make it hard to distinguish between '0' and 'O', or '1' and 'l', so picking a good one will save time and effort. Nice examples are Source Code Pro developed by Adobe, and Inconsolata created by Ralf Levien.","title":"Getting help from your development environment"},{"location":"Appendices/getting_help_from_dev_env/#getting-help-from-your-development-environment","text":"Although syntax errors are not really bugs, they are a nuisance nevertheless. For compiled languages such as Fortran, C and C++, you'd have to compile the code to spot the issues. Integrated development environments such as Eclipse can be a great help in this respect. They will perform a syntactic analysis in the background as you type, and warn you about syntax errors early, saving you the time to build your software. However, for those of you who prefer to keep things simple and use vim for software development, there are a number of plugins you can benefit from. Managing vim plugins is quite straightforward using vundle, and below you'll find my personal setup. \" setup for Vundle vim plugin manager set nocompatible \" required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'gmarik/Vundle.vim' \" add all your plugins here (note older versions of Vundle \" used Bundle instead of Plugin) Plugin 'vim-scripts/indentpython.vim' Plugin 'tmhedberg/SimpylFold' Plugin 'vim-syntastic/syntastic' Plugin 'nvie/vim-flake8' Plugin 'jnurmine/Zenburn' Plugin 'tpope/vim-surround' Plugin 'tpope/vim-commentary' \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" setup for Vundle vim plugin manager done When I save a file, its syntax is automatically checked, warnings, and errors are added as annotations. This often saves time by not having to build or run the software to catch mistakes. For interpreted languages such as Python a static code analysis is performed, and although this will not catch all errors, it is still worth the setup. A point to mention about your development environment: use a font for your editor that makes code easy to read. Some fonts make it hard to distinguish between '0' and 'O', or '1' and 'l', so picking a good one will save time and effort. Nice examples are Source Code Pro developed by Adobe, and Inconsolata created by Ralf Levien.","title":"Getting help from your development environment"},{"location":"Appendices/rosetta_stone/","text":"Rosetta Stone for programming languages This MOOC doesn't concentrate on a single programming language, but targets C, C++ and Fortran programmers. The terminology for similar concepts in those programming languages is unfortunately somewhat different, so this section tries to be a Rosetta Stone for this terminology. functions In Fortran, we distinguish between functions and subroutines, which are collectively called procedures. Functions have a return value, while subroutines rely on side effects by modifying the arguments passed to the subroutine. The following code fragment defines a function: FUNCTION maximum(array) IMPLICIT none REAL, DIMENSION(:), INTENT(IN) :: array INTEGER :: i maximum = array(1) DO i = 2, SIZE(array) IF (maximum < array(i)) & maximum = array(i) END DO END FUNCTION maximum The following subroutine would have similar functionality: SUBROUTINE maximum(array, maxi) IMPLICIT none REAL, DIMENSION(:), INTENT(IN) :: array REAL, INTENT(OUT) :: maxi INTEGER :: i maxi = array(1) DO i = 2, SIZE(array) IF (maximum < array(i)) & maximum = array(i) END DO END SUBROUTINE maximum Note that you should use Fortran's intrinsic function MAX , rather than roll your own. C and C++ only have functions, although you could view a function with return type void as the equivalent of a Fortran subroutine. A difference in terminology is that the values passed to a function in C and C++ are often referred to as parameters, rather than arguments. User-defined data type In Fortran, we call this a derived data type, and the data fields are components. TYPE, PUBLIC :: stats_type REAL :: sum = 0.0_f8 INTEGER :: n = 0_i8 END TYPE stats_type In the code fragment above, stats_type is the name of the derived data type, while sum and n are its two components. In C, there are multiple user defined types, but the one of interest here is the struct , which is rougly equivalent to the Fortran derived data type. Its data fields are called members. struct { double sum; int n; } stats_type; So here struct stats_type is the name of the C structure, and it has sum and n as members. For C++, the difference would be that the name of the structure is simply stats_type , so the struct keyword isn't required.","title":"Rosetta stone"},{"location":"Appendices/rosetta_stone/#rosetta-stone-for-programming-languages","text":"This MOOC doesn't concentrate on a single programming language, but targets C, C++ and Fortran programmers. The terminology for similar concepts in those programming languages is unfortunately somewhat different, so this section tries to be a Rosetta Stone for this terminology.","title":"Rosetta Stone for programming languages"},{"location":"Appendices/rosetta_stone/#functions","text":"In Fortran, we distinguish between functions and subroutines, which are collectively called procedures. Functions have a return value, while subroutines rely on side effects by modifying the arguments passed to the subroutine. The following code fragment defines a function: FUNCTION maximum(array) IMPLICIT none REAL, DIMENSION(:), INTENT(IN) :: array INTEGER :: i maximum = array(1) DO i = 2, SIZE(array) IF (maximum < array(i)) & maximum = array(i) END DO END FUNCTION maximum The following subroutine would have similar functionality: SUBROUTINE maximum(array, maxi) IMPLICIT none REAL, DIMENSION(:), INTENT(IN) :: array REAL, INTENT(OUT) :: maxi INTEGER :: i maxi = array(1) DO i = 2, SIZE(array) IF (maximum < array(i)) & maximum = array(i) END DO END SUBROUTINE maximum Note that you should use Fortran's intrinsic function MAX , rather than roll your own. C and C++ only have functions, although you could view a function with return type void as the equivalent of a Fortran subroutine. A difference in terminology is that the values passed to a function in C and C++ are often referred to as parameters, rather than arguments.","title":"functions"},{"location":"Appendices/rosetta_stone/#user-defined-data-type","text":"In Fortran, we call this a derived data type, and the data fields are components. TYPE, PUBLIC :: stats_type REAL :: sum = 0.0_f8 INTEGER :: n = 0_i8 END TYPE stats_type In the code fragment above, stats_type is the name of the derived data type, while sum and n are its two components. In C, there are multiple user defined types, but the one of interest here is the struct , which is rougly equivalent to the Fortran derived data type. Its data fields are called members. struct { double sum; int n; } stats_type; So here struct stats_type is the name of the C structure, and it has sum and n as members. For C++, the difference would be that the name of the structure is simply stats_type , so the struct keyword isn't required.","title":"User-defined data type"},{"location":"Appendices/syntax_vs_semantics/","text":"Syntax versus semantics \"Syntax\" and \"semantics\" are two terms we will use often in this course. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. PROGRAM syntax_error IMPLICIT none USE, INTRINSIC :: iso_fortran_env, ONLY : output_unit WRITE (UNIT=output_unit, FMT='(A)') 'hello world' END PROGRAM syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~fortran $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: USE, INTRINSIC :: iso_fortran_env, ONLY : output_unit 1 syntax_error.f90:2.17: IMPLICIT none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function defintion has a semantic error, although it is syntactically correct. ~~~~fortran iNTEGER FUNCTION fac(n) IMPLICIT NONE INTEGER, INTENT(IN) :: n INTEGER :: i fac = 1 DO i = 0, n fac = fac*i END DO END FUNCTION fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as we'll see in the next section.","title":"Syntax versus semantics"},{"location":"Appendices/syntax_vs_semantics/#syntax-versus-semantics","text":"\"Syntax\" and \"semantics\" are two terms we will use often in this course. Most of you are probably familiar with these concepts, but just to make sure, let's define them briefly. The syntax of a programming language is its grammar, i.e., the rules the source code text must satisfy in order to be considered syntactically correct. Let's illustrate that with an example from natural language. The sentence \"the dog barks\" is syntactically correct, while \"the dog bark\" is not. Semantics on the other hand has to do with meaning, or interpretation. Again drawing on natural language examples, the sentence \"the dog spoke\" is syntactically correct, but the semantics is wrong. Obviously, dogs do not speak, except in fairy tales. The sentence \"the dog barks\" is both syntactically and semantically correct. The following code fragments would be syntactically incorrect. PROGRAM syntax_error IMPLICIT none USE, INTRINSIC :: iso_fortran_env, ONLY : output_unit WRITE (UNIT=output_unit, FMT='(A)') 'hello world' END PROGRAM syntax_error This Fortran program has a syntax error that is dutifully reported by the gfortran compiler as: ~~~fortran $ gfortran -c syntax_error.f90 syntax_error.f90:3.57: USE, INTRINSIC :: iso_fortran_env, ONLY : output_unit 1 syntax_error.f90:2.17: IMPLICIT none 2 Error: USE statement at (1) cannot follow IMPLICIT NONE statement at (2) The following Fortran function defintion has a semantic error, although it is syntactically correct. ~~~~fortran iNTEGER FUNCTION fac(n) IMPLICIT NONE INTEGER, INTENT(IN) :: n INTEGER :: i fac = 1 DO i = 0, n fac = fac*i END DO END FUNCTION fac The code fragment above will be compiled without errors or even warnings, but the function will return zero when invoked, regardless of the argument passed to it. This illustrates an important point: syntax errors are always caught by the compiler. Although having to fix syntax errors is a nuisance, it is relatively easy. Most semantic errors are not caught by the compiler, so these are harder to spot and more difficult to fix. Compilers do offer some help detecting certain classes of semantic error, as we'll see in the next section.","title":"Syntax versus semantics"},{"location":"BugsAtRuntime/finding_bugs_best_practices/","text":"Debugging best practices The debugging process is very often quite ad-hoc. You don't really know what you are looking for, which makes it hard to follow a systematic approach. However, there is some advice you can follow to help cut down on the time you spend hunting bugs. Reproducibility The most important thing that makes debugging easier is reproducibility. When you cannot reliably reproduce a bug, it will be very hard to find. Scientific code that relies on pseudo-random number generators are a good example of this. Depending on the sequence of random numbers, the bug may or may not be triggered. Hence it is useful to ensure that you seed the pseudo-random number generator systematically with the same seed while developing, and that you test with a range of seed values. Although the latter doesn't guarantee that a lurking bug would be triggered, it at least increases the likelihood a bit. Some memory-related bugs may only be triggered when the memory is tight, so explicitly maxing out the memory consumption of your code to ensure it should run into trouble allows you to systematically test that this situation is dealt with gracefully. When you add test cases that use long arrays sizes or other large data structures, checking for errors you expect will help. When dealing with parallel code, you typically loose determinism, so heisenbugs (now you see it, now you don't) can occur. Typically, there is not much to be done about that. Debugging strategies Trying to solve a problem systematically helps a lot. An aphorism attributed to Albert Einstein states that \"everything should be as simple as it can be, but not simpler\". To simplify the process of identifying a bug, it is good practice to try and reduce the code to the minimum that still exhibits the bug. Typically, it is not necessary to rewrite your code, but rather to construct a test case that reproduces the problem. Once the bug is captured in a reproducible test case, it is easier to find and fix. The test case remains in the code base and serves as a regression test. For longer debug sessions, it may be hard to remember what you've already tried. Many developers will keep track of their debugging efforts by keeping a log. An important point that was already raised when we discussed unit testing is the granularity of your development process. It is more likely that bugs have been introduced in code that was recently added. Hence it is a good idea to add new code in small increments and add tests immediately. This way the amount of code you have to review is typically limited. Of course, a bug may actually be triggered by some code that was written a while ago and that is now used in new ways. A very good way to figure out issues with your code is explaining it to someone else. You can only explain something you truly understand yourself. While thinking about how to explain it, you are forced to another person's point of view, and typically spot problems more easily. When no one is at hand, some developers actually explain their code to a rubber duck. I'm quite certain that can be substituted with your favourite teddy bear, though. Although this may sound somewhat ridiculous, it may actually help you.","title":"Best practices"},{"location":"BugsAtRuntime/finding_bugs_best_practices/#debugging-best-practices","text":"The debugging process is very often quite ad-hoc. You don't really know what you are looking for, which makes it hard to follow a systematic approach. However, there is some advice you can follow to help cut down on the time you spend hunting bugs.","title":"Debugging best practices"},{"location":"BugsAtRuntime/finding_bugs_best_practices/#reproducibility","text":"The most important thing that makes debugging easier is reproducibility. When you cannot reliably reproduce a bug, it will be very hard to find. Scientific code that relies on pseudo-random number generators are a good example of this. Depending on the sequence of random numbers, the bug may or may not be triggered. Hence it is useful to ensure that you seed the pseudo-random number generator systematically with the same seed while developing, and that you test with a range of seed values. Although the latter doesn't guarantee that a lurking bug would be triggered, it at least increases the likelihood a bit. Some memory-related bugs may only be triggered when the memory is tight, so explicitly maxing out the memory consumption of your code to ensure it should run into trouble allows you to systematically test that this situation is dealt with gracefully. When you add test cases that use long arrays sizes or other large data structures, checking for errors you expect will help. When dealing with parallel code, you typically loose determinism, so heisenbugs (now you see it, now you don't) can occur. Typically, there is not much to be done about that.","title":"Reproducibility"},{"location":"BugsAtRuntime/finding_bugs_best_practices/#debugging-strategies","text":"Trying to solve a problem systematically helps a lot. An aphorism attributed to Albert Einstein states that \"everything should be as simple as it can be, but not simpler\". To simplify the process of identifying a bug, it is good practice to try and reduce the code to the minimum that still exhibits the bug. Typically, it is not necessary to rewrite your code, but rather to construct a test case that reproduces the problem. Once the bug is captured in a reproducible test case, it is easier to find and fix. The test case remains in the code base and serves as a regression test. For longer debug sessions, it may be hard to remember what you've already tried. Many developers will keep track of their debugging efforts by keeping a log. An important point that was already raised when we discussed unit testing is the granularity of your development process. It is more likely that bugs have been introduced in code that was recently added. Hence it is a good idea to add new code in small increments and add tests immediately. This way the amount of code you have to review is typically limited. Of course, a bug may actually be triggered by some code that was written a while ago and that is now used in new ways. A very good way to figure out issues with your code is explaining it to someone else. You can only explain something you truly understand yourself. While thinking about how to explain it, you are forced to another person's point of view, and typically spot problems more easily. When no one is at hand, some developers actually explain their code to a rubber duck. I'm quite certain that can be substituted with your favourite teddy bear, though. Although this may sound somewhat ridiculous, it may actually help you.","title":"Debugging strategies"},{"location":"BugsAtRuntime/finding_bugs_intro/","text":"Introduction to finding bugs Even while you maintain a clean coding style and practices, and use the compiler's abilities to catch mistakes, you'll still have to find and fix bugs. Programming is not easy, often algorithms and data structures are quite sophisticated, and the application fails for edge or corner cases that were not taken into account. The semantics of a programming language can be quite subtle at times, so that the code may do things we don't expect. Whatever the cause, bugs will be present, and given the somewhat depressing statistics on the time the average developer spends on that, we would better go about it as efficiently as possible. Many of us would start inserting statements to write information about the program state to the screen to get a handle on what is going on, and identify the point in the code where variables are assigned suspicious values. When done systematically, this approach will work. However, when searching for an elusive bug, this will require quite a number of extra lines of code, which we subsequently have to clean up again, potentially introducing a new bug. Debuggers are designed to help us searching for bugs in a much more efficient way, without requiring code modifications. They allow us to monitor the execution of an application step by step if we need to, keeping track of changes to the values of variables. They allow us to run an application, but automatically halt executing based on a Boolean condition or access of a variable. In this section, we will familiarize ourselves with the GDB debugger and its more sophisticated features. While doing so, we will discover some debugging techniques that are general, so that we can apply them when using other debugging tools as well. A second debugging application we will discuss is Valgrind , essentially a collection of tools to identify issues related to using memory inappropriately. Everyone is familiar with the dreaded segmentation fault crash, and knows that bugs causing this \"unpleasantness\" are notoriously hard to identify. Valgrind can be a great help in such situations, as well as in identifying the source of memory leaks. In this section, we will deal with serial code, while the next will be devoted to tools for debugging parallel code.","title":"Introduction"},{"location":"BugsAtRuntime/finding_bugs_intro/#introduction-to-finding-bugs","text":"Even while you maintain a clean coding style and practices, and use the compiler's abilities to catch mistakes, you'll still have to find and fix bugs. Programming is not easy, often algorithms and data structures are quite sophisticated, and the application fails for edge or corner cases that were not taken into account. The semantics of a programming language can be quite subtle at times, so that the code may do things we don't expect. Whatever the cause, bugs will be present, and given the somewhat depressing statistics on the time the average developer spends on that, we would better go about it as efficiently as possible. Many of us would start inserting statements to write information about the program state to the screen to get a handle on what is going on, and identify the point in the code where variables are assigned suspicious values. When done systematically, this approach will work. However, when searching for an elusive bug, this will require quite a number of extra lines of code, which we subsequently have to clean up again, potentially introducing a new bug. Debuggers are designed to help us searching for bugs in a much more efficient way, without requiring code modifications. They allow us to monitor the execution of an application step by step if we need to, keeping track of changes to the values of variables. They allow us to run an application, but automatically halt executing based on a Boolean condition or access of a variable. In this section, we will familiarize ourselves with the GDB debugger and its more sophisticated features. While doing so, we will discover some debugging techniques that are general, so that we can apply them when using other debugging tools as well. A second debugging application we will discuss is Valgrind , essentially a collection of tools to identify issues related to using memory inappropriately. Everyone is familiar with the dreaded segmentation fault crash, and knows that bugs causing this \"unpleasantness\" are notoriously hard to identify. Valgrind can be a great help in such situations, as well as in identifying the source of memory leaks. In this section, we will deal with serial code, while the next will be devoted to tools for debugging parallel code.","title":"Introduction to finding bugs"},{"location":"BugsAtRuntime/heisenbugs/","text":"Are bugs deterministic? Thanks to the hype about quantum computing, chances are that you are familiar with the German theoretical physicist Werner Heisenberg (1901-1976). He was one of the pioneers of quantum physics and formulated the uncertainty principle that is named after him. However, one of his key insights was that the state of a system can be altered by observing it. Hence the term 'Heisenbug' was coined to refer to bugs that seem to vanish once you start to debug. Often, to allow for effective debugging, the applications are modified. For instance, you would probably be compiling with optimisation disabled ( -O0 ) rather than the default -O2 optimisation level. You may be inserting print statements (don't, use a debugger) to try and see what is going on. All these small changes can in fact suppress the symptoms of the bug, making debugging all the more \"interesting\". There are a few other categories of bugs that are named after scientists or events, but those names haven't really caught on. bohrbug: a good, old fashioned, reliable bug (like Niels Bohr 's model for the atom: simple, deterministic); mandelbug: when searching for such a bug, you discover more and more bugs (like a Mandelbrot fractal); it is also used for bugs that appear to be non-deterministic; schroedinbug: a bug that only manifests itself after the programmer noticed that it should be there (like Schroedinger 's cat thought experiment); hindenbug: a bug with truly catastrophic consequences (Like the Hindenburg zeppelin disaster ); higgs-bugson: a bug you know is likely there because of unconfirmed user reports but that you can't pin down (like the Higgs boson until its existence was finally confirmed recently). Given that these terms are fun, but not widely known and used, I'll use the term Heisenbug somewhat loosely to also include mandelbugs. A scary example of a mandelbug occurs in the following code: #include <err.h> #include <stdio.h> #include <stdlib.h> static double __arg__; #define SQR(a) ((__arg__ = (a)) == 0.0 ? 0.0 : __arg__*__arg__) int main(int argc, char *argv[]) { double *a, *b, *c, sum; int n = 1000000, i; if ((a = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate a[%d]\", n); if ((b = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate b[%d]\", n); if ((c = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate c[%d]\", n); sum = 0.0; for (i = 0; i < n; i++) { b[i] = (rand() % 2) ? 0.01*i : 0.0; c[i] = (rand() % 2) ? 0.03*i : 0.0; sum += b[i]*b[i] + c[i]*c[i]; } printf(\"sum = %.2lf\\n\", sum); for (i = 0; i < n; i++) a[i] = SQR(b[i]) + SQR(c[i]); sum = 0.0; for (i = 0; i < n; i++) sum += a[i]; printf(\"sum = %.2lf\\n\", sum); free(a); free(b); free(c); return EXIT_SUCCESS; } When you compile this using GCC (7.x, but any other I'm aware of) this application will produce the correct result, i.e., $ ./a.out sum = 166581086880546.25 sum = 166581086880546.25 However, compiling the same code using the Intel compiler (upto 18.x) will produce an application that prints the following output: $ ./a.out sum = 166581086880546.25 sum = 224904286352835.62 The output is very clearly wrong. It will be an exercise for you to figure out what the problem might be.","title":"Reproducibility & Heisenbugsi"},{"location":"BugsAtRuntime/heisenbugs/#are-bugs-deterministic","text":"Thanks to the hype about quantum computing, chances are that you are familiar with the German theoretical physicist Werner Heisenberg (1901-1976). He was one of the pioneers of quantum physics and formulated the uncertainty principle that is named after him. However, one of his key insights was that the state of a system can be altered by observing it. Hence the term 'Heisenbug' was coined to refer to bugs that seem to vanish once you start to debug. Often, to allow for effective debugging, the applications are modified. For instance, you would probably be compiling with optimisation disabled ( -O0 ) rather than the default -O2 optimisation level. You may be inserting print statements (don't, use a debugger) to try and see what is going on. All these small changes can in fact suppress the symptoms of the bug, making debugging all the more \"interesting\". There are a few other categories of bugs that are named after scientists or events, but those names haven't really caught on. bohrbug: a good, old fashioned, reliable bug (like Niels Bohr 's model for the atom: simple, deterministic); mandelbug: when searching for such a bug, you discover more and more bugs (like a Mandelbrot fractal); it is also used for bugs that appear to be non-deterministic; schroedinbug: a bug that only manifests itself after the programmer noticed that it should be there (like Schroedinger 's cat thought experiment); hindenbug: a bug with truly catastrophic consequences (Like the Hindenburg zeppelin disaster ); higgs-bugson: a bug you know is likely there because of unconfirmed user reports but that you can't pin down (like the Higgs boson until its existence was finally confirmed recently). Given that these terms are fun, but not widely known and used, I'll use the term Heisenbug somewhat loosely to also include mandelbugs. A scary example of a mandelbug occurs in the following code: #include <err.h> #include <stdio.h> #include <stdlib.h> static double __arg__; #define SQR(a) ((__arg__ = (a)) == 0.0 ? 0.0 : __arg__*__arg__) int main(int argc, char *argv[]) { double *a, *b, *c, sum; int n = 1000000, i; if ((a = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate a[%d]\", n); if ((b = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate b[%d]\", n); if ((c = (double *) malloc(n*sizeof(double))) == NULL) errx(EXIT_FAILURE, \"can't allocate c[%d]\", n); sum = 0.0; for (i = 0; i < n; i++) { b[i] = (rand() % 2) ? 0.01*i : 0.0; c[i] = (rand() % 2) ? 0.03*i : 0.0; sum += b[i]*b[i] + c[i]*c[i]; } printf(\"sum = %.2lf\\n\", sum); for (i = 0; i < n; i++) a[i] = SQR(b[i]) + SQR(c[i]); sum = 0.0; for (i = 0; i < n; i++) sum += a[i]; printf(\"sum = %.2lf\\n\", sum); free(a); free(b); free(c); return EXIT_SUCCESS; } When you compile this using GCC (7.x, but any other I'm aware of) this application will produce the correct result, i.e., $ ./a.out sum = 166581086880546.25 sum = 166581086880546.25 However, compiling the same code using the Intel compiler (upto 18.x) will produce an application that prints the following output: $ ./a.out sum = 166581086880546.25 sum = 224904286352835.62 The output is very clearly wrong. It will be an exercise for you to figure out what the problem might be.","title":"Are bugs deterministic?"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/","text":"Parallel programming best practices Parallel programming is fraught with its own opportunities to introduce bugs. The specification of both OpenMP and MPI are quite clear, but sometimes fairly complex. Misinterpretation or lack of awareness may lead to very interesting and long debugging sessions. PRACE offers excellent courses on the topic which you are strongly encouraged to attend if you didn't do so yet. Often, bugs are introduced as a side effect of code optimisation. Semantics are often quite subtle and it is easy to get things wrong. Therefore it is recommended to start with a non-optimised version of your code, and ensure you have comprehensive tests. Only then proceed to the optimisation of the code. This discussion is limited to best practices to avoid bugs in parallel code. Best practices for performance optimisation is a topic in itself. PRACE organises excellent trainings and workshops on this topic as well. Multithreaded programming For multithreaded programming, data races are a potential issue. It is crucial to ensure that updates to variables by multiple threads are done correctly. OpenMP offers several constructs to support common use cases, e.g., the reduction clause and the critical directive. A common source of errors is inadvertently sharing a variable across threads. To ensure that you have to decide whether a variable should be shared or thread-private, it is good practice to set the default to none ( default(none) clause). The compiler will report errors for each variable that is not explicitly declared either shared or private. However, in C and C++ you can often completely side step this issue by using local variables in the parallel region. Consider the following code fragment int num_threads, thread_num; #pragma omp parallel private(num_threads, thread_num) { num_threads = omp_get_num_threads(); thread_num = omp_get_thread_num(); ... } The version below is simpler, and reduces the scope of the variables to the parallel region. #pragma omp parallel { int num_threads = omp_get_num_threads(); int thread_num = omp_get_thread_num(); ... } In Fortran, you might expect to achieve the same using the block ... end block construct. However, the OpenMP standard is silent on the semantics, so results will be implementation depedent. The Intel Fortran compiler ifort will yield an error notifying you that a block construct is not allowed in an OpenMP block. GCC gfortran on the other hand will happily compile the code, which, at least in situations I tested, seems to do the intuitively right thing. Given this ambiguity, you obviously should not use a block construct in an OpenMP block. Subtle errors may also be introduced by the nowait clause that you would typically introduce to get better performance. A subsequent loop may start iterating on data that is not yet processed by the previous one, leading to data races. Controlling the loop schedule explicitly may resolve such issues. Forgetting a taskwait directive has similar consequences as an ill-advised nowait , it will also lead to race conditions. When using locks, there is the potential of deadlock. If possible, it is preferable to avoid locks altogether. A word of caution: when developing OpenMP applications, ensure that you test it on a system that has at least the number of cores as the number of threads you detect. Race conditions and/or deadlocks may be hidden by thread scheduling done by the operating system if the number of threads exceeds sthe number of cores. MPI Almost all MPI functions in the C API return error values. Similarly, most of the Fortran procedures take an error value argument. It may seem like a good idea to check these values, and take appropriate action. However, in practice that often makes no sense. For instance, if an MPI_Bcast were to fail, the application will crash before the error handling code gets executed. Quoting from the OpenMPI documentation: Almost all MPI routines return an error value; C routines as the value of the function and Fortran routines in the last argument. C++ functions do not return errors. If the default error handler is set to MPI::ERRORS_THROW_EXCEPTIONS , then on error the C++ exception mechanism will be used to throw an MPI::Exception object. Before the error value is returned, the current MPI error handler is called. By default, this error handler aborts the MPI job, except for I/O function errors. The error handler may be changed with MPI_Comm_set_errhandler ; the predefined error handler MPI_ERRORS_RETURN may be used to cause error values to be returned. Note that MPI does not guarantee that an MPI program can continue past an error. So essentially, writing error handling code for MPI is, with the exception of MPI-I/O related calls, counter productive. It increases the length of your code, obscures the logic, and doesn't contribute to its quality. On the topic of error values, as a Fortran programmer you should be aware that when you use the mpi module, you have to include an error value argument for most procedures. Forgetting to do so leads to very interesting, non-local bugs that are hard to track down. The procedure will in that case still write the error value to memory, but to some unspecified location, leading to trouble down the road. A much better alternative is to use the mpi_f08 module. A first advantage is that the error value is an optional argument. Since you will not use the value anyway (except perhaps for MPI-I/O), it is simpler to just leave it out in the procedure call. A second advantage is that many types were introduced, e.g., MPI_Stats , MPI_Request . The procedures are defined using these types, and hence the compiler can catch a lot of errors that would go unnoticed when using the older mpi module. If you accidentally swap two arguments, and they have distinct types, the compiler will yield an error. Given the non-trial procedure signatures, using keyword arguments is very helpful as well. C++ programmers might be happy to notice there is a C++ API for MPI, but don't get excited. It has been deprecated in the MPI-3 standard, and will disappear in some future specification release. For new development, you would be wise to avoid it. Deadlocks When using MPI, there are quite some opportunities to create a deadlock situation. Some are obvious, others more subtle. Among the more obvious causes of a deadlock are tag mismatch in MPI send and receives, source and destination mix-ups in MPI sends and receives, incorrect ordering of MPI send and receive calls between peers. In the first two situations, a process will be stuck in a communication operation because its partner is expecting either a different tag, or another partner to communicate with altogether. The third situation usually occurs when pairs of processes are exchanging information. Both start a, e.g., send operation, which blocks because no corresponding receive is done. Note that this may actually be a heisenbug. Depending on whether the send operation is synchronous or asynchronous, the deadlock may manifest itself or not. For MPI_Send , that can be determined at runtime. There are various approaches to write more robust code in this scenario. You can use non-blocking sends/receives, and wait for completion before reusing/using the buffer. As alternatives, you can use MPI_Sendrecv to exchange information between two processes, or even MPI_Neighbor_alltoall . If not all processes in a communicator participate in a collective operation, the application will also deadlock. When using one-sided communication, care has to be taken with the order of the calls of MPI_Win_post , MPI_Win_start , MPI_Win_complete and MPI_Win_wait . If you have them in an inappropriate order you will get a deadlock as well. Although there are quite a number of creative ways to end up in a deadlock, this is a situation that is easy to detect, although perhaps not trivial to fix. Data races are more subtle. Data races In the context of point-to-point or collective asynchronous communication, using or reusing a buffer before the communication is completed can lead to data races. Most likely, you will need to use MPI_Wait or similar to ensure the asynchronous request is completed. When using one-sided communication, RMA epochs and local load/store epochs should not overlap, so you have to take care to use MPI_Win_fence or MPI_Win_post , MPI_Win_start , MPI_Win_complete and MPI_Win_wait in the appropriate order, and which operations to perform during the resulting epochs. To improve performance, each of those operations can be optimised using assertions on the operations performed in the previous, current and subsequent epochs. However, mistakes will lead to data races. MPI shared memory programming is subject to the same pitfalls that were discussed in the previous section on multithreaded programming with OpenMP. Typically, Fortran compilers do an excellent job on optimising your code. However, sometimes the compiler can go a bit too far, and you have to ensure that certain optimisations are not performed. Consider the following code fragment. call MPI_Irecv(buffer, count, data_type, source, tag, communicator, & request) ... call MPI_Wait(request, status) It should be obvious that the variable buffer can not be used between the calls to MPI_Irecv and MPI_Wait . Its value is only guaranteed to be updated when the call to MPI_Wait is done. Between the two calls, the value of buffer can be the old value, the new value, or even a partially updated value for a non-trivial message. The Fortran compiler may decide to optimise the code by storing the value of buffer in a register after the call to MPI_Irecv . The value of buffer in memory is updated, but after the call to MPI_Wait the value stored in the registry is used, since from the point of view of the compiler there was no way the value of buffer could have changed between the two MPI procedure calls. If that happens, you have a data race on your hands. You can prevent this optimisation by declaring the buffer variable asynchronous if your compiler correctly implements that. Such a compiler is compliant with TS-29113. The MPI variable MPI_ASYNC_PROTECTS_NONBLOCKING will be .true. in that case. The Intel compiler ifort (version 2018 for certain) is TS-29113 compliant, GCC's gfortran (version 7.3) is not. To ensure that even a non-TS-29113 complient compiler does the right thing, MPI-3 defines a procedure MPI_F_sync_reg that only serves to trick the compiler. real, asynchronous :: buffer ... call MPI_Irecv(buffer, count, data_type, source, tag, communicator, & request) ... if (.not. MPI_ASYNC_PROTECTS_NONBLOCKING) & MPI_F_sync_reg(buffer) call MPI_Wait(request, status) Miscellaneous issues For collective communication routines such as MPI_Bcast or MPI_Gather , buffer sizes at the root process and the other processes in the communicator should match. This is required by the MPI standard, and modern MPI implementations will at least warn if this is not the case. Similarly, types for send and receive buffers in communication routines should match. Failure to do so may result in interesting bugs that are hard to track down. Defining, but forgetting to commit an MPI user defined type using MPI_Type_commit is also a potential source of problems. Any resource that has been allocated should be freed as well. Failing to do so may lead to a memory leak. This is fairly obvious for dynamic memory allocation, but when using MPI, there are other resources as well, e.g., user defined types and groups. Those should be freed when no longer required using MPI_Type_free and MPI_Group_free respectively. For both one-sided communication and shared memory programming with MPI remote addresses have to be computed, which of course again leads to excellent opportunities to introduce nasty bugs in your code. Similarly, dynamic memory allocation using, e.g., MPI_Alloc_mem may lead to issues.","title":"Best practices for parallel applications"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#parallel-programming-best-practices","text":"Parallel programming is fraught with its own opportunities to introduce bugs. The specification of both OpenMP and MPI are quite clear, but sometimes fairly complex. Misinterpretation or lack of awareness may lead to very interesting and long debugging sessions. PRACE offers excellent courses on the topic which you are strongly encouraged to attend if you didn't do so yet. Often, bugs are introduced as a side effect of code optimisation. Semantics are often quite subtle and it is easy to get things wrong. Therefore it is recommended to start with a non-optimised version of your code, and ensure you have comprehensive tests. Only then proceed to the optimisation of the code. This discussion is limited to best practices to avoid bugs in parallel code. Best practices for performance optimisation is a topic in itself. PRACE organises excellent trainings and workshops on this topic as well.","title":"Parallel programming best practices"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#multithreaded-programming","text":"For multithreaded programming, data races are a potential issue. It is crucial to ensure that updates to variables by multiple threads are done correctly. OpenMP offers several constructs to support common use cases, e.g., the reduction clause and the critical directive. A common source of errors is inadvertently sharing a variable across threads. To ensure that you have to decide whether a variable should be shared or thread-private, it is good practice to set the default to none ( default(none) clause). The compiler will report errors for each variable that is not explicitly declared either shared or private. However, in C and C++ you can often completely side step this issue by using local variables in the parallel region. Consider the following code fragment int num_threads, thread_num; #pragma omp parallel private(num_threads, thread_num) { num_threads = omp_get_num_threads(); thread_num = omp_get_thread_num(); ... } The version below is simpler, and reduces the scope of the variables to the parallel region. #pragma omp parallel { int num_threads = omp_get_num_threads(); int thread_num = omp_get_thread_num(); ... } In Fortran, you might expect to achieve the same using the block ... end block construct. However, the OpenMP standard is silent on the semantics, so results will be implementation depedent. The Intel Fortran compiler ifort will yield an error notifying you that a block construct is not allowed in an OpenMP block. GCC gfortran on the other hand will happily compile the code, which, at least in situations I tested, seems to do the intuitively right thing. Given this ambiguity, you obviously should not use a block construct in an OpenMP block. Subtle errors may also be introduced by the nowait clause that you would typically introduce to get better performance. A subsequent loop may start iterating on data that is not yet processed by the previous one, leading to data races. Controlling the loop schedule explicitly may resolve such issues. Forgetting a taskwait directive has similar consequences as an ill-advised nowait , it will also lead to race conditions. When using locks, there is the potential of deadlock. If possible, it is preferable to avoid locks altogether. A word of caution: when developing OpenMP applications, ensure that you test it on a system that has at least the number of cores as the number of threads you detect. Race conditions and/or deadlocks may be hidden by thread scheduling done by the operating system if the number of threads exceeds sthe number of cores.","title":"Multithreaded programming"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#mpi","text":"Almost all MPI functions in the C API return error values. Similarly, most of the Fortran procedures take an error value argument. It may seem like a good idea to check these values, and take appropriate action. However, in practice that often makes no sense. For instance, if an MPI_Bcast were to fail, the application will crash before the error handling code gets executed. Quoting from the OpenMPI documentation: Almost all MPI routines return an error value; C routines as the value of the function and Fortran routines in the last argument. C++ functions do not return errors. If the default error handler is set to MPI::ERRORS_THROW_EXCEPTIONS , then on error the C++ exception mechanism will be used to throw an MPI::Exception object. Before the error value is returned, the current MPI error handler is called. By default, this error handler aborts the MPI job, except for I/O function errors. The error handler may be changed with MPI_Comm_set_errhandler ; the predefined error handler MPI_ERRORS_RETURN may be used to cause error values to be returned. Note that MPI does not guarantee that an MPI program can continue past an error. So essentially, writing error handling code for MPI is, with the exception of MPI-I/O related calls, counter productive. It increases the length of your code, obscures the logic, and doesn't contribute to its quality. On the topic of error values, as a Fortran programmer you should be aware that when you use the mpi module, you have to include an error value argument for most procedures. Forgetting to do so leads to very interesting, non-local bugs that are hard to track down. The procedure will in that case still write the error value to memory, but to some unspecified location, leading to trouble down the road. A much better alternative is to use the mpi_f08 module. A first advantage is that the error value is an optional argument. Since you will not use the value anyway (except perhaps for MPI-I/O), it is simpler to just leave it out in the procedure call. A second advantage is that many types were introduced, e.g., MPI_Stats , MPI_Request . The procedures are defined using these types, and hence the compiler can catch a lot of errors that would go unnoticed when using the older mpi module. If you accidentally swap two arguments, and they have distinct types, the compiler will yield an error. Given the non-trial procedure signatures, using keyword arguments is very helpful as well. C++ programmers might be happy to notice there is a C++ API for MPI, but don't get excited. It has been deprecated in the MPI-3 standard, and will disappear in some future specification release. For new development, you would be wise to avoid it.","title":"MPI"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#deadlocks","text":"When using MPI, there are quite some opportunities to create a deadlock situation. Some are obvious, others more subtle. Among the more obvious causes of a deadlock are tag mismatch in MPI send and receives, source and destination mix-ups in MPI sends and receives, incorrect ordering of MPI send and receive calls between peers. In the first two situations, a process will be stuck in a communication operation because its partner is expecting either a different tag, or another partner to communicate with altogether. The third situation usually occurs when pairs of processes are exchanging information. Both start a, e.g., send operation, which blocks because no corresponding receive is done. Note that this may actually be a heisenbug. Depending on whether the send operation is synchronous or asynchronous, the deadlock may manifest itself or not. For MPI_Send , that can be determined at runtime. There are various approaches to write more robust code in this scenario. You can use non-blocking sends/receives, and wait for completion before reusing/using the buffer. As alternatives, you can use MPI_Sendrecv to exchange information between two processes, or even MPI_Neighbor_alltoall . If not all processes in a communicator participate in a collective operation, the application will also deadlock. When using one-sided communication, care has to be taken with the order of the calls of MPI_Win_post , MPI_Win_start , MPI_Win_complete and MPI_Win_wait . If you have them in an inappropriate order you will get a deadlock as well. Although there are quite a number of creative ways to end up in a deadlock, this is a situation that is easy to detect, although perhaps not trivial to fix. Data races are more subtle.","title":"Deadlocks"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#data-races","text":"In the context of point-to-point or collective asynchronous communication, using or reusing a buffer before the communication is completed can lead to data races. Most likely, you will need to use MPI_Wait or similar to ensure the asynchronous request is completed. When using one-sided communication, RMA epochs and local load/store epochs should not overlap, so you have to take care to use MPI_Win_fence or MPI_Win_post , MPI_Win_start , MPI_Win_complete and MPI_Win_wait in the appropriate order, and which operations to perform during the resulting epochs. To improve performance, each of those operations can be optimised using assertions on the operations performed in the previous, current and subsequent epochs. However, mistakes will lead to data races. MPI shared memory programming is subject to the same pitfalls that were discussed in the previous section on multithreaded programming with OpenMP. Typically, Fortran compilers do an excellent job on optimising your code. However, sometimes the compiler can go a bit too far, and you have to ensure that certain optimisations are not performed. Consider the following code fragment. call MPI_Irecv(buffer, count, data_type, source, tag, communicator, & request) ... call MPI_Wait(request, status) It should be obvious that the variable buffer can not be used between the calls to MPI_Irecv and MPI_Wait . Its value is only guaranteed to be updated when the call to MPI_Wait is done. Between the two calls, the value of buffer can be the old value, the new value, or even a partially updated value for a non-trivial message. The Fortran compiler may decide to optimise the code by storing the value of buffer in a register after the call to MPI_Irecv . The value of buffer in memory is updated, but after the call to MPI_Wait the value stored in the registry is used, since from the point of view of the compiler there was no way the value of buffer could have changed between the two MPI procedure calls. If that happens, you have a data race on your hands. You can prevent this optimisation by declaring the buffer variable asynchronous if your compiler correctly implements that. Such a compiler is compliant with TS-29113. The MPI variable MPI_ASYNC_PROTECTS_NONBLOCKING will be .true. in that case. The Intel compiler ifort (version 2018 for certain) is TS-29113 compliant, GCC's gfortran (version 7.3) is not. To ensure that even a non-TS-29113 complient compiler does the right thing, MPI-3 defines a procedure MPI_F_sync_reg that only serves to trick the compiler. real, asynchronous :: buffer ... call MPI_Irecv(buffer, count, data_type, source, tag, communicator, & request) ... if (.not. MPI_ASYNC_PROTECTS_NONBLOCKING) & MPI_F_sync_reg(buffer) call MPI_Wait(request, status)","title":"Data races"},{"location":"BugsAtRuntime/parallel_debugging_best_practices/#miscellaneous-issues","text":"For collective communication routines such as MPI_Bcast or MPI_Gather , buffer sizes at the root process and the other processes in the communicator should match. This is required by the MPI standard, and modern MPI implementations will at least warn if this is not the case. Similarly, types for send and receive buffers in communication routines should match. Failure to do so may result in interesting bugs that are hard to track down. Defining, but forgetting to commit an MPI user defined type using MPI_Type_commit is also a potential source of problems. Any resource that has been allocated should be freed as well. Failing to do so may lead to a memory leak. This is fairly obvious for dynamic memory allocation, but when using MPI, there are other resources as well, e.g., user defined types and groups. Those should be freed when no longer required using MPI_Type_free and MPI_Group_free respectively. For both one-sided communication and shared memory programming with MPI remote addresses have to be computed, which of course again leads to excellent opportunities to introduce nasty bugs in your code. Similarly, dynamic memory allocation using, e.g., MPI_Alloc_mem may lead to issues.","title":"Miscellaneous issues"},{"location":"BugsAtRuntime/tracing/","text":"What is changing? When you observe programmers that are not familiar with debuggers, you'll often see them insert print statement to monitor how the value of a variable changes over time. This pollutes the source code, and you'll certainly forget to take out some print statements when you are done debugging. A debugger offers multiple ways to achieve a similar result without the hassle. Display values Often, you want to inspect a value each time the debugger halts at a breakpoint. This can easily be achieved using the command display . The argument to display is an expression that will be evaluated and displayed each time the application hits a breakpoint. A trivial example would be to simply show the value of a variable, consider, e.g., real :: total integer :: i ... total = 0.0 do i = 0, max_pow total = total + 10.0**i end do When you set a breakpoint in the do-loop, so at the statement that increments total , and run till you hit that breakpoint, you can display the values of i and total . (gdb) display i 1: i = 0 (gdb) display total 2: total = 0 Now when you resume execution and hit a breakpoint, the values of i and total will be displayed, i.e., (gdb) c Continuing. Breakpoint 1, display_demo () at display_demo.f90:9 9 total = total + 10.0**i 1: i = 1 2: total = 1 You can get an overview of all displays in your debug session using info display , i.e., (gdb) i display Auto-display expressions now in effect: Num Enb Expression 1: y i 2: y total As with breakpoints, displays have a numerical identifier that you can use to enable or disable them, delete them, and so on, e.g., (gdb) d display 1 Although this can be useful, GDB offers more powerful options. Commands at breakpoints This feature actually exceeds tracing, it can be used for other purposes as well. The command command allows to specify action to be taken when a breakpoint is hit. Here, you will see how to use it to trace the value of variables as your code executes. First you set a breakpoint, next you add the command to be executed, i.e., (gdb) b 9 Breakpoint 1 at 0x5555555548a3: file display_demo.f90, line 9. (gdb) commands Type commands for breakpoint(s) 2, one per line. End with a line saying just \"end\". >silent >printf \"i = %d, total = %f\\n\", i, total >continue >end The silent command suppresses the usual output that is displayed when the application hits a breakpoint. In this case, you use it because you just want to display variable values as the code executes. The printf command is quite similar to the Bash printf built-in. Its first argument is a formatting string, the following arguments are the expressions to display the values for. Don't forget to add the newline character \\n to get readable output. Some format specifiers you can use are %f : floating point number; %d , %x : signed integer in decimal, and hexadecimal, respectively; %s , %c : string and character, respectively. The continue command ensures that the application will resume execution. When you execute the application, you would see output similar to the following for our running example, (gdb) r Starting program: /home/gjb/Documents/Projects/training-material/Debugging/Gdb/Fortran/Tree/display_demo.exe i = 0, total = 0.000000 i = 1, total = 1.000000 i = 2, total = 11.000000 i = 3, total = 111.000000 i = 4, total = 1111.000000 i = 5, total = 11111.000000 i = 6, total = 111111.000000 i = 7, total = 1111111.000000 i = 8, total = 11111111.000000 i = 9, total = 111111112.000000 i = 10, total = 1111111168.000000 Dynamic print As mentioned, command has more applications than simple tracing, and frankly, it is an overkill for that purpose thanks to GDB's dynamic print command dprintf . As its first argument this command takes a location, at which it should be executed each time that location is reached during the execution. This is convenient, since no breakpoint has to be set. The format string is the same as for the printf command. For our running example, (gdb) dprintf 9, \"i = %d, total = %f\\n\", i, total Dprintf 1 at 0x5555555548a3: file display_demo.f90, line 9. (gdb) r Starting program: /home/gjb/Documents/Projects/training-material/Debugging/Gdb/Fortran/Tree/display_demo.exe i = 0, total = 0.000000 i = 1, total = 1.000000 i = 2, total = 11.000000 i = 3, total = 111.000000 i = 4, total = 1111.000000 i = 5, total = 11111.000000 i = 6, total = 111111.000000 i = 7, total = 1111111.000000 i = 8, total = 11111111.000000 i = 9, total = 111111112.000000 i = 10, total = 1111111168.000000 As you can see, the same output is generated as the one you got by using command , with a lot less effort. Watching for change Although tracing can be useful, it can be fairly cumbersome when not much changes over time. It would be more useful to only display values that change. Consider the following straightforward code sample. integer :: i, j integer, dimension(rows, cols) :: values character(len=32) :: fmt_str ... do j = 1, cols do i = 1, rows values(i, j) = (i - 1)*cols + j end do end do You want to monitor changes on the outer do-loop variable j . When that variable changes, you want to see the value of i , as well as the new and old value of j . This can easily be achieved by combining a number of concepts you already know. We define a GDB variable $current_j to keep track of the current value of the Fortran variable j . We set a conditional breakpoint in the inner do-loop, so that execution will only halt when j is not equal to $current_j , and we attach commands to that breakpoint. (gdb) set $current_j = 0 (gdb) b 10 if j .ne. $current_j Breakpoint 1 at 0x8ef: file watch.f90, line 10. (gdb) commands Type commands for breakpoint(s) 1, one per line. End with a line saying just \"end\". >silent >printf \"i = %d, j = %d, old j = %d\\n\", i, j, $current_j >set $current_j = j >continue >end (gdb) r i = 1, j = 1, old j = 0 i = 1, j = 2, old j = 1 i = 1, j = 3, old j = 2 i = 1, j = 4, old j = 3 Hardware watch points As you saw in the video, hardware watch points can be very useful to pinpoint suspicious memory access patterns. Typically, there are situations when the value of a variable or a data structure is modified, but you don't know where in the code that might happen. Consider the following situation. The application computes the evolution of the temperature of a rectangular flat surface represented as a 2D array. The boundaries of the surface have a constant temperature throughout the computation. The 2D matrix is initialised such that its first and last row, and its first and last column are set to the boundary temperature ( init_temp ). In each time step, the temperature at each interior point of the 2D array is updated by taking into account the temperatures of its neighbours ( update_temp ). In addition, with a given probability, an interior point of the 2D array is set to a random value ( perturb_temp ). real, dimension(x_max, y_max) :: temp real :: boundary, prob integer :: t ... call init_temp(temp, boundary) call show_temp(temp) do t = 1, t_max call update_temp(temp) call perturb_temp(temp, prob) end do call show_temp(temp) When you run the application, you notice that some values on the boundary have changed, and that shouldn't happen according to the design of the program. There must be a bug. This is a toy example, but you can easily imagine a similar, but vastly more complicated setting. You could set breakpoints in the do-loop, but to identify the culprit you have to check for change after each procedure call, which is tedious and error prone. It is much easier and faster to just set a watch point on a boundary, so that the application will halt as soon as that it is modified. So first set a breakpoint after the initialisation of temp is done, run the application, set a watch point, and continue. (gdb) b 9 (gdb) r (gdb) watch temp(1, 1)@x_max Hardware watchpoint 2: temp(1, 1)@x_max (gdb) c Continuing. Hardware watchpoint 2: temp(1, 1)@x_max Old value = (0, 0, 0, 0) New value = (0, 0, 0.101466358, 0) perturb_temp (temp=..., prob=0.699999988) at watch_point.f90:62 62 end if Presto! The boundary is inadvertently modified by the procedure perturb_temp . Now you can concentrate on that procedure to pinpoint the exact problem in the algorithm. Contrary to appearances, he watch command doesn't actually monitor the value of variables, but rather the value stored in the memory the variable refers to. This explains why it is called a \"hardware watch point\". Watch points come in a few flavours: watch : break when the value is modified; rwatch : break when the value is read; awatch : break when the values is read or modified. As you would expect, info watchpoints will list the watch points in your current debug session. Watch points will also be shown when you use info breakpoints , and you can manage them in the exact same way, e.g., disabling/enabling, deleting, and so on.","title":"Tracing applications"},{"location":"BugsAtRuntime/tracing/#what-is-changing","text":"When you observe programmers that are not familiar with debuggers, you'll often see them insert print statement to monitor how the value of a variable changes over time. This pollutes the source code, and you'll certainly forget to take out some print statements when you are done debugging. A debugger offers multiple ways to achieve a similar result without the hassle.","title":"What is changing?"},{"location":"BugsAtRuntime/tracing/#display-values","text":"Often, you want to inspect a value each time the debugger halts at a breakpoint. This can easily be achieved using the command display . The argument to display is an expression that will be evaluated and displayed each time the application hits a breakpoint. A trivial example would be to simply show the value of a variable, consider, e.g., real :: total integer :: i ... total = 0.0 do i = 0, max_pow total = total + 10.0**i end do When you set a breakpoint in the do-loop, so at the statement that increments total , and run till you hit that breakpoint, you can display the values of i and total . (gdb) display i 1: i = 0 (gdb) display total 2: total = 0 Now when you resume execution and hit a breakpoint, the values of i and total will be displayed, i.e., (gdb) c Continuing. Breakpoint 1, display_demo () at display_demo.f90:9 9 total = total + 10.0**i 1: i = 1 2: total = 1 You can get an overview of all displays in your debug session using info display , i.e., (gdb) i display Auto-display expressions now in effect: Num Enb Expression 1: y i 2: y total As with breakpoints, displays have a numerical identifier that you can use to enable or disable them, delete them, and so on, e.g., (gdb) d display 1 Although this can be useful, GDB offers more powerful options.","title":"Display values"},{"location":"BugsAtRuntime/tracing/#commands-at-breakpoints","text":"This feature actually exceeds tracing, it can be used for other purposes as well. The command command allows to specify action to be taken when a breakpoint is hit. Here, you will see how to use it to trace the value of variables as your code executes. First you set a breakpoint, next you add the command to be executed, i.e., (gdb) b 9 Breakpoint 1 at 0x5555555548a3: file display_demo.f90, line 9. (gdb) commands Type commands for breakpoint(s) 2, one per line. End with a line saying just \"end\". >silent >printf \"i = %d, total = %f\\n\", i, total >continue >end The silent command suppresses the usual output that is displayed when the application hits a breakpoint. In this case, you use it because you just want to display variable values as the code executes. The printf command is quite similar to the Bash printf built-in. Its first argument is a formatting string, the following arguments are the expressions to display the values for. Don't forget to add the newline character \\n to get readable output. Some format specifiers you can use are %f : floating point number; %d , %x : signed integer in decimal, and hexadecimal, respectively; %s , %c : string and character, respectively. The continue command ensures that the application will resume execution. When you execute the application, you would see output similar to the following for our running example, (gdb) r Starting program: /home/gjb/Documents/Projects/training-material/Debugging/Gdb/Fortran/Tree/display_demo.exe i = 0, total = 0.000000 i = 1, total = 1.000000 i = 2, total = 11.000000 i = 3, total = 111.000000 i = 4, total = 1111.000000 i = 5, total = 11111.000000 i = 6, total = 111111.000000 i = 7, total = 1111111.000000 i = 8, total = 11111111.000000 i = 9, total = 111111112.000000 i = 10, total = 1111111168.000000","title":"Commands at breakpoints"},{"location":"BugsAtRuntime/tracing/#dynamic-print","text":"As mentioned, command has more applications than simple tracing, and frankly, it is an overkill for that purpose thanks to GDB's dynamic print command dprintf . As its first argument this command takes a location, at which it should be executed each time that location is reached during the execution. This is convenient, since no breakpoint has to be set. The format string is the same as for the printf command. For our running example, (gdb) dprintf 9, \"i = %d, total = %f\\n\", i, total Dprintf 1 at 0x5555555548a3: file display_demo.f90, line 9. (gdb) r Starting program: /home/gjb/Documents/Projects/training-material/Debugging/Gdb/Fortran/Tree/display_demo.exe i = 0, total = 0.000000 i = 1, total = 1.000000 i = 2, total = 11.000000 i = 3, total = 111.000000 i = 4, total = 1111.000000 i = 5, total = 11111.000000 i = 6, total = 111111.000000 i = 7, total = 1111111.000000 i = 8, total = 11111111.000000 i = 9, total = 111111112.000000 i = 10, total = 1111111168.000000 As you can see, the same output is generated as the one you got by using command , with a lot less effort.","title":"Dynamic print"},{"location":"BugsAtRuntime/tracing/#watching-for-change","text":"Although tracing can be useful, it can be fairly cumbersome when not much changes over time. It would be more useful to only display values that change. Consider the following straightforward code sample. integer :: i, j integer, dimension(rows, cols) :: values character(len=32) :: fmt_str ... do j = 1, cols do i = 1, rows values(i, j) = (i - 1)*cols + j end do end do You want to monitor changes on the outer do-loop variable j . When that variable changes, you want to see the value of i , as well as the new and old value of j . This can easily be achieved by combining a number of concepts you already know. We define a GDB variable $current_j to keep track of the current value of the Fortran variable j . We set a conditional breakpoint in the inner do-loop, so that execution will only halt when j is not equal to $current_j , and we attach commands to that breakpoint. (gdb) set $current_j = 0 (gdb) b 10 if j .ne. $current_j Breakpoint 1 at 0x8ef: file watch.f90, line 10. (gdb) commands Type commands for breakpoint(s) 1, one per line. End with a line saying just \"end\". >silent >printf \"i = %d, j = %d, old j = %d\\n\", i, j, $current_j >set $current_j = j >continue >end (gdb) r i = 1, j = 1, old j = 0 i = 1, j = 2, old j = 1 i = 1, j = 3, old j = 2 i = 1, j = 4, old j = 3","title":"Watching for change"},{"location":"BugsAtRuntime/tracing/#hardware-watch-points","text":"As you saw in the video, hardware watch points can be very useful to pinpoint suspicious memory access patterns. Typically, there are situations when the value of a variable or a data structure is modified, but you don't know where in the code that might happen. Consider the following situation. The application computes the evolution of the temperature of a rectangular flat surface represented as a 2D array. The boundaries of the surface have a constant temperature throughout the computation. The 2D matrix is initialised such that its first and last row, and its first and last column are set to the boundary temperature ( init_temp ). In each time step, the temperature at each interior point of the 2D array is updated by taking into account the temperatures of its neighbours ( update_temp ). In addition, with a given probability, an interior point of the 2D array is set to a random value ( perturb_temp ). real, dimension(x_max, y_max) :: temp real :: boundary, prob integer :: t ... call init_temp(temp, boundary) call show_temp(temp) do t = 1, t_max call update_temp(temp) call perturb_temp(temp, prob) end do call show_temp(temp) When you run the application, you notice that some values on the boundary have changed, and that shouldn't happen according to the design of the program. There must be a bug. This is a toy example, but you can easily imagine a similar, but vastly more complicated setting. You could set breakpoints in the do-loop, but to identify the culprit you have to check for change after each procedure call, which is tedious and error prone. It is much easier and faster to just set a watch point on a boundary, so that the application will halt as soon as that it is modified. So first set a breakpoint after the initialisation of temp is done, run the application, set a watch point, and continue. (gdb) b 9 (gdb) r (gdb) watch temp(1, 1)@x_max Hardware watchpoint 2: temp(1, 1)@x_max (gdb) c Continuing. Hardware watchpoint 2: temp(1, 1)@x_max Old value = (0, 0, 0, 0) New value = (0, 0, 0.101466358, 0) perturb_temp (temp=..., prob=0.699999988) at watch_point.f90:62 62 end if Presto! The boundary is inadvertently modified by the procedure perturb_temp . Now you can concentrate on that procedure to pinpoint the exact problem in the algorithm. Contrary to appearances, he watch command doesn't actually monitor the value of variables, but rather the value stored in the memory the variable refers to. This explains why it is called a \"hardware watch point\". Watch points come in a few flavours: watch : break when the value is modified; rwatch : break when the value is read; awatch : break when the values is read or modified. As you would expect, info watchpoints will list the watch points in your current debug session. Watch points will also be shown when you use info breakpoints , and you can manage them in the exact same way, e.g., disabling/enabling, deleting, and so on.","title":"Hardware watch points"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/","text":"Arm DDT Arm DDT is an excellent debugger that helps you debug applications at almost any scale, that run on tens of thousands of cores. It is a commercial product, so you need a license to run it, but perhaps your HPC center has one. On their website, Arm describes DDT as follows: Arm DDT is the number one debugger in research, industry, and academia for software engineers and scientists developing C++, C, Fortran parallel and threaded applications on CPUs, GPUs, Intel and Arm. Arm DDT is trusted as a powerful tool for automatic detection of memory bugs and divergent behavior to achieve lightning-fast performance at all scales. DDT is part of Arm Forge, which also includes MAP, an excellent profiler. As an aside, although officially DDT stands for Distributed Debugging Tool, it is very likely a pun on dichlorodiphenyltrichloroethane, commonly known as DDT. It is an insecticide that was used extensively from 1950s to 1980s. It was mostly banned for its side effects on the environment. However, Arm DDT is entirely safe to use, and will help you eradicate the bugs in your software. Debugging with DDT Everything you know about GDB can be applied to DDT. You can set breakpoints and inspect the values of variables. Stepping through the execution can be done using buttons or keyboard shortcuts. Just like in GDB, breakpoints can be conditional, and you can set watchpoints as well. Tracepoints offer the functionality that the dynamic print ( dprintf ) implements in GDB. Inspecting the call stack and switching frames is very intuitive, as is switching between threads and/or processes. Evaluating expressions at runtime can also be done. These expressions are displayed in their own view, which is updated each time the application is paused. Data exploration Arm DDT was indeed designed to work at scale, to debug applications that have thousands of processes and threads. A lot of attention has been paid to ensure that information is aggregated as much as possible. From the visualisation of the call stack, you can immediately see how many processes are paused in various function calls. Another area where information aggregation is very important is the inspection of variables. The user interface is cleverly designed to support doing this effectively for parallel code. Spark lines give you insight into the values across processes or threads. This helps to spot potential issues quickly, even without switching to other processes or threads. Another feature that is quite convenient in this context is the option to compare a value across processes or threads. This shows the value of that variable, but also the statistics over these values. It includes counts for values such as infinity and NaN. For arrays there is a specialised tool, i.e., the array view. This view can be used to visually inspect an array and create a 2D layout. If the array is distributed over multiple processes, this can be visualised as well. Just as for the comparison across processes, statistics are available as well. Given that visualisation can be a great help to spot anomalies in data, and hence bugs, the scientific visualisation package VisIt can be used integrated with Arm DDT. Memory access In addition, Arm DDT has extensive support for memory access debugging, similar to what you can do with Valgrind. You can tune the level of debugging, which impacts the performance. You can check for basic: invalid pointers passed to memory allocation functions; check-funcs: checks more functions for invalid pointers (mostly string functions); check-heap: heap corruption by, e.g., writes to invalid addresses; check-fence: checks whether the end of an allocation has been overwritten when it is freed; alloc-blank: initialises allocated memory to some known value; free-blank: assigns a known value to memory locations that are freed; check-blank: checks whether memory that was blanked has been overwritten (implies alloc-blank and free-blank); realloc-copy: ensures that data is copied to a new pointer upon reallocation; free-protect: enables (when possible) hardware checks to verify that memory that was freed is not overwritten. As mentioned, this can have an impact of performance which you can mitigate at least in part by reconfiguring options at runtime, e.g., only enabling some checks during a specific phase of the execution. It is also possible to limit memory checks to a range of processes only. However, this may cause load imbalance, and hence not actually improve performance. Forging code As mentioned, DDT is part of Arm Forge. The name refers to the fact that the source code view in DDT is actually a full fledged editor. From within DDT, you can modify and rebuild your code and it even integrates with version control systems. This allows for a fast and convenient round-trip experience when identifying and fixing code defects. Log book A seemingly minor feature can be a great convenience. Arm DDT automatically maintains a log of your debug session by recording each action you take. In addition, you can add your own annotations. This can be a great help during lengthy debug sessions. Alternatives Of course, there are alternatives to Arm DDT. At a pinch, you can use GDB, but you would face several inconveniences since you would have to ensure your application idles after starting it to ensure you can attach GDB debug sessions to the processes, and you have to attach a GDB session to all the processes you are interested in. In this setup, it would be very hard to aggregate information across processes and conveniently explore your application's data. The Eclipse PTP (Parallel Tools Platform) should offer more convenient debugging options, but at least on the clusters I've access to, that doesn't work reliably. Lastly, RogueWave has a commercial product that has many similarities to Arm DDT, e.g., TotalView.","title":"Arm DDT parallel debugger"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#arm-ddt","text":"Arm DDT is an excellent debugger that helps you debug applications at almost any scale, that run on tens of thousands of cores. It is a commercial product, so you need a license to run it, but perhaps your HPC center has one. On their website, Arm describes DDT as follows: Arm DDT is the number one debugger in research, industry, and academia for software engineers and scientists developing C++, C, Fortran parallel and threaded applications on CPUs, GPUs, Intel and Arm. Arm DDT is trusted as a powerful tool for automatic detection of memory bugs and divergent behavior to achieve lightning-fast performance at all scales. DDT is part of Arm Forge, which also includes MAP, an excellent profiler. As an aside, although officially DDT stands for Distributed Debugging Tool, it is very likely a pun on dichlorodiphenyltrichloroethane, commonly known as DDT. It is an insecticide that was used extensively from 1950s to 1980s. It was mostly banned for its side effects on the environment. However, Arm DDT is entirely safe to use, and will help you eradicate the bugs in your software.","title":"Arm DDT"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#debugging-with-ddt","text":"Everything you know about GDB can be applied to DDT. You can set breakpoints and inspect the values of variables. Stepping through the execution can be done using buttons or keyboard shortcuts. Just like in GDB, breakpoints can be conditional, and you can set watchpoints as well. Tracepoints offer the functionality that the dynamic print ( dprintf ) implements in GDB. Inspecting the call stack and switching frames is very intuitive, as is switching between threads and/or processes. Evaluating expressions at runtime can also be done. These expressions are displayed in their own view, which is updated each time the application is paused.","title":"Debugging with DDT"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#data-exploration","text":"Arm DDT was indeed designed to work at scale, to debug applications that have thousands of processes and threads. A lot of attention has been paid to ensure that information is aggregated as much as possible. From the visualisation of the call stack, you can immediately see how many processes are paused in various function calls. Another area where information aggregation is very important is the inspection of variables. The user interface is cleverly designed to support doing this effectively for parallel code. Spark lines give you insight into the values across processes or threads. This helps to spot potential issues quickly, even without switching to other processes or threads. Another feature that is quite convenient in this context is the option to compare a value across processes or threads. This shows the value of that variable, but also the statistics over these values. It includes counts for values such as infinity and NaN. For arrays there is a specialised tool, i.e., the array view. This view can be used to visually inspect an array and create a 2D layout. If the array is distributed over multiple processes, this can be visualised as well. Just as for the comparison across processes, statistics are available as well. Given that visualisation can be a great help to spot anomalies in data, and hence bugs, the scientific visualisation package VisIt can be used integrated with Arm DDT.","title":"Data exploration"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#memory-access","text":"In addition, Arm DDT has extensive support for memory access debugging, similar to what you can do with Valgrind. You can tune the level of debugging, which impacts the performance. You can check for basic: invalid pointers passed to memory allocation functions; check-funcs: checks more functions for invalid pointers (mostly string functions); check-heap: heap corruption by, e.g., writes to invalid addresses; check-fence: checks whether the end of an allocation has been overwritten when it is freed; alloc-blank: initialises allocated memory to some known value; free-blank: assigns a known value to memory locations that are freed; check-blank: checks whether memory that was blanked has been overwritten (implies alloc-blank and free-blank); realloc-copy: ensures that data is copied to a new pointer upon reallocation; free-protect: enables (when possible) hardware checks to verify that memory that was freed is not overwritten. As mentioned, this can have an impact of performance which you can mitigate at least in part by reconfiguring options at runtime, e.g., only enabling some checks during a specific phase of the execution. It is also possible to limit memory checks to a range of processes only. However, this may cause load imbalance, and hence not actually improve performance.","title":"Memory access"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#forging-code","text":"As mentioned, DDT is part of Arm Forge. The name refers to the fact that the source code view in DDT is actually a full fledged editor. From within DDT, you can modify and rebuild your code and it even integrates with version control systems. This allows for a fast and convenient round-trip experience when identifying and fixing code defects.","title":"Forging code"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#log-book","text":"A seemingly minor feature can be a great convenience. Arm DDT automatically maintains a log of your debug session by recording each action you take. In addition, you can add your own annotations. This can be a great help during lengthy debug sessions.","title":"Log book"},{"location":"BugsAtRuntime/Debuggers/ArmDdt/arm_ddt/#alternatives","text":"Of course, there are alternatives to Arm DDT. At a pinch, you can use GDB, but you would face several inconveniences since you would have to ensure your application idles after starting it to ensure you can attach GDB debug sessions to the processes, and you have to attach a GDB session to all the processes you are interested in. In this setup, it would be very hard to aggregate information across processes and conveniently explore your application's data. The Eclipse PTP (Parallel Tools Platform) should offer more convenient debugging options, but at least on the clusters I've access to, that doesn't work reliably. Lastly, RogueWave has a commercial product that has many similarities to Arm DDT, e.g., TotalView.","title":"Alternatives"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/","text":"How do I get a handle on a running program? In some circumstance, you need information on what a running program is doing. You can't or don't want to stop and relaunch it under control of a debugger. No problem, GDB can actually attach to a running application. However, most modern Linux operating systems have hardened kernels, and by default, process tracing is not permitted. The error message you would get when trying it would be similar to ptrace: Operation not permitted You will need root access to change the default behaviour, either temporarily or permanently. The latter may be convenient, but perhaps not such a good idea from a security point of view. Kernel hardening has been done for a reason. A third alternative is to run the debugger as root, but I would strongly advise against that. Enabling process tracing Temporarily enabling process tracing for non-root users As root on an Ubuntu system, execute the following command # echo 0 > /proc/sys/kernel/yama/ptrace_scope Process tracing will be allowed until the next reboot, or until you revert it manually by executing, again as root # echo 1 > /proc/sys/kernel/yama/ptrace_scope Permanently enabling process tracing for non-root users On Ubuntu, edit the file /etc/sysctl.d/10-ptrace.conf . Its last line reads kernel.yama.ptrace_scope = 1 This line should be changed to kernel.yama.ptrace_scope = 0 Again, this may not be wise. Attach GDB to a running application To attach the GDB debugger to a running application, you first need the application's process ID. You can use the top command for that purpose. The process ID for each process is displayed in the first column (PID). Alternatively, you can use pgrep . For instance, suppose the application you want to attach to is infinite_loop.exe (perhaps aptly named), you would get the process ID using $ pgrep infinite_loop 13661 The process ID will of course be different on your system, and vary from run to run. You can attach GDB as follows: $ gdb ./infinite_loop.exe 13661 This will immediately halt the application, showing the statement that will be executed next. You can now use GDB to explore the state of your application, printing variables, stepping, and so on. When you quit the debugger, your application will continue to run. If you decide while debugging that there is no point for it to run, you can stop the application from within GDB by using the kill command. You will be prompted for confirmation. Tracing system calls The Linux command strace can be used to quickly get an idea of what is going on if your application seems to be stuck. It will print a trace of the system calls done by the process, e.g., read/write operations or exec calls. Although this gives of course much less information than attaching a debugger, it can be useful nevertheless. For instance, you can detect at a glance that the application is stuck in a read operation, so it might be waiting for a reply from a network operation. Using strace is straightforward, simply determine the process ID of the application you want to trace, and run strace , e.g., $ strace -p 13661 What files does my application use? Sometimes an application uses temporary files without you being aware of it, which may lead to interesting problems. To determine which files are used by a running process, determine its process ID and run lsof (list open files). $ lsof -p 13661 For a dynamically linked application, this will also show the currently loaded shared object files. What resources does my application use? To check the amount of memory an application currently has allocated, or the user time versus the system time, prtstat can be quite useful. Again, determine the process ID of the running application, and use $ prtstat 13661","title":"Attaching GDB to running processes"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#how-do-i-get-a-handle-on-a-running-program","text":"In some circumstance, you need information on what a running program is doing. You can't or don't want to stop and relaunch it under control of a debugger. No problem, GDB can actually attach to a running application. However, most modern Linux operating systems have hardened kernels, and by default, process tracing is not permitted. The error message you would get when trying it would be similar to ptrace: Operation not permitted You will need root access to change the default behaviour, either temporarily or permanently. The latter may be convenient, but perhaps not such a good idea from a security point of view. Kernel hardening has been done for a reason. A third alternative is to run the debugger as root, but I would strongly advise against that.","title":"How do I get a handle on a running program?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#enabling-process-tracing","text":"","title":"Enabling process tracing"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#temporarily-enabling-process-tracing-for-non-root-users","text":"As root on an Ubuntu system, execute the following command # echo 0 > /proc/sys/kernel/yama/ptrace_scope Process tracing will be allowed until the next reboot, or until you revert it manually by executing, again as root # echo 1 > /proc/sys/kernel/yama/ptrace_scope","title":"Temporarily enabling process tracing for non-root users"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#permanently-enabling-process-tracing-for-non-root-users","text":"On Ubuntu, edit the file /etc/sysctl.d/10-ptrace.conf . Its last line reads kernel.yama.ptrace_scope = 1 This line should be changed to kernel.yama.ptrace_scope = 0 Again, this may not be wise.","title":"Permanently enabling process tracing for non-root users"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#attach-gdb-to-a-running-application","text":"To attach the GDB debugger to a running application, you first need the application's process ID. You can use the top command for that purpose. The process ID for each process is displayed in the first column (PID). Alternatively, you can use pgrep . For instance, suppose the application you want to attach to is infinite_loop.exe (perhaps aptly named), you would get the process ID using $ pgrep infinite_loop 13661 The process ID will of course be different on your system, and vary from run to run. You can attach GDB as follows: $ gdb ./infinite_loop.exe 13661 This will immediately halt the application, showing the statement that will be executed next. You can now use GDB to explore the state of your application, printing variables, stepping, and so on. When you quit the debugger, your application will continue to run. If you decide while debugging that there is no point for it to run, you can stop the application from within GDB by using the kill command. You will be prompted for confirmation.","title":"Attach GDB to a running application"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#tracing-system-calls","text":"The Linux command strace can be used to quickly get an idea of what is going on if your application seems to be stuck. It will print a trace of the system calls done by the process, e.g., read/write operations or exec calls. Although this gives of course much less information than attaching a debugger, it can be useful nevertheless. For instance, you can detect at a glance that the application is stuck in a read operation, so it might be waiting for a reply from a network operation. Using strace is straightforward, simply determine the process ID of the application you want to trace, and run strace , e.g., $ strace -p 13661","title":"Tracing system calls"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#what-files-does-my-application-use","text":"Sometimes an application uses temporary files without you being aware of it, which may lead to interesting problems. To determine which files are used by a running process, determine its process ID and run lsof (list open files). $ lsof -p 13661 For a dynamically linked application, this will also show the currently loaded shared object files.","title":"What files does my application use?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_attach/#what-resources-does-my-application-use","text":"To check the amount of memory an application currently has allocated, or the user time versus the system time, prtstat can be quite useful. Again, determine the process ID of the running application, and use $ prtstat 13661","title":"What resources does my application use?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/","text":"What can you do with GDB? The screencasts have shown you a number of features of GDB, but there is a lot more you can do. Here you'll get a recap of what was shown, as well as an overview of a few additional features. How to prepare for debugging? Make sure your application can be debugged conveniently. This implies compiling with * -g to add references to the source code in the executable, and * -O0 to switch off optimisation. Note that -g has no influence on the performance of your application, it just adds a little bit to its size. In most circumstances, you don't care about it. Remember to build with optimisation enabled once you're done debugging. How do you start the debugger? Start GDB on the command line to debug the executable application.exe using $ gdb ./application.exe If your application takes command line arguments, those can be conveniently passed by using GDB's --args option. You normally run your application using $ ./application.exe --verbose --n 5 To debug it with the same command line arguments, simply invoke the debugger as follows $ gdb --args ./application.exe --verbose --n 5 How to get help? GDB has a help system that is quite useful. The help command will list the available topics. You can also request help on specific command, e.g., help print . If you don't really know which topic covers the concept you are looking for, you can use the apropos command, e.g., apropos variable . How to quit the debugger? The quit or q command will quit the debugger, but you'll be asked for confirmation. How to view your source code? Although you would typically have an editor open that displays the source code of the application you are debugging, it can nevertheless be convenient to list it in the debugger itself, just to have a quick peek. The list or l command will do that for you. It can be used in a number of ways, e.g., l : list the code at the current location; l <ln> : list the code in the current file around line <ln> ; l <file> : list the code in file <file> ; l <file>:<ln> : list the code in the file <file> around line <ln> ; l <func> : list the source code of the function named <func> . If you don't feel comfortable with this way of working, you can enable TUI mode. This will split your screen into two parts. The upper part shows your source code, the lower is the GDB command line. When GDB is running, you can switch to TUI mode as follows: (gdb) tui enable You can switch back to non-TUI using (gdb) tui disable You should experiment with it to see what is most comfortable for you. If you decide you like TUI, you can start GDB in TUI mode by invoking gdbtui , rather than gdb , on the command line of your shell. Breakpoints Setting a breakpoint can be done with the break or b command, followed by a line number, e.g., b 15 a function name, e.g., b fib a file name and a line number, e.g., b fib.c:17 or a file name and a function name, e.g., b fib.c:fib A breakpoint can be conditional, and that condition is formulated in the programming language of the application being debugged. E.g., the following GDB command would halt the application when it enters the function fib when called with an argument n that is less than 2. (gdb) b fib if n < 2 The syntax of the condition is that of the programming language being debugged, so for C/C++ versus Fortran, respectively == versus .eq. != versus .ne. < versus .lt. && versus .and. ... Note that the \"new style\" Fortran comparison operators == , < , ..., and /= will not work, which is, frankly, a bit of a pain. GDB offers quite some options to manage breakpoints. Breakpoints have a numerical identifier so that specific actions can be performed on them. In the following <n> denotes a breakpoint number. Current breakpoints can be listed using info breakpoints , or i b temporarily disabled/enabled using disable b <n> and enable b <n> delete a breakpoint using delete <n> or d <n> delete all current breakpoints using delete or d If you want to make an existing breakpoint conditional, that is as simple as attaching a condition to it using the cond command, e.g., (gdb) b 17 Breakpoint 3 at 0x156f: file watch_point.f90, line 19. ... cond b i == 10 Now the breakpoint with identifier 3 will be conditional, i.e., the application will only halt when the variable i is equal to 10. So there is no need to create a new breakpoint. How to execute and to step through code? To run the application, use the run command, followed by the required command line arguments, if any, e.g., (gdb) r 15 The command above will start the application with 15 as the first and only command line argument. The application will run until it reaches the first enabled breakpoint, or until it terminates abnormally, or until it terminates normally. Once the application hits a breakpoint, you can continue the execution in several ways: use continue or c to resume execution until the next breakpoint is hit (which might be the same if the code iterates), use next or n to execute the statement the execution halted at, stepping over function calls, or use step or s to step into a function that is in the current statement. The continue command can optionally be followed by a number, e.g., for the following code fragment the breakpoint is in the for-loop: for (int i = 0; i < 10; i++) printf(\"%d\\n\", i); When the execution is halted the first time, the do-loop will be in its first iteration, so i == 1 . Using c 5 at this point will resume the application and halt it the fifth time the breakpoint is hit. So when the application is halted, i == 6 (the breakpoint was skipped 4 times). The until <ln> command can be very useful. It will run the application until the specified line number <ln> is reached, without having to set a breakpoint. However, it works only in the current frame. More generally, advance <loc> will continue to execute the application up to the specified location <loc> that takes the same format as that for the break command. While evaluating a function, it is often convenient to halt at the end of the function, just before it goes out of scope. This is easily done using the finish command. What can you do at a breakpoint? Checking the value of variables is one of the main operations once your application hits a breakpoint. This way, you can verify that your expectations are met. By the way, during lengthy debug sessions, you may want a reminder of the point in the code you are currently at. For this purpose, the frame command is quite useful. It will display the current frame and the line of code the application will execute next, e.g., (gdb) #0 main () at features.c:11 11 if (vectors == NULL) We will discuss frames in more detail in a later section. Inspecting values The value of a variable can be displayed using the print or p command, e.g., (gdb) p a GDB will attempt to provide a view that is as useful as possible if it can determine the semantics of the variable. For an array, it will print the elements. For a C structure, member names and their respective values will be displayed. Since pointers have no semantics for the compiler, the usual dereferencing is required to display relevant information besides the raw address. For example, consider the following data structure that represents a vector with n double precision floating point elements. The element member is a pointer, do before you can store values as elements, dynamic memory allocation has to be done. typedef struct { double *element; int n; } Vector; Now consider the following variable: Vector *vectors[5]; Note the difference between vectors and vectors[0]->element -- the former is an array, the second is a pointer. After it has been properly initialised, you could print several things at a breakpoint: p vectors : array of 5 addresses, each for a Vector ; p &vectors : address of the first Vector in the array; p vectors[0] : address of the first Vector ; p *vectors[0] : struct representation with the value of the n field, and the address assigned to the element field for the first Vector ; p vectors[0]->n : value of the n member of the first Vector ; p &vectors[0]->n : address of the member n of the first Vector ; p vectors[0]->element : address of the first element of the first Vector ; p *(vectors[0]->element) : value of the first element of the first Vector ; p vectors[0]->element[0] : value of the first element of the first Vector ; p vectors[1]@3 : the addresses of the second up to and including the fourth Vector ; p vectors[0]->element[0]@vectors[0]->n : all element values of the first Vector . The last example is fairly involved, so step by step: 1. vectors[0]->n is the number of elements of the first Vector in the array, call that k for convenience, 1. vectors[0]->element[0]@k is a slice that starts at index 0 and has length k . Since this expression is somewhat unwieldy, you could use a GDB variable to store the length of the Vector , i.e., (gdb) set $len = vectors[0]->n (gdb) p vectors[0]->element[0]@$len Note that when you display a result using print , the result is in fact stored in a GDB variable, e.g., (gdb) p vectors[0]->n $5 = 3 (gdb) p $5 + 5 $6 = 8 Inspecting types Very often, you are not only interested in the value of variables, but also in the definition of their data type. Two commands can be quite useful to get that information, whatis and ptype . The former provides a more high-level view on the type, while the latter gives you more details. For instance, for the running example, you might want to explore the data structure, e.g., (gdb) whatis vectors type = Vector *[100] (gdb) ptype vectors type = struct { double *element; int n; } *[100] Using whatis , you would have to dig up the type definition of Vectors , which is a typedef to the structure that is displayed directly by ptype . Executing functions and modifying state It is even possible to evaluate arbitrary function calls in GDB. Consider the following declarations: double vector_length(Vector *v); void fill_vector(Vector *v, double start_value, double delta_value); Vector *init_vector(int n); The first function can be evaluated at a breakpoint using print which will display the result of the computation. (gdb) p vector_length(vectors[0]) The second function does not return a value, so it is better to call it using the call command. (gdb) call fill_vector(vectors[0], 1.0, 0.2) It is even possible to create a new Vector structure, assign it to a GDB variable, and call additional functions on it. (gdb) set $v = init_vector(5) (gdb) p vector_length($v) You can modify the value of variables at runtime while in the debugger using the set var command. (gdb) set var vectors[0] = init_vector(vector_len) This can help you experiment while debugging by setting values to what you suspect they should be. However, thread with care, there be dragons. It is all too easy to completely mess up the state of your application. Exploring data structures If you know your data structures well and are proficient referencing/dereferencing memory then print will serve you well. However, to explore code that you are not familiar with, the explore command may be a great help. It interactively guides you through the data exploration. The session below would be typical for the data structure in the running example. (gdb) explore vectors 'vectors' is an array of 'Vector *'. Enter the index of the element you want to explore in 'vectors': 0 'vectors[0]' is a pointer to a value of type 'Vector' Continue exploring it as a pointer to a single value [y/n]: y The value of '*(vectors[0])' is of type 'Vector' which is a typedef of type 'struct {...}' The value of '*(vectors[0])' is a struct/class of type 'struct {...}' with the following fields: element = <Enter 0 to explore this field of type 'double *'> n = 5 .. (Value of type 'int') Enter the field number of choice: 0 '(*(vectors[0])).element' is a pointer to a value of type 'double' Continue exploring it as a pointer to a single value [y/n]: y '*((*(vectors[0])).element)' is a scalar value of type 'double'. *((*(vectors[0])).element) = 0 Press enter to return to parent value: Returning to parent value... The value of '*(vectors[0])' is a struct/class of type 'struct {...}' with the following fields: element = <Enter 0 to explore this field of type 'double *'> n = 5 .. (Value of type 'int') Enter the field number of choice: 0 '(*(vectors[0])).element' is a pointer to a value of type 'double' Continue exploring it as a pointer to a single value [y/n]: n Continue exploring it as a pointer to an array [y/n]: y Enter the index of the element you want to explore in '(*(vectors[0])).element': 1 '((*(vectors[0])).element)[1]' is a scalar value of type 'double'. ((*(vectors[0])).element)[1] = 0.10000000000000001 Press enter to return to parent value: Returning to parent value... Enter the index of the element you want to explore in '(*(vectors[0])).element'---Type <return> to continue, or q <return> to quit--- As you can see, this can be rather useful to gather information on an unknown data structure. Whether you like this approach is very much a personal preference. Personally, I prefer going with print and ptype , but les gouts et les couleurs ... How to deal with function calls? When a function is called, information is placed on the stack, an area of memory reserved for that purpose. It contains values of the arguments the function was called with, as well as local variables in that function. Using the frame command, you can check at any time which function you are currently looking at, including the arguments it was called with, and see the line of code that will be executed next. To list all the local variables and their value in the function you can use the info locals command. This is useful to get a quick overview. Similarly, info args will display the arguments names and values to the function in this frame. The call stack that stores the frames can be visualised using the backtrace or bt command, e.g., (gdb) bt #0 fill_vector (v=0x555555757280, start_value=0, delta_value=0.10000000000000001) at vectors.c:23 #1 0x0000555555554bc4 in main () at features.c:16 In this case, there are two frames. The top frame is the function fill_vector where the debugger is currently halted, the bottom frame is the main function from where fill_vector was called. Often, you will want to examine the calling context of a function, i.e., a frame that is higher up in the call stack. You can move to the frame of the main function by using the up command, i.e., (gdb) up #1 0x0000555555554bc4 in main () at features.c:16 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); Now, it is easy to inspect the value of variable in the main function. To move back to the called function you can use down . up and down allow you to easily traverse the call stack. However, you can jump to a particular frame immediately by using frame <fn> where <fn> denotes the frame number as displayed in the output of backtrace . Note that regardless of the frame you are examining, the application is halted in the exact same statement until you decide to step or otherwise resume the execution. GDB has of course many more features, which will be explored later.","title":"GDB Features"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#what-can-you-do-with-gdb","text":"The screencasts have shown you a number of features of GDB, but there is a lot more you can do. Here you'll get a recap of what was shown, as well as an overview of a few additional features.","title":"What can you do with GDB?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-prepare-for-debugging","text":"Make sure your application can be debugged conveniently. This implies compiling with * -g to add references to the source code in the executable, and * -O0 to switch off optimisation. Note that -g has no influence on the performance of your application, it just adds a little bit to its size. In most circumstances, you don't care about it. Remember to build with optimisation enabled once you're done debugging.","title":"How to prepare for debugging?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-do-you-start-the-debugger","text":"Start GDB on the command line to debug the executable application.exe using $ gdb ./application.exe If your application takes command line arguments, those can be conveniently passed by using GDB's --args option. You normally run your application using $ ./application.exe --verbose --n 5 To debug it with the same command line arguments, simply invoke the debugger as follows $ gdb --args ./application.exe --verbose --n 5","title":"How do you start the debugger?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-get-help","text":"GDB has a help system that is quite useful. The help command will list the available topics. You can also request help on specific command, e.g., help print . If you don't really know which topic covers the concept you are looking for, you can use the apropos command, e.g., apropos variable .","title":"How to get help?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-quit-the-debugger","text":"The quit or q command will quit the debugger, but you'll be asked for confirmation.","title":"How to quit the debugger?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-view-your-source-code","text":"Although you would typically have an editor open that displays the source code of the application you are debugging, it can nevertheless be convenient to list it in the debugger itself, just to have a quick peek. The list or l command will do that for you. It can be used in a number of ways, e.g., l : list the code at the current location; l <ln> : list the code in the current file around line <ln> ; l <file> : list the code in file <file> ; l <file>:<ln> : list the code in the file <file> around line <ln> ; l <func> : list the source code of the function named <func> . If you don't feel comfortable with this way of working, you can enable TUI mode. This will split your screen into two parts. The upper part shows your source code, the lower is the GDB command line. When GDB is running, you can switch to TUI mode as follows: (gdb) tui enable You can switch back to non-TUI using (gdb) tui disable You should experiment with it to see what is most comfortable for you. If you decide you like TUI, you can start GDB in TUI mode by invoking gdbtui , rather than gdb , on the command line of your shell.","title":"How to view your source code?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#breakpoints","text":"Setting a breakpoint can be done with the break or b command, followed by a line number, e.g., b 15 a function name, e.g., b fib a file name and a line number, e.g., b fib.c:17 or a file name and a function name, e.g., b fib.c:fib A breakpoint can be conditional, and that condition is formulated in the programming language of the application being debugged. E.g., the following GDB command would halt the application when it enters the function fib when called with an argument n that is less than 2. (gdb) b fib if n < 2 The syntax of the condition is that of the programming language being debugged, so for C/C++ versus Fortran, respectively == versus .eq. != versus .ne. < versus .lt. && versus .and. ... Note that the \"new style\" Fortran comparison operators == , < , ..., and /= will not work, which is, frankly, a bit of a pain. GDB offers quite some options to manage breakpoints. Breakpoints have a numerical identifier so that specific actions can be performed on them. In the following <n> denotes a breakpoint number. Current breakpoints can be listed using info breakpoints , or i b temporarily disabled/enabled using disable b <n> and enable b <n> delete a breakpoint using delete <n> or d <n> delete all current breakpoints using delete or d If you want to make an existing breakpoint conditional, that is as simple as attaching a condition to it using the cond command, e.g., (gdb) b 17 Breakpoint 3 at 0x156f: file watch_point.f90, line 19. ... cond b i == 10 Now the breakpoint with identifier 3 will be conditional, i.e., the application will only halt when the variable i is equal to 10. So there is no need to create a new breakpoint.","title":"Breakpoints"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-execute-and-to-step-through-code","text":"To run the application, use the run command, followed by the required command line arguments, if any, e.g., (gdb) r 15 The command above will start the application with 15 as the first and only command line argument. The application will run until it reaches the first enabled breakpoint, or until it terminates abnormally, or until it terminates normally. Once the application hits a breakpoint, you can continue the execution in several ways: use continue or c to resume execution until the next breakpoint is hit (which might be the same if the code iterates), use next or n to execute the statement the execution halted at, stepping over function calls, or use step or s to step into a function that is in the current statement. The continue command can optionally be followed by a number, e.g., for the following code fragment the breakpoint is in the for-loop: for (int i = 0; i < 10; i++) printf(\"%d\\n\", i); When the execution is halted the first time, the do-loop will be in its first iteration, so i == 1 . Using c 5 at this point will resume the application and halt it the fifth time the breakpoint is hit. So when the application is halted, i == 6 (the breakpoint was skipped 4 times). The until <ln> command can be very useful. It will run the application until the specified line number <ln> is reached, without having to set a breakpoint. However, it works only in the current frame. More generally, advance <loc> will continue to execute the application up to the specified location <loc> that takes the same format as that for the break command. While evaluating a function, it is often convenient to halt at the end of the function, just before it goes out of scope. This is easily done using the finish command.","title":"How to execute and to step through code?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#what-can-you-do-at-a-breakpoint","text":"Checking the value of variables is one of the main operations once your application hits a breakpoint. This way, you can verify that your expectations are met. By the way, during lengthy debug sessions, you may want a reminder of the point in the code you are currently at. For this purpose, the frame command is quite useful. It will display the current frame and the line of code the application will execute next, e.g., (gdb) #0 main () at features.c:11 11 if (vectors == NULL) We will discuss frames in more detail in a later section.","title":"What can you do at a breakpoint?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#inspecting-values","text":"The value of a variable can be displayed using the print or p command, e.g., (gdb) p a GDB will attempt to provide a view that is as useful as possible if it can determine the semantics of the variable. For an array, it will print the elements. For a C structure, member names and their respective values will be displayed. Since pointers have no semantics for the compiler, the usual dereferencing is required to display relevant information besides the raw address. For example, consider the following data structure that represents a vector with n double precision floating point elements. The element member is a pointer, do before you can store values as elements, dynamic memory allocation has to be done. typedef struct { double *element; int n; } Vector; Now consider the following variable: Vector *vectors[5]; Note the difference between vectors and vectors[0]->element -- the former is an array, the second is a pointer. After it has been properly initialised, you could print several things at a breakpoint: p vectors : array of 5 addresses, each for a Vector ; p &vectors : address of the first Vector in the array; p vectors[0] : address of the first Vector ; p *vectors[0] : struct representation with the value of the n field, and the address assigned to the element field for the first Vector ; p vectors[0]->n : value of the n member of the first Vector ; p &vectors[0]->n : address of the member n of the first Vector ; p vectors[0]->element : address of the first element of the first Vector ; p *(vectors[0]->element) : value of the first element of the first Vector ; p vectors[0]->element[0] : value of the first element of the first Vector ; p vectors[1]@3 : the addresses of the second up to and including the fourth Vector ; p vectors[0]->element[0]@vectors[0]->n : all element values of the first Vector . The last example is fairly involved, so step by step: 1. vectors[0]->n is the number of elements of the first Vector in the array, call that k for convenience, 1. vectors[0]->element[0]@k is a slice that starts at index 0 and has length k . Since this expression is somewhat unwieldy, you could use a GDB variable to store the length of the Vector , i.e., (gdb) set $len = vectors[0]->n (gdb) p vectors[0]->element[0]@$len Note that when you display a result using print , the result is in fact stored in a GDB variable, e.g., (gdb) p vectors[0]->n $5 = 3 (gdb) p $5 + 5 $6 = 8","title":"Inspecting values"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#inspecting-types","text":"Very often, you are not only interested in the value of variables, but also in the definition of their data type. Two commands can be quite useful to get that information, whatis and ptype . The former provides a more high-level view on the type, while the latter gives you more details. For instance, for the running example, you might want to explore the data structure, e.g., (gdb) whatis vectors type = Vector *[100] (gdb) ptype vectors type = struct { double *element; int n; } *[100] Using whatis , you would have to dig up the type definition of Vectors , which is a typedef to the structure that is displayed directly by ptype .","title":"Inspecting types"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#executing-functions-and-modifying-state","text":"It is even possible to evaluate arbitrary function calls in GDB. Consider the following declarations: double vector_length(Vector *v); void fill_vector(Vector *v, double start_value, double delta_value); Vector *init_vector(int n); The first function can be evaluated at a breakpoint using print which will display the result of the computation. (gdb) p vector_length(vectors[0]) The second function does not return a value, so it is better to call it using the call command. (gdb) call fill_vector(vectors[0], 1.0, 0.2) It is even possible to create a new Vector structure, assign it to a GDB variable, and call additional functions on it. (gdb) set $v = init_vector(5) (gdb) p vector_length($v) You can modify the value of variables at runtime while in the debugger using the set var command. (gdb) set var vectors[0] = init_vector(vector_len) This can help you experiment while debugging by setting values to what you suspect they should be. However, thread with care, there be dragons. It is all too easy to completely mess up the state of your application.","title":"Executing functions and modifying state"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#exploring-data-structures","text":"If you know your data structures well and are proficient referencing/dereferencing memory then print will serve you well. However, to explore code that you are not familiar with, the explore command may be a great help. It interactively guides you through the data exploration. The session below would be typical for the data structure in the running example. (gdb) explore vectors 'vectors' is an array of 'Vector *'. Enter the index of the element you want to explore in 'vectors': 0 'vectors[0]' is a pointer to a value of type 'Vector' Continue exploring it as a pointer to a single value [y/n]: y The value of '*(vectors[0])' is of type 'Vector' which is a typedef of type 'struct {...}' The value of '*(vectors[0])' is a struct/class of type 'struct {...}' with the following fields: element = <Enter 0 to explore this field of type 'double *'> n = 5 .. (Value of type 'int') Enter the field number of choice: 0 '(*(vectors[0])).element' is a pointer to a value of type 'double' Continue exploring it as a pointer to a single value [y/n]: y '*((*(vectors[0])).element)' is a scalar value of type 'double'. *((*(vectors[0])).element) = 0 Press enter to return to parent value: Returning to parent value... The value of '*(vectors[0])' is a struct/class of type 'struct {...}' with the following fields: element = <Enter 0 to explore this field of type 'double *'> n = 5 .. (Value of type 'int') Enter the field number of choice: 0 '(*(vectors[0])).element' is a pointer to a value of type 'double' Continue exploring it as a pointer to a single value [y/n]: n Continue exploring it as a pointer to an array [y/n]: y Enter the index of the element you want to explore in '(*(vectors[0])).element': 1 '((*(vectors[0])).element)[1]' is a scalar value of type 'double'. ((*(vectors[0])).element)[1] = 0.10000000000000001 Press enter to return to parent value: Returning to parent value... Enter the index of the element you want to explore in '(*(vectors[0])).element'---Type <return> to continue, or q <return> to quit--- As you can see, this can be rather useful to gather information on an unknown data structure. Whether you like this approach is very much a personal preference. Personally, I prefer going with print and ptype , but les gouts et les couleurs ...","title":"Exploring data structures"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_features/#how-to-deal-with-function-calls","text":"When a function is called, information is placed on the stack, an area of memory reserved for that purpose. It contains values of the arguments the function was called with, as well as local variables in that function. Using the frame command, you can check at any time which function you are currently looking at, including the arguments it was called with, and see the line of code that will be executed next. To list all the local variables and their value in the function you can use the info locals command. This is useful to get a quick overview. Similarly, info args will display the arguments names and values to the function in this frame. The call stack that stores the frames can be visualised using the backtrace or bt command, e.g., (gdb) bt #0 fill_vector (v=0x555555757280, start_value=0, delta_value=0.10000000000000001) at vectors.c:23 #1 0x0000555555554bc4 in main () at features.c:16 In this case, there are two frames. The top frame is the function fill_vector where the debugger is currently halted, the bottom frame is the main function from where fill_vector was called. Often, you will want to examine the calling context of a function, i.e., a frame that is higher up in the call stack. You can move to the frame of the main function by using the up command, i.e., (gdb) up #1 0x0000555555554bc4 in main () at features.c:16 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); Now, it is easy to inspect the value of variable in the main function. To move back to the called function you can use down . up and down allow you to easily traverse the call stack. However, you can jump to a particular frame immediately by using frame <fn> where <fn> denotes the frame number as displayed in the output of backtrace . Note that regardless of the frame you are examining, the application is halted in the exact same statement until you decide to step or otherwise resume the execution. GDB has of course many more features, which will be explored later.","title":"How to deal with function calls?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/","text":"How can you debug OpenMP code using GDB? Debugging OpenMP applications with GDB is fairly straightforward. To include debugging information in the application, use -g as usual when compiling and building the application. When starting the debugger, you may want to control the number of OpenMP threads. $ OMP_NUM_THREADS=2 gdb ./application.exe All GDB commands you are familiar with will work when debugging an OpenMP application. However, there are a few relevant ones specifically for multi-threaded applications. Note : none of the commands discussed below are specific to OpenMP as such, they will also work when you use threading libraries such as pthreads . Listing threads When the program is halted in a parallel region, it will display the thread ID that hit the breakpoint. You can get information on the running threads using (gdb) info threads The thread GDB halted in is marked with an asterisk. Notice that the thread ID listed by GDB is not the OpenMP thread number. So typically, GDB thread 1 will have OpenMP thread number 0, and so on. This may be somewhat confusing. Switching threads The variables that will be printed are those local to the thread, so all shared and thread-private variables. You can switch the context to another thread using the thread command, and the GDB thread ID to switch to, e.g., (gdb) thread 2 Note that it is very likely that this thread will be at another point in the source code. You can make it continue to the breakpoint using the continue command. However, all threads will be resumed, so the first thread that hit the breakpoint will continue past it. This may or may not be what you want. Scheduler locking Although it is quite useful to debug race conditions and deadlock situations that GDB halts all threads when the first reaches a breakpoint, sometimes you just like all threads to continue to the same breakpoint in order to conveniently check the value of variables across threads. This can be achieved by turning on scheduler locking, i.e., (gdb) set scheduler-locking on Now the continue , next , step commands will only advance the current thread. Note : this can lead to a deadlock. To turn scheduler locking off again, simply use (gdb) set scheduler-locking off To check the status of scheduler locking, use the show command, i.e., (gdb) show scheduler-locking Sending GDB command to multiple threads You can easily execute GDB command on multiple threads at once. For instance, suppose you would like all threads to halt on a breakpoint, and compare that values of variables across threads. Suppose that thread number 1 has halted on the breakpoint, first turn on scheduler locking, and then let all threads but the first continue to the breakpoint. The total number of threads is 4. (gdb) set scheduler-locking on (gdb) thread apply 2-4 continue You can verify with info thread that all threads have halted at the breakpoint. Now we can print the value of a variable, say tid , across threads. (gdb) thread apply all print tid The keyword all can be used as a shortcut for the list of all thread IDs. The thread list to apply the command to can be one or more thread IDs, separated by spaces, and/or one or more ranges, e.g., 2-4 ; or all .","title":"Debugging OpenMP applications with GDB"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/#how-can-you-debug-openmp-code-using-gdb","text":"Debugging OpenMP applications with GDB is fairly straightforward. To include debugging information in the application, use -g as usual when compiling and building the application. When starting the debugger, you may want to control the number of OpenMP threads. $ OMP_NUM_THREADS=2 gdb ./application.exe All GDB commands you are familiar with will work when debugging an OpenMP application. However, there are a few relevant ones specifically for multi-threaded applications. Note : none of the commands discussed below are specific to OpenMP as such, they will also work when you use threading libraries such as pthreads .","title":"How can you debug OpenMP code using GDB?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/#listing-threads","text":"When the program is halted in a parallel region, it will display the thread ID that hit the breakpoint. You can get information on the running threads using (gdb) info threads The thread GDB halted in is marked with an asterisk. Notice that the thread ID listed by GDB is not the OpenMP thread number. So typically, GDB thread 1 will have OpenMP thread number 0, and so on. This may be somewhat confusing.","title":"Listing threads"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/#switching-threads","text":"The variables that will be printed are those local to the thread, so all shared and thread-private variables. You can switch the context to another thread using the thread command, and the GDB thread ID to switch to, e.g., (gdb) thread 2 Note that it is very likely that this thread will be at another point in the source code. You can make it continue to the breakpoint using the continue command. However, all threads will be resumed, so the first thread that hit the breakpoint will continue past it. This may or may not be what you want.","title":"Switching threads"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/#scheduler-locking","text":"Although it is quite useful to debug race conditions and deadlock situations that GDB halts all threads when the first reaches a breakpoint, sometimes you just like all threads to continue to the same breakpoint in order to conveniently check the value of variables across threads. This can be achieved by turning on scheduler locking, i.e., (gdb) set scheduler-locking on Now the continue , next , step commands will only advance the current thread. Note : this can lead to a deadlock. To turn scheduler locking off again, simply use (gdb) set scheduler-locking off To check the status of scheduler locking, use the show command, i.e., (gdb) show scheduler-locking","title":"Scheduler locking"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_omp_debugging/#sending-gdb-command-to-multiple-threads","text":"You can easily execute GDB command on multiple threads at once. For instance, suppose you would like all threads to halt on a breakpoint, and compare that values of variables across threads. Suppose that thread number 1 has halted on the breakpoint, first turn on scheduler locking, and then let all threads but the first continue to the breakpoint. The total number of threads is 4. (gdb) set scheduler-locking on (gdb) thread apply 2-4 continue You can verify with info thread that all threads have halted at the breakpoint. Now we can print the value of a variable, say tid , across threads. (gdb) thread apply all print tid The keyword all can be used as a shortcut for the list of all thread IDs. The thread list to apply the command to can be one or more thread IDs, separated by spaces, and/or one or more ranges, e.g., 2-4 ; or all .","title":"Sending GDB command to multiple threads"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_reverse_debugging/","text":"Can you go back in time? Some debuggers support reverse debugging, i.e., when your application is halted at a breakpoint, you can actually take a step backward in the execution. The philosophy is that you run up to a point where there is trouble, and then step back in time to see when the problem arises. This may sound promising, and it is an interesting approach, but not without its issues. The main problem is the memory requirement of this approach. Since there is in general no way to reverse the actual computation, the state of the application has to be recorded. Imagine that one of the data structures in your application is a 100,000 by 100,000 matrix of floating point numbers, and that this matrix is modified in each time step of an iterative process, then it is easy to compute the memory requirements of storing the state for a single time step: it would be 40 billion bytes. Keeping track over multiple time steps would require an excessive amount of memory. Hence the designers of Arm DDT have decided to forego reverse debugging, since it does not scale for typical HPC applications that run thousands or even hundred thousands of processes. Another issue is that the execution time will potentially be much longer, due to the bookkeeping operations the debugger has to perform in order to keep track of the state of the application. This is an obvious concern for HPC applications, but it may already be an issue for small scale scientific applications as well. In GDB you can actually do reverse debugging, and for small scale applications it can sometimes be useful. Workflow for reverse debugging The workflow can be summarised as follows: find a location in the code that is close to the point were you know there is a problem, but where the state is still okay; set a breakpoint at that location; set a breakpoint at the earliest location where the problem manifests itself; run the application until it halts at the first breakpoint; enable reverse debugging by enabling state recording using the record command; continue executing the application until it halts at the second breakpoint; step back and inspect the state of variables to your heart's content. Note that for performance and to conserve memory, the number of statements executed between the first and second breakpoint should be minimal. The commands to go back in time are named after those to go forward in time, i.e., reverse-next reverse-step reverse-continue reverse-finish The command set exec-direction reverse changes the semantics of next , step and continue so that they step back in time. This may be convenient, but can also lead to considerable confusion. Example session Consider an application that operates on vectors, the relevant data structures and function declarations are listed below. typedef struct { double *element; int n; } Vector; Vector *init_vector(int n); void fill_vector(Vector *v, double start_value, double delta_value); void print_vector(Vector *v); The function init_vector allocates an array of length n to element , and initialises all elements to 0. The main code, lines 14 through 18, are shown below. for (int j = 0; j < nr_vectors; j++) { vectors[j] = init_vector(vector_len); fill_vector(vectors[j], j, 0.1*(j + 1.0)); print_vector(vectors[j]); } We start the application under the control of the debugger and set breakpoints at lines 15 and 17 respectively. We will start recording for reverse debugging at line 15, continue till line 17, and reverse. (gdb) b 15 Breakpoint 1 at 0xb61: file features.c, line 15. (gdb) b 17 Breakpoint 2 at 0xbc4: file features.c, line 17. (gdb) r Breakpoint 1, main () at features.c:15 15 vectors[j] = init_vector(vector_len); (gdb) record (gdb) n 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); (gdb) p vectors[j]->element[1] $1 = 0 (gdb) n Breakpoint 2, main () at features.c:17 17 print_vector(vectors[j]); (gdb) p vectors[j]->element[1] $2 = 0.10000000000000001 (gdb) reverse-next 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); (gdb) p vectors[j]->element[1] $3 = 0 Executing the reverse-next command has restored vectors[j] to the state before the function call to fill_vector .","title":"Reverse debugging with GDB"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_reverse_debugging/#can-you-go-back-in-time","text":"Some debuggers support reverse debugging, i.e., when your application is halted at a breakpoint, you can actually take a step backward in the execution. The philosophy is that you run up to a point where there is trouble, and then step back in time to see when the problem arises. This may sound promising, and it is an interesting approach, but not without its issues. The main problem is the memory requirement of this approach. Since there is in general no way to reverse the actual computation, the state of the application has to be recorded. Imagine that one of the data structures in your application is a 100,000 by 100,000 matrix of floating point numbers, and that this matrix is modified in each time step of an iterative process, then it is easy to compute the memory requirements of storing the state for a single time step: it would be 40 billion bytes. Keeping track over multiple time steps would require an excessive amount of memory. Hence the designers of Arm DDT have decided to forego reverse debugging, since it does not scale for typical HPC applications that run thousands or even hundred thousands of processes. Another issue is that the execution time will potentially be much longer, due to the bookkeeping operations the debugger has to perform in order to keep track of the state of the application. This is an obvious concern for HPC applications, but it may already be an issue for small scale scientific applications as well. In GDB you can actually do reverse debugging, and for small scale applications it can sometimes be useful.","title":"Can you go back in time?"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_reverse_debugging/#workflow-for-reverse-debugging","text":"The workflow can be summarised as follows: find a location in the code that is close to the point were you know there is a problem, but where the state is still okay; set a breakpoint at that location; set a breakpoint at the earliest location where the problem manifests itself; run the application until it halts at the first breakpoint; enable reverse debugging by enabling state recording using the record command; continue executing the application until it halts at the second breakpoint; step back and inspect the state of variables to your heart's content. Note that for performance and to conserve memory, the number of statements executed between the first and second breakpoint should be minimal. The commands to go back in time are named after those to go forward in time, i.e., reverse-next reverse-step reverse-continue reverse-finish The command set exec-direction reverse changes the semantics of next , step and continue so that they step back in time. This may be convenient, but can also lead to considerable confusion.","title":"Workflow for reverse debugging"},{"location":"BugsAtRuntime/Debuggers/Gdb/gdb_reverse_debugging/#example-session","text":"Consider an application that operates on vectors, the relevant data structures and function declarations are listed below. typedef struct { double *element; int n; } Vector; Vector *init_vector(int n); void fill_vector(Vector *v, double start_value, double delta_value); void print_vector(Vector *v); The function init_vector allocates an array of length n to element , and initialises all elements to 0. The main code, lines 14 through 18, are shown below. for (int j = 0; j < nr_vectors; j++) { vectors[j] = init_vector(vector_len); fill_vector(vectors[j], j, 0.1*(j + 1.0)); print_vector(vectors[j]); } We start the application under the control of the debugger and set breakpoints at lines 15 and 17 respectively. We will start recording for reverse debugging at line 15, continue till line 17, and reverse. (gdb) b 15 Breakpoint 1 at 0xb61: file features.c, line 15. (gdb) b 17 Breakpoint 2 at 0xbc4: file features.c, line 17. (gdb) r Breakpoint 1, main () at features.c:15 15 vectors[j] = init_vector(vector_len); (gdb) record (gdb) n 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); (gdb) p vectors[j]->element[1] $1 = 0 (gdb) n Breakpoint 2, main () at features.c:17 17 print_vector(vectors[j]); (gdb) p vectors[j]->element[1] $2 = 0.10000000000000001 (gdb) reverse-next 16 fill_vector(vectors[j], j, 0.1*(j + 1.0)); (gdb) p vectors[j]->element[1] $3 = 0 Executing the reverse-next command has restored vectors[j] to the state before the function call to fill_vector .","title":"Example session"},{"location":"BugsAtRuntime/Verification/verification_intro/","text":"How to verify correctness? Although compilers and static code checkers can detect quite a number of issues in code, some checks can only be done at runtime. Several tools are available, but for multi-threaded applications and distributed applications using MPI. OpenMP Valgrind was already mentioned before to detect memory leaks and erroneous memory accesses. However, it can also to used to verify that no deadlocks and data races occur in a multi-threaded application. Unfortunately, the DRD component of valgrind can only handle C and C++ code, and POSIX threads (the pthreads library) out of the box. Support for OpenMP using gcc and g++ is possible, but requires your own build of GCC. Hence Valgrind will not be discussed further here. Intel Inspector can detect many problems for multi-threaded code, both for POSIX threads, and OpenMP. It can handle C/C++ as well as Fortran code. Unfortunately, it is a commercial product, and requires a valid license. As usual, the code should be compiled with the -g flag in order to ensure that Intel Inspector can show the source code properly. You don't need to do anything in addition to this to use Intel Inspector. It is important to note that Intel Inspector generates a lot of overhead, both to the run time of the applications, and to the amount of memory being used. You may want to start using Intel Inspector as soon as possible in the development cycle. Intel Inspector has a GUI that that helps you to easily configure the verification that should be done, and how thorough the checks should be. The applications is started under control of Intel Inspector, and when it finishes, a report is generated and displayed. Intel Inspector can detect data races and deadlocks. MPI Just as OpenMP, MPI offers a lot of opportunities to make mistakes. When using Intel MPI as a communications library, it is possible to do runtime verification using Intel Trace Analyzer and Collector (ITAC). Besides detecting deadlocks, the VTmc library can also be linked in to check for various issues such as Buffer size or type mismatch in communication calls. Outstanding requests when the application terminates. Deadlock detection. Messages being sent, but never received. Resource leaks on data types, communicators. Using a configuration file, tests can be switched on or off. The configuration file to be used at runtime can be defined using the environment variable VT_CONFIG . If you want to use MPI checking, you have to link your application with libVTmc.so which is supplied by ITAC; optionally create a configuration file and assign its path to VT_CONFIG ; use the Intel MPI library as communications library; run the application using mpirun as usual, but with the extra option -check-mpi . Before the application starts, all checks will be listed. When a check fails at runtime, the application is halted with an appropriate error message. Below you see example output for an application that runs with several processes, all executing an MPI_Reduce . However, not all processes use the same length for the send buffer. This violates the MPI specification, may cause indeterminate behaviour, and is most likely not what you have in mind anyway. [0] ERROR: GLOBAL:COLLECTIVE:SIZE_MISMATCH: error [0] ERROR: Mismatch found in local rank [1] (global rank [1]), [0] ERROR: other processes may also be affected. [0] ERROR: No problem found in the 3 processes with local ranks [0:2:2, 3] (same as global ranks): [0] ERROR: MPI_Reduce(*sendbuf=..., *recvbuf=..., count=5, datatype=MPI_DOUBLE, op=MPI_SUM, root=0, comm=MPI_COMM_WORLD) [0] ERROR: main (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.c:21) [0] ERROR: __libc_start_main (/lib/x86_64-linux-gnu/libc-2.27.so) [0] ERROR: _start (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.exe) [0] ERROR: Root expects 5 items but 8 sent by local rank [1] (same as global rank): [0] ERROR: MPI_Reduce(*sendbuf=0xf136d0, *recvbuf=NULL, count=8, datatype=MPI_DOUBLE, op=MPI_SUM, root=0, comm=MPI_COMM_WORLD) [0] ERROR: main (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.c:21) [0] ERROR: __libc_start_main (/lib/x86_64-linux-gnu/libc-2.27.so) [0] ERROR: _start (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.exe) [0] INFO: 1 error, limit CHECK-MAX-ERRORS reached => aborting [0] WARNING: starting premature shutdown As you can see, the error message clearly states the issue, as well as the location in the code where the problem occurs. Alternative MUST is an open source correctness checker for MPI application developed by the Technical University of Aachen, Germany. If you manage to get the latest release candidate properly, it should offer some nice functionality at an attractive price.","title":"How to verify correctness?"},{"location":"BugsAtRuntime/Verification/verification_intro/#how-to-verify-correctness","text":"Although compilers and static code checkers can detect quite a number of issues in code, some checks can only be done at runtime. Several tools are available, but for multi-threaded applications and distributed applications using MPI.","title":"How to verify correctness?"},{"location":"BugsAtRuntime/Verification/verification_intro/#openmp","text":"Valgrind was already mentioned before to detect memory leaks and erroneous memory accesses. However, it can also to used to verify that no deadlocks and data races occur in a multi-threaded application. Unfortunately, the DRD component of valgrind can only handle C and C++ code, and POSIX threads (the pthreads library) out of the box. Support for OpenMP using gcc and g++ is possible, but requires your own build of GCC. Hence Valgrind will not be discussed further here. Intel Inspector can detect many problems for multi-threaded code, both for POSIX threads, and OpenMP. It can handle C/C++ as well as Fortran code. Unfortunately, it is a commercial product, and requires a valid license. As usual, the code should be compiled with the -g flag in order to ensure that Intel Inspector can show the source code properly. You don't need to do anything in addition to this to use Intel Inspector. It is important to note that Intel Inspector generates a lot of overhead, both to the run time of the applications, and to the amount of memory being used. You may want to start using Intel Inspector as soon as possible in the development cycle. Intel Inspector has a GUI that that helps you to easily configure the verification that should be done, and how thorough the checks should be. The applications is started under control of Intel Inspector, and when it finishes, a report is generated and displayed. Intel Inspector can detect data races and deadlocks.","title":"OpenMP"},{"location":"BugsAtRuntime/Verification/verification_intro/#mpi","text":"Just as OpenMP, MPI offers a lot of opportunities to make mistakes. When using Intel MPI as a communications library, it is possible to do runtime verification using Intel Trace Analyzer and Collector (ITAC). Besides detecting deadlocks, the VTmc library can also be linked in to check for various issues such as Buffer size or type mismatch in communication calls. Outstanding requests when the application terminates. Deadlock detection. Messages being sent, but never received. Resource leaks on data types, communicators. Using a configuration file, tests can be switched on or off. The configuration file to be used at runtime can be defined using the environment variable VT_CONFIG . If you want to use MPI checking, you have to link your application with libVTmc.so which is supplied by ITAC; optionally create a configuration file and assign its path to VT_CONFIG ; use the Intel MPI library as communications library; run the application using mpirun as usual, but with the extra option -check-mpi . Before the application starts, all checks will be listed. When a check fails at runtime, the application is halted with an appropriate error message. Below you see example output for an application that runs with several processes, all executing an MPI_Reduce . However, not all processes use the same length for the send buffer. This violates the MPI specification, may cause indeterminate behaviour, and is most likely not what you have in mind anyway. [0] ERROR: GLOBAL:COLLECTIVE:SIZE_MISMATCH: error [0] ERROR: Mismatch found in local rank [1] (global rank [1]), [0] ERROR: other processes may also be affected. [0] ERROR: No problem found in the 3 processes with local ranks [0:2:2, 3] (same as global ranks): [0] ERROR: MPI_Reduce(*sendbuf=..., *recvbuf=..., count=5, datatype=MPI_DOUBLE, op=MPI_SUM, root=0, comm=MPI_COMM_WORLD) [0] ERROR: main (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.c:21) [0] ERROR: __libc_start_main (/lib/x86_64-linux-gnu/libc-2.27.so) [0] ERROR: _start (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.exe) [0] ERROR: Root expects 5 items but 8 sent by local rank [1] (same as global rank): [0] ERROR: MPI_Reduce(*sendbuf=0xf136d0, *recvbuf=NULL, count=8, datatype=MPI_DOUBLE, op=MPI_SUM, root=0, comm=MPI_COMM_WORLD) [0] ERROR: main (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.c:21) [0] ERROR: __libc_start_main (/lib/x86_64-linux-gnu/libc-2.27.so) [0] ERROR: _start (/home/gjb/Documents/Projects/training-material/Debugging/Mpi/Itac/Verfification/buffer_size.exe) [0] INFO: 1 error, limit CHECK-MAX-ERRORS reached => aborting [0] WARNING: starting premature shutdown As you can see, the error message clearly states the issue, as well as the location in the code where the problem occurs.","title":"MPI"},{"location":"BugsAtRuntime/Verification/verification_intro/#alternative","text":"MUST is an open source correctness checker for MPI application developed by the Technical University of Aachen, Germany. If you manage to get the latest release candidate properly, it should offer some nice functionality at an attractive price.","title":"Alternative"},{"location":"BugsAtRuntime/Verification/Compilers/gcc_sanitize/","text":"GCC sanitizer GCC's gcc / g++ and gfortran compilers support a number of runtime checks by instrumenting your code at compile time. The workflow is straightforward: compile and link your application, letting the compiler instrument your code; run your application; and if it crashes, you get feedback on the nature of the issue. Note that the non-instrumented application doesn't always crash, it may happily run, enthusiastically producing erroneous results for you. The sanitizer can perform a variety of runtime checks, we will discuss a few of the most useful ones. The full documentation can be found in GCC's manual . Building and running your unit tests with the sanitizer enabled is a very good approach. Note: The sanitizer will display the line number of the code related to the problem at hand, provided you compile with the -g option to include debugging information into the executable. Pointer issues The first class of problems that the sanitizer tries to address is issues with pointers and arrays. The code can be instrumented using the following option: $ gcc -g -fsanitize=address ... When an array is accessed out of bounds, the program will halt, report the memory address of the illegal access, and show a stack trace. Memory leaks When using dynamic memory allocation ( malloc in C or new in C++) it is important to pair those allocations with deallocations ( free in C, delete in C++). Failing to do so may result in memory leaks, i.e., your application consumes more and more memory over time. Chances are that at some point during the execution, no more memory is available, and your application will exit if your code checks the result of the allocation, or crashes with a segmentation fault otherwise. Given the complexity of managing memory by hand, it shouldn't come as a surprise if your application has a memory leak. Again, the compiler can help you detect these at runtime by instrumenting the application. $ gcc -g -fsanitize=leak ... When the application finishes, a report is generated on the memory that is inaccessible at exit, and where in the source code this was allocated. It is then up to you to add a deallocation in the proper location to avoid the leak. Undefined behaviour The third, very broad class of issues that the sanitizer may spot is undefined behaviour. In general, these are issues that the programming language specification doesn't define the behaviour for. For example, the C specification doesn't define what should happen when an array is accessed out of bounds. GCC's documentation is a bit confusing on this topic. On the one hand, it states that -fsanitize=undefined: Enable UndefinedBehaviorSanitizer, a fast undefined behavior detector. Various computations are instrumented to detect undefined behavior at runtime. Current suboptions are: ... However, although all suboptions are equal, clearly some are more equal than others. For instance, the check for floating point division by zero is not switched on by -fsanitize=undefined . Admittedly, the documentation states this explicitly. -fsanitize=float-divide-by-zero: Detect floating-point division by zero. Unlike other similar options, -fsanitize=float-divide-by-zero is not enabled by -fsanitize=undefined, since floating-point division by zero can be a legitimate way of obtaining infinities and NaNs. You would have to specify -fsanitize=float-divide-by-zero explicitly, as well as some of the other suboptions. Read the documentation carefully. A few of the interesting undefined behaviours the sanitizer will check for are: 1. bounds : array indices out of bounds ( note: computationally cheaper than address sanitizing, but restricted to arrays only); 1. float-divide-by-zero : floating point division by zero; 1. integer-divide-by-zero : integer division by zero; 1. null : error on null pointer dereferencing, rather than segmentation fault. Note that both bounds and null are also captured by -fsanitize=address , but the former are more lightweight with respect to the instrumentation.","title":"Sanitizer instrumentation for C/C++"},{"location":"BugsAtRuntime/Verification/Compilers/gcc_sanitize/#gcc-sanitizer","text":"GCC's gcc / g++ and gfortran compilers support a number of runtime checks by instrumenting your code at compile time. The workflow is straightforward: compile and link your application, letting the compiler instrument your code; run your application; and if it crashes, you get feedback on the nature of the issue. Note that the non-instrumented application doesn't always crash, it may happily run, enthusiastically producing erroneous results for you. The sanitizer can perform a variety of runtime checks, we will discuss a few of the most useful ones. The full documentation can be found in GCC's manual . Building and running your unit tests with the sanitizer enabled is a very good approach. Note: The sanitizer will display the line number of the code related to the problem at hand, provided you compile with the -g option to include debugging information into the executable.","title":"GCC sanitizer"},{"location":"BugsAtRuntime/Verification/Compilers/gcc_sanitize/#pointer-issues","text":"The first class of problems that the sanitizer tries to address is issues with pointers and arrays. The code can be instrumented using the following option: $ gcc -g -fsanitize=address ... When an array is accessed out of bounds, the program will halt, report the memory address of the illegal access, and show a stack trace.","title":"Pointer issues"},{"location":"BugsAtRuntime/Verification/Compilers/gcc_sanitize/#memory-leaks","text":"When using dynamic memory allocation ( malloc in C or new in C++) it is important to pair those allocations with deallocations ( free in C, delete in C++). Failing to do so may result in memory leaks, i.e., your application consumes more and more memory over time. Chances are that at some point during the execution, no more memory is available, and your application will exit if your code checks the result of the allocation, or crashes with a segmentation fault otherwise. Given the complexity of managing memory by hand, it shouldn't come as a surprise if your application has a memory leak. Again, the compiler can help you detect these at runtime by instrumenting the application. $ gcc -g -fsanitize=leak ... When the application finishes, a report is generated on the memory that is inaccessible at exit, and where in the source code this was allocated. It is then up to you to add a deallocation in the proper location to avoid the leak.","title":"Memory leaks"},{"location":"BugsAtRuntime/Verification/Compilers/gcc_sanitize/#undefined-behaviour","text":"The third, very broad class of issues that the sanitizer may spot is undefined behaviour. In general, these are issues that the programming language specification doesn't define the behaviour for. For example, the C specification doesn't define what should happen when an array is accessed out of bounds. GCC's documentation is a bit confusing on this topic. On the one hand, it states that -fsanitize=undefined: Enable UndefinedBehaviorSanitizer, a fast undefined behavior detector. Various computations are instrumented to detect undefined behavior at runtime. Current suboptions are: ... However, although all suboptions are equal, clearly some are more equal than others. For instance, the check for floating point division by zero is not switched on by -fsanitize=undefined . Admittedly, the documentation states this explicitly. -fsanitize=float-divide-by-zero: Detect floating-point division by zero. Unlike other similar options, -fsanitize=float-divide-by-zero is not enabled by -fsanitize=undefined, since floating-point division by zero can be a legitimate way of obtaining infinities and NaNs. You would have to specify -fsanitize=float-divide-by-zero explicitly, as well as some of the other suboptions. Read the documentation carefully. A few of the interesting undefined behaviours the sanitizer will check for are: 1. bounds : array indices out of bounds ( note: computationally cheaper than address sanitizing, but restricted to arrays only); 1. float-divide-by-zero : floating point division by zero; 1. integer-divide-by-zero : integer division by zero; 1. null : error on null pointer dereferencing, rather than segmentation fault. Note that both bounds and null are also captured by -fsanitize=address , but the former are more lightweight with respect to the instrumentation.","title":"Undefined behaviour"},{"location":"BugsAtRuntime/Verification/Compilers/gfortran_flags/","text":"GCC Fortran compiler flags for runtime checks The gfortran compiler can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled. Floating point exceptions Floating point exception can be trapped when using the -ffpe-trap option that takes a comma-separated list of the following values: invalid : traps, e.g., sqrt(-1.0) ; overflow : numerical overflow; zero : traps division by zero; underflow : numerical underflow; inexact : likely not useful, since most floating point operations incur loss of precision; denomralized : operations on denormal values, i.e., infinity or NaN. Unless you explicitly handle floating point exceptions in your code, it may be useful to compile with trapping of the first three exceptions enabled, i.e., $ gfortran -ffpe-trap=invalid,overflow,zero ... Array bounds checks Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ gfortran -fcheck=bounds ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Other checks Several other runtime checks besides bounds can be enabled using the -fcheck option that take the value all , or a comma-separated list of the following values: do : verify that no invalid loop control variables modifications are done; mem : check implicit memory allocations; pointer : check use of pointers and allocations; recursion : checks that only procedures that are declared recursive are used recursively; array-temps : checks construction of temporary arrays (not useful for debugging, but very interesting for optimisation). Again, instrumenting your code with these runtime checks will generate overhead, so you will probably only want to enable this while developing and testing your code.","title":"Compiler instrumentation for gfortran"},{"location":"BugsAtRuntime/Verification/Compilers/gfortran_flags/#gcc-fortran-compiler-flags-for-runtime-checks","text":"The gfortran compiler can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled.","title":"GCC Fortran compiler flags for runtime checks"},{"location":"BugsAtRuntime/Verification/Compilers/gfortran_flags/#floating-point-exceptions","text":"Floating point exception can be trapped when using the -ffpe-trap option that takes a comma-separated list of the following values: invalid : traps, e.g., sqrt(-1.0) ; overflow : numerical overflow; zero : traps division by zero; underflow : numerical underflow; inexact : likely not useful, since most floating point operations incur loss of precision; denomralized : operations on denormal values, i.e., infinity or NaN. Unless you explicitly handle floating point exceptions in your code, it may be useful to compile with trapping of the first three exceptions enabled, i.e., $ gfortran -ffpe-trap=invalid,overflow,zero ...","title":"Floating point exceptions"},{"location":"BugsAtRuntime/Verification/Compilers/gfortran_flags/#array-bounds-checks","text":"Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ gfortran -fcheck=bounds ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs.","title":"Array bounds checks"},{"location":"BugsAtRuntime/Verification/Compilers/gfortran_flags/#other-checks","text":"Several other runtime checks besides bounds can be enabled using the -fcheck option that take the value all , or a comma-separated list of the following values: do : verify that no invalid loop control variables modifications are done; mem : check implicit memory allocations; pointer : check use of pointers and allocations; recursion : checks that only procedures that are declared recursive are used recursively; array-temps : checks construction of temporary arrays (not useful for debugging, but very interesting for optimisation). Again, instrumenting your code with these runtime checks will generate overhead, so you will probably only want to enable this while developing and testing your code.","title":"Other checks"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/","text":"Intel C/C++ compiler flags for runtime checks The icc / icpc compilers can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled. Floating point exceptions The compiler can instrument the code to trap floating point exceptions. The fp-trap (in main ) and -fp-trap-all (in all functions) options take the following values as a comma-separated list: invalid , divzero , overflow , common : the previous three values, denormal , underflow , inexact : probably not very useful since most floating point operations will lead to loss of accuracy, all . It may also be helpful to switch on floating point stack checking after each function call, which you can do by adding the -fp-stack-check flag. To instrument for the most common floating point exceptions, compile with: $ icc -fp-trap-all=common -fp-stack-check ... Bounds & pointer checks Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ icc -check-pointers=rw ... In the example above, both read and write access through pointers is checked at runtime. To limit checks to writes only, specify write rather than rw . Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Additionally, the compiler can also generate code to check whether dangling pointers are used, i.e., pointers to memory that has already been deallocated. $ icc -check-pointers=rw -check-pointers-dangling=all ... To check for stack buffer overruns, the icc / icpc compilers provide a specific flags as well. The -fstack-protector enables overrun checks on some types of buffers, while -fstack-protector-strong will check for all types of buffers. Additionally, the -fstack-protector-all flag will ensure checks in all functions. $ icpc -fstack-protector-strong -fstack-protector-all ... Alternatively, the options -check=stack can also be used to detect stack buffer overruns and underruns. Uninitialised variables It is good practice to explicitly initialise variables. In many circumstances, forgetting to initialise a variable can lead to interesting and random results. The compiler can instrument the code to check for uninitialised variables at runtime. When such a variable is used, your application will crash with an appropriate informative error message. $ icc -check=uninit ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Note: when the compiler flag -Wremarks is used, the compiler will also generate warnings on potentially uninitialised variables. Other checks To check for narrowing conversions at runtime, the -check=conversions option is useful.","title":"Compiler instrumentation for icc/icpc"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/#intel-cc-compiler-flags-for-runtime-checks","text":"The icc / icpc compilers can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled.","title":"Intel C/C++ compiler flags for runtime checks"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/#floating-point-exceptions","text":"The compiler can instrument the code to trap floating point exceptions. The fp-trap (in main ) and -fp-trap-all (in all functions) options take the following values as a comma-separated list: invalid , divzero , overflow , common : the previous three values, denormal , underflow , inexact : probably not very useful since most floating point operations will lead to loss of accuracy, all . It may also be helpful to switch on floating point stack checking after each function call, which you can do by adding the -fp-stack-check flag. To instrument for the most common floating point exceptions, compile with: $ icc -fp-trap-all=common -fp-stack-check ...","title":"Floating point exceptions"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/#bounds-pointer-checks","text":"Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ icc -check-pointers=rw ... In the example above, both read and write access through pointers is checked at runtime. To limit checks to writes only, specify write rather than rw . Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Additionally, the compiler can also generate code to check whether dangling pointers are used, i.e., pointers to memory that has already been deallocated. $ icc -check-pointers=rw -check-pointers-dangling=all ... To check for stack buffer overruns, the icc / icpc compilers provide a specific flags as well. The -fstack-protector enables overrun checks on some types of buffers, while -fstack-protector-strong will check for all types of buffers. Additionally, the -fstack-protector-all flag will ensure checks in all functions. $ icpc -fstack-protector-strong -fstack-protector-all ... Alternatively, the options -check=stack can also be used to detect stack buffer overruns and underruns.","title":"Bounds &amp; pointer checks"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/#uninitialised-variables","text":"It is good practice to explicitly initialise variables. In many circumstances, forgetting to initialise a variable can lead to interesting and random results. The compiler can instrument the code to check for uninitialised variables at runtime. When such a variable is used, your application will crash with an appropriate informative error message. $ icc -check=uninit ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Note: when the compiler flag -Wremarks is used, the compiler will also generate warnings on potentially uninitialised variables.","title":"Uninitialised variables"},{"location":"BugsAtRuntime/Verification/Compilers/icc_flags/#other-checks","text":"To check for narrowing conversions at runtime, the -check=conversions option is useful.","title":"Other checks"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/","text":"Intel Fortran compiler flags for runtime checks The ifort compiler can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled. Stack traces When your application crashes, you want as much information as possible, so that the problem can be fixed as soon as possible. Compiling with debugging information enabled will of course help, but the Intel compiler has an additional option to provide more useful information. $ ifort -traceback ... This traceback option will produce a stack dump that is much more informative than what is ordinarily produced by the Fortran runtime. Floating point exceptions Floating point exception can be trapped when using the -fpe-all option that takes a level as value. Level 0 will abort the application when one of the following floating point exceptions occurs: invalid, divide-by-zero, overflow. It will set underflow results to zero as well. To get more feedback on the origin of the floating point exception this options is best combined with -traceback , i.e., $ ifort -traceback -fpe-all=0 ... Array bounds checks Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ ifort -check bounds ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Uninitialised variables It is good practice to explicitly initialise variables. In some circumstances, forgetting to initialise a variable can lead to interesting and random results. The compiler can instrument the code to check for uninitialised variables at runtime. When such a variable is used, your application will crash with an appropriate informative error message. $ ifort -check uninit ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs. Other checks Besides bounds and uninit , various other runtime checks are available. All of the can be activated by specifying -check all . More specifically, the following are of interest for debugging purposes: contiguous : verifies that a pointer to contiguous data is not assigned to a non-contiguous object, e.g., by slicing; pointers : verifies that no disassociated or uninitialized pointers are used, nor unallocated allocatable objects; shape : checks array conformance, e.g., in assignments and allocate with an source argument; stack : checks the stack for buffer overrun or underrun; format and output_conversion : enables various checks on formats for output. Note : The stack check will disable all optimisations, i.e., implies -O0 so this check should never be enabled for production, and by implication, the same holds for all .","title":"Compiler instrumentation for ifort"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#intel-fortran-compiler-flags-for-runtime-checks","text":"The ifort compiler can instrument code to do a number of runtime checks. This will have a performance impact, so production code should never be compiled with these options enabled.","title":"Intel Fortran compiler flags for runtime checks"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#stack-traces","text":"When your application crashes, you want as much information as possible, so that the problem can be fixed as soon as possible. Compiling with debugging information enabled will of course help, but the Intel compiler has an additional option to provide more useful information. $ ifort -traceback ... This traceback option will produce a stack dump that is much more informative than what is ordinarily produced by the Fortran runtime.","title":"Stack traces"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#floating-point-exceptions","text":"Floating point exception can be trapped when using the -fpe-all option that takes a level as value. Level 0 will abort the application when one of the following floating point exceptions occurs: invalid, divide-by-zero, overflow. It will set underflow results to zero as well. To get more feedback on the origin of the floating point exception this options is best combined with -traceback , i.e., $ ifort -traceback -fpe-all=0 ...","title":"Floating point exceptions"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#array-bounds-checks","text":"Getting an array index wrong is easy, and trying to access an array using an index that is out of bounds usually results in a crash of your application with a segmentation fault (or not, which is most likely worse). The compiler can insert code into your application to check array bounds at runtime. When you run an application that has been compiled using this option, your application will still crash, but with an informative error message. $ ifort -check bounds ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs.","title":"Array bounds checks"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#uninitialised-variables","text":"It is good practice to explicitly initialise variables. In some circumstances, forgetting to initialise a variable can lead to interesting and random results. The compiler can instrument the code to check for uninitialised variables at runtime. When such a variable is used, your application will crash with an appropriate informative error message. $ ifort -check uninit ... Note: this compiler option should only be used for development and testing, not for production. A performance penalty is incurred since extra instructions have to be executed when your application runs.","title":"Uninitialised variables"},{"location":"BugsAtRuntime/Verification/Compilers/ifort_flags/#other-checks","text":"Besides bounds and uninit , various other runtime checks are available. All of the can be activated by specifying -check all . More specifically, the following are of interest for debugging purposes: contiguous : verifies that a pointer to contiguous data is not assigned to a non-contiguous object, e.g., by slicing; pointers : verifies that no disassociated or uninitialized pointers are used, nor unallocated allocatable objects; shape : checks array conformance, e.g., in assignments and allocate with an source argument; stack : checks the stack for buffer overrun or underrun; format and output_conversion : enables various checks on formats for output. Note : The stack check will disable all optimisations, i.e., implies -O0 so this check should never be enabled for production, and by implication, the same holds for all .","title":"Other checks"},{"location":"CodeValidation/Compilers/compiler_flags_intro/","text":"Introduction to compiler flags You would not be the first to be intimidated while browsing the documentation of a modern compiler. The number of command line flags and options easily runs into the hundreds. You can use many of these options to tune the compilation process to improve the performance of your code, but quite a number are available to generate errors and warnings whenever the compiler has misgivings about your code. In this section we will discuss compiler options that will help you find code defects. We will also illustrate that if you ignore compiler warnings, you do so at your own peril. \"All compilers are equal, but some are more equal than others.\" We will also give some examples of the advantage of using multiple compilers, since each has its strengths and its weaknesses when it comes to warning about potential bugs in your code. As general advise, you should not be satisfied unless your code compiles without a warning with any compiler you can get your hands on.","title":"Introduction to compiler flags"},{"location":"CodeValidation/Compilers/compiler_flags_intro/#introduction-to-compiler-flags","text":"You would not be the first to be intimidated while browsing the documentation of a modern compiler. The number of command line flags and options easily runs into the hundreds. You can use many of these options to tune the compilation process to improve the performance of your code, but quite a number are available to generate errors and warnings whenever the compiler has misgivings about your code. In this section we will discuss compiler options that will help you find code defects. We will also illustrate that if you ignore compiler warnings, you do so at your own peril. \"All compilers are equal, but some are more equal than others.\" We will also give some examples of the advantage of using multiple compilers, since each has its strengths and its weaknesses when it comes to warning about potential bugs in your code. As general advise, you should not be satisfied unless your code compiles without a warning with any compiler you can get your hands on.","title":"Introduction to compiler flags"},{"location":"CodeValidation/Compilers/gcc_flags/","text":"GCC C/C++ compiler flags By default, the gcc and g++ compilers issue some warnings, but it can produce more. It is always good practice to switch on \"all warnings\" with -Wall . Contrary to expectations, -Wall doesn't activate all warnings though. A number of extra warnings on top of those enabled by -Wall can be switched on by specifying -Wextra . It is highly recommended to always use these two compiler options, and eliminate all warnings reported by them, i.e., $ gcc -Wall -Wextra ... Language specification conformance The gcc compiler can also check for language specification conformity.\" The -Wpedantic flag will activate this, and you should specify the specification it should check, i.e., -std=c89 -std=c99 -std=c11 -std=c17 For g++ , these values are: -std=c++98 -std=c++11 -std=c++14 -std=c++17 For example, to check for C++14 compliance, use $ g++ ... -Wpedantic -std=c++14 ... Additional warnings It is almost always a bad idea to test floating point numbers for equality, the -Wfloat-equal will warn you about this. Although not necessarily a problem, shadowing outer scope variable declarations may lead to confusion. The -Wshadow compiler flag will alert you to this. A warning can be issued when a switch statement is based on named enum values, but not all of those are cases in the switch . To enable, compile with -Wswitch-enum . When you cast away a const qualifier from a pointer, you may inadvertently modify a value you really shouldn't. The compiler can check for this if you add the -Wcast-qual flag. For C code, the options -Wbad-function-cast will warn when the result of a function call is cast to an inappropriate type. Although some conversions from a larger to a smaller type are legal, they may be unintended, use the -Wconversion flag to be warned about these. The preprocessor can also issue warnings when required. -Wundef will warn when a macro variable that is used has not been assigned a value. -Wunused-macros will warn about macros that were defined in the main file, but never used. If-statements with many many branches are generally hard to read, and it is easy to make mistakes by repeating the same condition twice, or even duplicate code in branches. This typically results from copy/pasting code fragments. The options that help you detect such issues are -Wduplicated-cond and -Wduplicated-branches respectively. Although the option -Wmaybe-uninitialized is activated by -Wall it is worth noting that it is only enabled when the compiler also optimises the code, i.e., at least at -O1 . C++ specific warnings Specifically for C++, the -Wnon-virtual-dtor option is very useful as a class derived from a virtual base class that has no virtual destructor may lead to memory leaks. The -Woverloaded-virtual option will warn you when a virtual function in a base class is overloaded, rather than overridden in the derived class, which may not have been your intention. C++ has a safer way of performing type cast than C using static_cast , dynamic_cast and so on. It is good practice to use appropriate C++ style casting, rather than C h style casts. Warnings can be generated using -Wold-style-cast .","title":"Flags for gcc/g++"},{"location":"CodeValidation/Compilers/gcc_flags/#gcc-cc-compiler-flags","text":"By default, the gcc and g++ compilers issue some warnings, but it can produce more. It is always good practice to switch on \"all warnings\" with -Wall . Contrary to expectations, -Wall doesn't activate all warnings though. A number of extra warnings on top of those enabled by -Wall can be switched on by specifying -Wextra . It is highly recommended to always use these two compiler options, and eliminate all warnings reported by them, i.e., $ gcc -Wall -Wextra ...","title":"GCC C/C++ compiler flags"},{"location":"CodeValidation/Compilers/gcc_flags/#language-specification-conformance","text":"The gcc compiler can also check for language specification conformity.\" The -Wpedantic flag will activate this, and you should specify the specification it should check, i.e., -std=c89 -std=c99 -std=c11 -std=c17 For g++ , these values are: -std=c++98 -std=c++11 -std=c++14 -std=c++17 For example, to check for C++14 compliance, use $ g++ ... -Wpedantic -std=c++14 ...","title":"Language specification conformance"},{"location":"CodeValidation/Compilers/gcc_flags/#additional-warnings","text":"It is almost always a bad idea to test floating point numbers for equality, the -Wfloat-equal will warn you about this. Although not necessarily a problem, shadowing outer scope variable declarations may lead to confusion. The -Wshadow compiler flag will alert you to this. A warning can be issued when a switch statement is based on named enum values, but not all of those are cases in the switch . To enable, compile with -Wswitch-enum . When you cast away a const qualifier from a pointer, you may inadvertently modify a value you really shouldn't. The compiler can check for this if you add the -Wcast-qual flag. For C code, the options -Wbad-function-cast will warn when the result of a function call is cast to an inappropriate type. Although some conversions from a larger to a smaller type are legal, they may be unintended, use the -Wconversion flag to be warned about these. The preprocessor can also issue warnings when required. -Wundef will warn when a macro variable that is used has not been assigned a value. -Wunused-macros will warn about macros that were defined in the main file, but never used. If-statements with many many branches are generally hard to read, and it is easy to make mistakes by repeating the same condition twice, or even duplicate code in branches. This typically results from copy/pasting code fragments. The options that help you detect such issues are -Wduplicated-cond and -Wduplicated-branches respectively. Although the option -Wmaybe-uninitialized is activated by -Wall it is worth noting that it is only enabled when the compiler also optimises the code, i.e., at least at -O1 .","title":"Additional warnings"},{"location":"CodeValidation/Compilers/gcc_flags/#c-specific-warnings","text":"Specifically for C++, the -Wnon-virtual-dtor option is very useful as a class derived from a virtual base class that has no virtual destructor may lead to memory leaks. The -Woverloaded-virtual option will warn you when a virtual function in a base class is overloaded, rather than overridden in the derived class, which may not have been your intention. C++ has a safer way of performing type cast than C using static_cast , dynamic_cast and so on. It is good practice to use appropriate C++ style casting, rather than C h style casts. Warnings can be generated using -Wold-style-cast .","title":"C++ specific warnings"},{"location":"CodeValidation/Compilers/gfortran_flags/","text":"Options for GCC's Fortran compiler By default, the gfortran compiler issues some warnings, but it can produce more. It is always good practice to switch on \"all warnings\" with -Wall . Contrary to expectations, -Wall doesn't activate all warnings though. A number of extra warnings on top of those enabled by -Wall can be switched on by specifying -Wextra . It is highly recommended to always use these two compiler options, and eliminate all warnings reported by them, i.e., $ gfortran -Wall -Wextra ... Language specification conformance The gfortran compiler can also check for language specification conformity, at least up to some level, to quote the documentation, \"improvements to GNU Fortran in this area are welcome.\" The -Wpedantic flag will activate this, and you should specify the specification it should check, i.e., -std=f95 -std=f2003 -std=f2008 -std=f2018 For example, to check for Fortran 2003 compliance, use $ gfortran ... -Wpedantic -std=f2003 ... Additional warnings A few other options can be helpful as well. Those are not activated by -Wall and -Wextra . It is good practice to have implicit none in each compilation unit, but that is also easy to forget. The compiler has a flag, -fimplicit-none , that will verify that all variables have been declared, regardless of whether implicit none was specified. Another good practice is to explicitly declare what to use from a module in a use statement. The compiler can warn you when this has not been done if you specify the -Wuse-without-only flags. You may want to be warned if you use variables that are not uninitialised, since this could lead to nasty bugs. Use the -Wmaybe-uninitialized flag to do so (included in -Wall ). Note: this warning will only be reported when the compiler is optimising the code, i.e., when you specify at least -O1 . Without optimisation, enabling this warning would produce too many false positives. It can also be useful to verify that procedures are either intrinsic, or have been declared external explicitly. You can activate this check using the -Wimplicit-procedure flag. A potential source of bugs is unintended integer division. Computing a value such as 1/2 will yield 0, which is most likely not your intention. The compiler can detect this and issue a warning when you add the -Winteger-division flag. However, remember that this is a compile time check, so you will not get a warnings for an expression such as a/b where a and b are integer variables, but their values are the result of a computation. Some additional checks on implicit conversion between types and kinds can be activated using the -Wconversion-extra flag.","title":"Flags for gfortran"},{"location":"CodeValidation/Compilers/gfortran_flags/#options-for-gccs-fortran-compiler","text":"By default, the gfortran compiler issues some warnings, but it can produce more. It is always good practice to switch on \"all warnings\" with -Wall . Contrary to expectations, -Wall doesn't activate all warnings though. A number of extra warnings on top of those enabled by -Wall can be switched on by specifying -Wextra . It is highly recommended to always use these two compiler options, and eliminate all warnings reported by them, i.e., $ gfortran -Wall -Wextra ...","title":"Options for GCC's Fortran compiler"},{"location":"CodeValidation/Compilers/gfortran_flags/#language-specification-conformance","text":"The gfortran compiler can also check for language specification conformity, at least up to some level, to quote the documentation, \"improvements to GNU Fortran in this area are welcome.\" The -Wpedantic flag will activate this, and you should specify the specification it should check, i.e., -std=f95 -std=f2003 -std=f2008 -std=f2018 For example, to check for Fortran 2003 compliance, use $ gfortran ... -Wpedantic -std=f2003 ...","title":"Language specification conformance"},{"location":"CodeValidation/Compilers/gfortran_flags/#additional-warnings","text":"A few other options can be helpful as well. Those are not activated by -Wall and -Wextra . It is good practice to have implicit none in each compilation unit, but that is also easy to forget. The compiler has a flag, -fimplicit-none , that will verify that all variables have been declared, regardless of whether implicit none was specified. Another good practice is to explicitly declare what to use from a module in a use statement. The compiler can warn you when this has not been done if you specify the -Wuse-without-only flags. You may want to be warned if you use variables that are not uninitialised, since this could lead to nasty bugs. Use the -Wmaybe-uninitialized flag to do so (included in -Wall ). Note: this warning will only be reported when the compiler is optimising the code, i.e., when you specify at least -O1 . Without optimisation, enabling this warning would produce too many false positives. It can also be useful to verify that procedures are either intrinsic, or have been declared external explicitly. You can activate this check using the -Wimplicit-procedure flag. A potential source of bugs is unintended integer division. Computing a value such as 1/2 will yield 0, which is most likely not your intention. The compiler can detect this and issue a warning when you add the -Winteger-division flag. However, remember that this is a compile time check, so you will not get a warnings for an expression such as a/b where a and b are integer variables, but their values are the result of a computation. Some additional checks on implicit conversion between types and kinds can be activated using the -Wconversion-extra flag.","title":"Additional warnings"},{"location":"CodeValidation/Compilers/icc_flags/","text":"Intel C/C++ compiler flags Although the GCC compiler suite is quite powerful, it may be worth your while to look into the Intel compilers as well. Often they deliver executables that are better optimised than those produced by GCC. Note however that the Intel compilers are a commercial product, although you can (at the time of writing) obtain a free license for developing open source software or for training purposes. Note: you may want to read the terms of the various Intel license agreements quite carefully. Some of the compiler flags available for gcc / g++ will work with icc / icpc , Intel's C and C++ compilers. However, we will point out some differences here. Generating warnings The first order of business is to ensure that your code compiles without warnings. To switch on a lot of warnings with one convenient flag, you can use the same flag as for GCC, i.e., $ icc -Wall ... Even more warnings can be switched on by: $ icc -Wall -Wremarks -Wchecks -w3 ... Floating point model The Intel compilers will optimise more aggressively than their GCC counterparts when the -O2 flag is specified (incidentally, this is the default for Intel compilers). A notable difference is the floating model being used. At -O2 the Intel compiler is free to make some assumptions that allow optimisations of your code by, e.g., using commutativity, distributive property and associativity of operators. Although these properties strictly hold for real numbers, they do not for operations on floating point numbers. This may give rise to different, and potentially erroneous results. Hence it is good practice to verify results based on (non-trivial) floating point computations enforcing a floating point model that is faithful to the source code. $ icc -fp-model source ... This will ensure that no \"adventurous\" optimisations involving floating point operations are carried out. The result obtained with an application compiled this way should be compared to one with the default floating point value for verification. Note: For numerical intensive code, this option will severely degrade performance, and hence should be used for verification only.","title":"Flags for icc/icpc"},{"location":"CodeValidation/Compilers/icc_flags/#intel-cc-compiler-flags","text":"Although the GCC compiler suite is quite powerful, it may be worth your while to look into the Intel compilers as well. Often they deliver executables that are better optimised than those produced by GCC. Note however that the Intel compilers are a commercial product, although you can (at the time of writing) obtain a free license for developing open source software or for training purposes. Note: you may want to read the terms of the various Intel license agreements quite carefully. Some of the compiler flags available for gcc / g++ will work with icc / icpc , Intel's C and C++ compilers. However, we will point out some differences here.","title":"Intel C/C++ compiler flags"},{"location":"CodeValidation/Compilers/icc_flags/#generating-warnings","text":"The first order of business is to ensure that your code compiles without warnings. To switch on a lot of warnings with one convenient flag, you can use the same flag as for GCC, i.e., $ icc -Wall ... Even more warnings can be switched on by: $ icc -Wall -Wremarks -Wchecks -w3 ...","title":"Generating warnings"},{"location":"CodeValidation/Compilers/icc_flags/#floating-point-model","text":"The Intel compilers will optimise more aggressively than their GCC counterparts when the -O2 flag is specified (incidentally, this is the default for Intel compilers). A notable difference is the floating model being used. At -O2 the Intel compiler is free to make some assumptions that allow optimisations of your code by, e.g., using commutativity, distributive property and associativity of operators. Although these properties strictly hold for real numbers, they do not for operations on floating point numbers. This may give rise to different, and potentially erroneous results. Hence it is good practice to verify results based on (non-trivial) floating point computations enforcing a floating point model that is faithful to the source code. $ icc -fp-model source ... This will ensure that no \"adventurous\" optimisations involving floating point operations are carried out. The result obtained with an application compiled this way should be compared to one with the default floating point value for verification. Note: For numerical intensive code, this option will severely degrade performance, and hence should be used for verification only.","title":"Floating point model"},{"location":"CodeValidation/Compilers/ifort_flags/","text":"Intel Fortran compiler flags Although the GCC compiler suite is quite powerful, it may be worth your while to look into the Intel compilers as well. Often they deliver executables that are better optimised than those produced by GCC. Note however that the Intel compilers are a commercial product, although you can (at the time of writing) obtain a free license for developing open source software or for training purposes. Note: you may want to read the terms of the various Intel license agreements quite carefully. Some of the compiler flags available for gfortran will work with ifort , Intel's Fortran compiler. However, we will point out some differences here. Generating warnings The first order of business is to ensure that your code compiles without warnings. To switch on a lot of warnings with one convenient flag, you can use: $ ifort -warn all ... Even more warnings can be switched on by: $ ifort -warn all -diag-enable remark ... Language specification conformance The current Intel Fortran compiler is heir to a long lineage of compilers that implemented various extensions to the Fortran specifications over the decades. This implies that if you, intentionally or otherwise, use such an extension, your code may not compile using other Fortran compilers, or that the semantics for these extensions differ subtly between compiler implementations. It is good practice to adhere strictly to the standards, and the compiler can enforce that if you instruct it to. The standard is specified as an argument, i.e., f90/f95/f03/f08/f15 for Fortran 90, 95, 2003, 2008 and 2015 respectively, e.g., $ ifort -stand f08 ... Regardless of this option, it is good practice to ensure that neither the Intel, nor the GCC compiler generates warnings on your code, so compiling with both during development is definitely a good idea. Additional warnings To disable Fortran implicit typing, it is good practice to add an implicit none statement at the start of each compilation unit. Needless to say, this is easy to forget, so it is good practice to compile your code with a flag that disables implicit typing for all code. $ ifort -implicit-none ... Floating point model The Intel compilers will optimise more aggressively than their GCC counterparts when the -O2 flag is specified (incidentally, this is the default for Intel compilers). A notable difference is the floating model being used. At -O2 the Intel compiler is free to make some assumptions that allow optimisations of your code by, e.g., using commutativity, distributive property and associativity of operators. Although these properties strictly hold for real numbers, they do not for operations on floating point numbers. This may give rise to different, and potentially erroneous results. Hence it is good practice to verify results based on (non-trivial) floating point computations enforcing a floating point model that is faithful to the source code. $ ifort -fp-model source ... This will ensure that no \"adventurous\" optimisations involving floating point operations are carried out. The result obtained with an application compiled this way should be compared to one with the default floating point value for verification. Note: For numerical intensive code, this option will severely degrade performance, and hence should be used for verification only.","title":"Flags for ifort"},{"location":"CodeValidation/Compilers/ifort_flags/#intel-fortran-compiler-flags","text":"Although the GCC compiler suite is quite powerful, it may be worth your while to look into the Intel compilers as well. Often they deliver executables that are better optimised than those produced by GCC. Note however that the Intel compilers are a commercial product, although you can (at the time of writing) obtain a free license for developing open source software or for training purposes. Note: you may want to read the terms of the various Intel license agreements quite carefully. Some of the compiler flags available for gfortran will work with ifort , Intel's Fortran compiler. However, we will point out some differences here.","title":"Intel Fortran compiler flags"},{"location":"CodeValidation/Compilers/ifort_flags/#generating-warnings","text":"The first order of business is to ensure that your code compiles without warnings. To switch on a lot of warnings with one convenient flag, you can use: $ ifort -warn all ... Even more warnings can be switched on by: $ ifort -warn all -diag-enable remark ...","title":"Generating warnings"},{"location":"CodeValidation/Compilers/ifort_flags/#language-specification-conformance","text":"The current Intel Fortran compiler is heir to a long lineage of compilers that implemented various extensions to the Fortran specifications over the decades. This implies that if you, intentionally or otherwise, use such an extension, your code may not compile using other Fortran compilers, or that the semantics for these extensions differ subtly between compiler implementations. It is good practice to adhere strictly to the standards, and the compiler can enforce that if you instruct it to. The standard is specified as an argument, i.e., f90/f95/f03/f08/f15 for Fortran 90, 95, 2003, 2008 and 2015 respectively, e.g., $ ifort -stand f08 ... Regardless of this option, it is good practice to ensure that neither the Intel, nor the GCC compiler generates warnings on your code, so compiling with both during development is definitely a good idea.","title":"Language specification conformance"},{"location":"CodeValidation/Compilers/ifort_flags/#additional-warnings","text":"To disable Fortran implicit typing, it is good practice to add an implicit none statement at the start of each compilation unit. Needless to say, this is easy to forget, so it is good practice to compile your code with a flag that disables implicit typing for all code. $ ifort -implicit-none ...","title":"Additional warnings"},{"location":"CodeValidation/Compilers/ifort_flags/#floating-point-model","text":"The Intel compilers will optimise more aggressively than their GCC counterparts when the -O2 flag is specified (incidentally, this is the default for Intel compilers). A notable difference is the floating model being used. At -O2 the Intel compiler is free to make some assumptions that allow optimisations of your code by, e.g., using commutativity, distributive property and associativity of operators. Although these properties strictly hold for real numbers, they do not for operations on floating point numbers. This may give rise to different, and potentially erroneous results. Hence it is good practice to verify results based on (non-trivial) floating point computations enforcing a floating point model that is faithful to the source code. $ ifort -fp-model source ... This will ensure that no \"adventurous\" optimisations involving floating point operations are carried out. The result obtained with an application compiled this way should be compared to one with the default floating point value for verification. Note: For numerical intensive code, this option will severely degrade performance, and hence should be used for verification only.","title":"Floating point model"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/","text":"clang-tidy Cppcheck is a nice and useful static code analyser, but clang-tidy is definitely worth mentioning as an alternative. It is part of the clang toolchain that compiles C ( clang ) and C++ ( clang++ ) code against the LLVM backend. clang is still not as popular on HPC systems as the GNU and Intel families of compilers, but for debugging, it can be a valuable tool. The warnings generated by clang and clang++ are pretty thorough. To get to know everything the compilers can warn about, you can use a single flag -Weverything (easy to remember too). However, don't use this in production, as you may get warnings that are inconsistent with other command line options. The -Weverything is in fact only intended for compiler developers. clang-tidy is a very useful tool, part of the clang toolchain, and can detect additional problems that the compilers won't warn about. A drawback is that to use clang-tidy , you first have to create a compilation database. This is easy if CMake is your build tool. Since an introduction to CMake is out of scope for this course, there are two options: 1. you are familiar with CMake, just read on; 1. you are not familiar with CMake, feel free to treat this as optional material, and skip it. How do you set it up? If CMake is your build tool, then generating a compilation database for clang-tidy is trivial: $ cmake -DCMAKE_EXPORT_COMPILER_COMMANDS=on path/to/source This generates a file compile_commands.json , the compilation database for the project. What does it check? clang-tidy implements many checks, and not all are useful in your context. To see which checks are enabled, simply run clang-tidy with the -list-checks options, i.e., $ clang-tidy -list-checks You will see that by default, only checks with the prefix clang-analyzer are enabled, in total some 80 checks at the time of writing. However, many more checks can be enable in addition to these 80 core checks. To view all available checks, enable them all. $ clang-tidy -checks='*' -list-checks You will recognize check categories, * android: checks related to Android; not useful for HPC; * bugprone: checks that target bugprone code constructs; * cert: checks related to CERT Secure Coding Guidelines; * clang-analyzer: Clang Static Analyzer checks; * cppcoreguidelines: checks related to C++ Core Guidelines; * fuchsia: checks related to Fuchsia coding conventions; not useful for HPC; * google: checks related to Google style guide; * hicpp: checks related to High Integrity C++ Coding Standard; * llvm: checks related to the LLVM coding conventions; * misc: checks that they didn\u2019t have a better category for; * modernize: checks that advocate usage of modern (currently \u201cmodern\u201d means \u201cC++11\u201d) language constructs; * mpi: checks related to MPI (Message Passing Interface); * objc: checks related to Objective-C coding conventions; not useful for HPC, unless you use Objective-C; * performance: checks that target performance-related issues; * readability: checks that target readability-related issues that don\u2019t relate to any particular coding style. Some of these categories are quite useful to catch bugs as you can guess by their names, e.g., bugprone , clang-analyzer . Other categories focus more on coding style, e.g., google and readability . Yet others are only useful in a very specific context, e.g., android if you happen to develop for the Android platform, or mpi for distributed parallel programming using an MPI library. Enabling checks is based on globbing, e.g., to enable the readability checks, you would use -checks='read*' . Note that the clang-analyzer checks are enabled as well. The simplest approach is to enable all checks, i.e., -check='*' . However, this may take some time for large projects, and some tests are certainly useless for your project. Although you could specify the checks explicitly on the command line, that would be error prone and inconvenient. A more convenient approach is to use a configuration file. clang-tidy can generate one for you that you can modify later, e.g., to have a configuration file that enables the readability checks, use $ clang-tidy -checks='read*' -dump-config The output looks like the following -- Checks: 'clang-diagnostic-*,clang-analyzer-*,read*' WarningsAsErrors: '' HeaderFilterRegex: '' AnalyzeTemporaryDtors: false FormatStyle: none User: gjb CheckOptions: - key: google-readability-braces-around-statements.ShortStatementLines value: '1' - key: google-readability-function-size.StatementThreshold value: '800' ... Saving this output into a file .clang-tidy will ensure that this configuration is used when you run clang-tidy . By modifying the Checks line, you can enable additional checks. How does it report issues? The output clang-tidy generates is very similar to that of a compiler, e.g., $ clang-tidy array.c 1 warning generated. /home/gjb/training-material/Debugging/ClangTidy/array.c:8:11: warning: The left operand of '>' is a garbage value [clang-analyzer-core.UndefinedBinaryOperatorResult] if (m > n) { ^ /home/gjb/training-material/Debugging/ClangTidy/array.c:7:5: note: 'm' declared without an initial value int m; ^ /home/gjb/training-material/Debugging/ClangTidy/array.c:8:11: note: The left operand of '>' is a garbage value if (m > n) { ^ As with all verification tools, it pays to use them from the start of your project. It can be both intimidating and depressing to be confronted with clang-tidy output for a large code base. Can it do more? Indeed, actually clang-tidy can fix your code in some circumstances. Caveat emptor, it is an automatic tool, so I would advice using it with care. For instance, using the -fix option on the example above would insert braces around the body of the for-loop. $ clang-tidy -fix -checks='google*' array.c","title":"Clang Tidy"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/#clang-tidy","text":"Cppcheck is a nice and useful static code analyser, but clang-tidy is definitely worth mentioning as an alternative. It is part of the clang toolchain that compiles C ( clang ) and C++ ( clang++ ) code against the LLVM backend. clang is still not as popular on HPC systems as the GNU and Intel families of compilers, but for debugging, it can be a valuable tool. The warnings generated by clang and clang++ are pretty thorough. To get to know everything the compilers can warn about, you can use a single flag -Weverything (easy to remember too). However, don't use this in production, as you may get warnings that are inconsistent with other command line options. The -Weverything is in fact only intended for compiler developers. clang-tidy is a very useful tool, part of the clang toolchain, and can detect additional problems that the compilers won't warn about. A drawback is that to use clang-tidy , you first have to create a compilation database. This is easy if CMake is your build tool. Since an introduction to CMake is out of scope for this course, there are two options: 1. you are familiar with CMake, just read on; 1. you are not familiar with CMake, feel free to treat this as optional material, and skip it.","title":"clang-tidy"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/#how-do-you-set-it-up","text":"If CMake is your build tool, then generating a compilation database for clang-tidy is trivial: $ cmake -DCMAKE_EXPORT_COMPILER_COMMANDS=on path/to/source This generates a file compile_commands.json , the compilation database for the project.","title":"How do you set it up?"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/#what-does-it-check","text":"clang-tidy implements many checks, and not all are useful in your context. To see which checks are enabled, simply run clang-tidy with the -list-checks options, i.e., $ clang-tidy -list-checks You will see that by default, only checks with the prefix clang-analyzer are enabled, in total some 80 checks at the time of writing. However, many more checks can be enable in addition to these 80 core checks. To view all available checks, enable them all. $ clang-tidy -checks='*' -list-checks You will recognize check categories, * android: checks related to Android; not useful for HPC; * bugprone: checks that target bugprone code constructs; * cert: checks related to CERT Secure Coding Guidelines; * clang-analyzer: Clang Static Analyzer checks; * cppcoreguidelines: checks related to C++ Core Guidelines; * fuchsia: checks related to Fuchsia coding conventions; not useful for HPC; * google: checks related to Google style guide; * hicpp: checks related to High Integrity C++ Coding Standard; * llvm: checks related to the LLVM coding conventions; * misc: checks that they didn\u2019t have a better category for; * modernize: checks that advocate usage of modern (currently \u201cmodern\u201d means \u201cC++11\u201d) language constructs; * mpi: checks related to MPI (Message Passing Interface); * objc: checks related to Objective-C coding conventions; not useful for HPC, unless you use Objective-C; * performance: checks that target performance-related issues; * readability: checks that target readability-related issues that don\u2019t relate to any particular coding style. Some of these categories are quite useful to catch bugs as you can guess by their names, e.g., bugprone , clang-analyzer . Other categories focus more on coding style, e.g., google and readability . Yet others are only useful in a very specific context, e.g., android if you happen to develop for the Android platform, or mpi for distributed parallel programming using an MPI library. Enabling checks is based on globbing, e.g., to enable the readability checks, you would use -checks='read*' . Note that the clang-analyzer checks are enabled as well. The simplest approach is to enable all checks, i.e., -check='*' . However, this may take some time for large projects, and some tests are certainly useless for your project. Although you could specify the checks explicitly on the command line, that would be error prone and inconvenient. A more convenient approach is to use a configuration file. clang-tidy can generate one for you that you can modify later, e.g., to have a configuration file that enables the readability checks, use $ clang-tidy -checks='read*' -dump-config The output looks like the following -- Checks: 'clang-diagnostic-*,clang-analyzer-*,read*' WarningsAsErrors: '' HeaderFilterRegex: '' AnalyzeTemporaryDtors: false FormatStyle: none User: gjb CheckOptions: - key: google-readability-braces-around-statements.ShortStatementLines value: '1' - key: google-readability-function-size.StatementThreshold value: '800' ... Saving this output into a file .clang-tidy will ensure that this configuration is used when you run clang-tidy . By modifying the Checks line, you can enable additional checks.","title":"What does it check?"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/#how-does-it-report-issues","text":"The output clang-tidy generates is very similar to that of a compiler, e.g., $ clang-tidy array.c 1 warning generated. /home/gjb/training-material/Debugging/ClangTidy/array.c:8:11: warning: The left operand of '>' is a garbage value [clang-analyzer-core.UndefinedBinaryOperatorResult] if (m > n) { ^ /home/gjb/training-material/Debugging/ClangTidy/array.c:7:5: note: 'm' declared without an initial value int m; ^ /home/gjb/training-material/Debugging/ClangTidy/array.c:8:11: note: The left operand of '>' is a garbage value if (m > n) { ^ As with all verification tools, it pays to use them from the start of your project. It can be both intimidating and depressing to be confronted with clang-tidy output for a large code base.","title":"How does it report issues?"},{"location":"CodeValidation/StaticCodeAnalyzers/clang_tidy/#can-it-do-more","text":"Indeed, actually clang-tidy can fix your code in some circumstances. Caveat emptor, it is an automatic tool, so I would advice using it with care. For instance, using the -fix option on the example above would insert braces around the body of the for-loop. $ clang-tidy -fix -checks='google*' array.c","title":"Can it do more?"},{"location":"CodeValidation/StaticCodeAnalyzers/static_code_analyzers_intro/","text":"Introduction to static code checkers For some programming languages such as C, C++ and Python among others, we can use tools that analyse our source code to find potential defects. These tools work by analysing the source code files, looking for patterns in the code that may indicate issues. Static analysers can also be used to report violations of style conventions and best practices for the target language. They offer considerable help to ensure that our code is consistenly formatted, clean, and idiomatic. For an interpreted language such as Python, static code analysers such as Pylint and Flake8 are a great help. Programming errors only occur at runtime, so you easily use a lot of time by having to run and rerun your code until they have all been fixed. This can be time consuming, and you are never certain that all code paths have been executed. Using a static analyser will detect at least a number of common defects, saving you quite some time. For compiled languages such as Fortran, C and C++, the compiler usually does a good job detecting many of the problems a static analyser would report, but these tools will still alert you to issues that are missed by the compiler. For C and C++, many static analysers are available, some as commercial products, others as free and open source software. Here we will discuss Cppcheck and illustrate some of its capabilities that help us improve our coding style and detect bugs. It will report some issues the compilers will typically miss, so it is worth being added to your development toolchain. Cppcheck also provides some advice on improving the performance of your code, but that is outside the scope of this training.","title":"Introduction to static code checkers"},{"location":"CodeValidation/StaticCodeAnalyzers/static_code_analyzers_intro/#introduction-to-static-code-checkers","text":"For some programming languages such as C, C++ and Python among others, we can use tools that analyse our source code to find potential defects. These tools work by analysing the source code files, looking for patterns in the code that may indicate issues. Static analysers can also be used to report violations of style conventions and best practices for the target language. They offer considerable help to ensure that our code is consistenly formatted, clean, and idiomatic. For an interpreted language such as Python, static code analysers such as Pylint and Flake8 are a great help. Programming errors only occur at runtime, so you easily use a lot of time by having to run and rerun your code until they have all been fixed. This can be time consuming, and you are never certain that all code paths have been executed. Using a static analyser will detect at least a number of common defects, saving you quite some time. For compiled languages such as Fortran, C and C++, the compiler usually does a good job detecting many of the problems a static analyser would report, but these tools will still alert you to issues that are missed by the compiler. For C and C++, many static analysers are available, some as commercial products, others as free and open source software. Here we will discuss Cppcheck and illustrate some of its capabilities that help us improve our coding style and detect bugs. It will report some issues the compilers will typically miss, so it is worth being added to your development toolchain. Cppcheck also provides some advice on improving the performance of your code, but that is outside the scope of this training.","title":"Introduction to static code checkers"},{"location":"CodingBestPractices/coding_best_practices/","text":"Coding best practices: reading material A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion. Format your code nicely To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, like your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. Whichever convention you follow, be consistent! Use language idioms Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write REAL, DIMENSION(10) :: a ... a = value rather than INTEGER :: i REAL, DIMENSION(10) :: a ... DO i = 1, 10 a(i) = value END DO Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language. Choose descriptive names In a way, programming is storytelling. The data are the protagonist in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favour of short but less descriptive names to save typing. A long but descriptive name is just a tab character away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise. Choosing appropriate names for our variables and functions helps a lot in this respect. Keep it simple Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures, keep the number of arguments limited. However, Fortran supports using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature. Limit scope Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99 and C++ allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., PROGRAM , FUNCTION , SUBROUTINE , MODULE , but Fortran 2008 introduced the BLOCK statement in which local variables can be declared. Their scope doesn't extend beyond the BLOCK . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be INTEGER if its starts with the characters i to n , otherwise its type will be REAL . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as INTEGER and total as REAL . However, the misspelled totl is also implicitly typed as REAL , initialised to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. INTEGER :: i REAL :: total DO i = 1, 10 total = totl + 10.0 END DO To avoid these problems caused by simple typos, use the IMPLICIT NONE statement before variable declarations in PROGRAM , MODULE , FUNCTION , SUBROUTINE , and BLOCK , e.g, IMPLICIT NONE INTEGER :: i REAL :: total DO i = 1, 10 total = totl + 10.0 END DO The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines. Be explicit about constants If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use PARAMETER . If arguments passed to function should be read-only, use const in C/C++ code, and INTENT(IN) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines. Control access When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public Variable initialisation The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialise variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialise a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialised variables. We will discuss these and other compiler flags in a later section. When initialising or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use list initialisation, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialised to 7 without any warnings, while the compiler will generate a warning for the initialisation of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; } To comment or not to comment? Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials. Stick to the standard The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are three specifications that are still relevant, C89, C99 and C11. For C++ that would be C++11, C++14 and C++17. The relevant specification for Fortran are those of 2003 and 2008. References to those specifications can be found in the section on additional material. For C, you may be interested to read the MISRA C software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the additional material section. Copy/paste is evil If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only.","title":"Coding style"},{"location":"CodingBestPractices/coding_best_practices/#coding-best-practices-reading-material","text":"A number of very simple things go a long way towards improving your code substantially. For good programmers, they are second nature, and you should strive to make them a habit. In this section, we will use the term function in a very broad sense, simply to keep the text easy to read. In the context of Fortran, a function refers to a program unit, any procedure, either a function or a subroutine. It also refers to a method defined in a class. In the context of C++, we will use the term function for methods as well. Similarly, we use the term variable for constants, and also for attributes of objects and classes, whenever that doesn't lead to confusion.","title":"Coding best practices: reading material"},{"location":"CodingBestPractices/coding_best_practices/#format-your-code-nicely","text":"To quote Robert C. Martin, \"Code formatting is about communication, and communication is the professional developer\u2019s first order of business\". All programming languages have one or possibly multiple conventions about how to format source code. For example, consistent indentation of code helps considerably to assess the structure of a function at a glance. For multi-level indentation, always use the same width, e.g., multiples of four spaces. The convention you have to use is often determined by the community you are working with, like your co-workers. It is best to stick to that convention, since coding is communicating. If no convention is established, consider introducing one. The one which is prevalent in the programming language community is most likely to be your best choice. Whichever convention you follow, be consistent!","title":"Format your code nicely"},{"location":"CodingBestPractices/coding_best_practices/#use-language-idioms","text":"Linguists use the term \"idiom\" for an expression that is very specific to a certain language and that cannot be translated literally to another. For instance, the English idiom \"it is raining cats and dogs\" would translate to \"il pleut des cordes\" in French. The corresponding idiom in French is completely unrelated to its counterpart in English. Mastering idioms is one of the requirements for C1 certification, i.e., to be considered to have a proficiency close to that of native speakers. We observe a similar phenomenon for programming languages. Some syntactic constructs are typical for a specific programming language but, when translated one-to-one into another language, lead to code constructs that are unfamiliar to programmers who are proficient in that language. The code fragments below illustrate this for Fortran and C. Although you could write line 4 of the C function below in this way, you most likely wouldn't since it is not idiomatic C. int factorial(int n) { fac = 1; for (int i = 2; i <= n; i++) fac = fac*i; return fac; } The idiomatic formulation of line 4 would be fac *= i . In Fortran for example, you would write REAL, DIMENSION(10) :: a ... a = value rather than INTEGER :: i REAL, DIMENSION(10) :: a ... DO i = 1, 10 a(i) = value END DO Using idioms, i.e., expressions that are particular to a (programming) language, will make your code much easier to interpret correctly by programmers that are fluent in that language.","title":"Use language idioms"},{"location":"CodingBestPractices/coding_best_practices/#choose-descriptive-names","text":"In a way, programming is storytelling. The data are the protagonist in the story, and the functions are the actions they take, or what happens to them. Hence variable names should be nouns and functions names should be verbs. If a function returns a property, it should be phrased as a question. Any editor worth its salt provides completion, so you can't argue in favour of short but less descriptive names to save typing. A long but descriptive name is just a tab character away. Choosing descriptive names for variables and functions is another aspect that can make reading your code much easier. Consider the following pseudo-code fragment, and although I'll grant that it is something of a caricature, I've seen some in the wild that are not significantly better. f = open(fn, 'r') for i in f: x = get(i) if condition(x): a = compute(x) if a < 3.14: do_something(a) f.close() A key principle of good software design is that of the least surprise. Choosing appropriate names for our variables and functions helps a lot in this respect.","title":"Choose descriptive names"},{"location":"CodingBestPractices/coding_best_practices/#keep-it-simple","text":"Ideally, code is simple. A function should have two levels of indentation at most. This is advice you'll find in the literature on general purpose programming. Although this is good advice, there are some caveats in the context of scientific computing. However, the gist is clear: code is as simple as possible, but not simpler. Even for scientific code, a function has no more lines of code than fit comfortably on your screen. It is all too easy to lose track of the semantics if you can't get an overview of the code. Remember, not everyone has the budget for a 5K monitor. If you find yourself writing a very long code fragment, ask yourself whether that is atomic, or whether the task it represents can be broken up into sub-tasks. If so, and that is very likely, introduce new functions for those sub-tasks with descriptive names. This will make the narrative all the easier to understand. A function should have a single purpose, i.e., you should design it to do one thing, and one thing only. For function signatures, simplicity matters as well. Functions that take many arguments may lead to confusion. In C and C++, you have to remember the order of the function arguments. Accidentally swapping argument values with the same type in a function call can lead to interesting debugging sessions. The same advice applies to Fortran procedures, keep the number of arguments limited. However, Fortran supports using keyword arguments, a nice feature that makes your code more robust. Consider the following procedure signature: real function random_gaussian(mu, sigma) implicit none real, intent(in) :: mu, sigma ... end function random_gaussian You would have to check the documentation to know the order of the function arguments. Consider the following four function calls: random_gaussian(0.0, 1.0) : okay; random_gaussian(1.0, 0.0) : not okay; random_gaussian(mu=0.0, sigma=1.0) : okay; random_gaussian(sigma=1.0, mu=0.0) : okay. The two last versions of this call are easier to understand, since the meaning of the numbers is clear. Moreover, since you can use any order, it eliminates a source of bugs. Unfortunately, neither C nor C++ support this feature.","title":"Keep it simple"},{"location":"CodingBestPractices/coding_best_practices/#limit-scope","text":"Many programmers will declare all variables at the start of a block, or even at the start of a function's implementation. This is a syntax requirement in C89 and Fortran. However, C99 and C++ allow you to declare variables anywhere before their first use. Since the scope of a variable starts from its declaration, and extends throughout the block, that means it is in fact too wide. Limiting the scope of declarations to a minimum reduces the probability of inadvertently using the variable, but it also improves code quality: the declaration of the variable is at the same location where the variable is first used, so the narrative is easier to follow. In C++ this may even have performance benefits since a declaration may trigger a call to a potentially expensive constructor. Fortran requires that variables are declared at the start of a compilation unit, i.e., PROGRAM , FUNCTION , SUBROUTINE , MODULE , but Fortran 2008 introduced the BLOCK statement in which local variables can be declared. Their scope doesn't extend beyond the BLOCK . Modern compilers support this Fortran 2008 feature. Note that Fortran still allows variables to be implicitly typed, i.e., if you don't declare a variable explicitly, its type will be INTEGER if its starts with the characters i to n , otherwise its type will be REAL . Consider the code fragment below. Since the variables were not declared explicitly, i is interpreted as INTEGER and total as REAL . However, the misspelled totl is also implicitly typed as REAL , initialised to 0.0 , and hence the value of total will be 10.0 when the iterations ends, rather than 100.0 as was intended. INTEGER :: i REAL :: total DO i = 1, 10 total = totl + 10.0 END DO To avoid these problems caused by simple typos, use the IMPLICIT NONE statement before variable declarations in PROGRAM , MODULE , FUNCTION , SUBROUTINE , and BLOCK , e.g, IMPLICIT NONE INTEGER :: i REAL :: total DO i = 1, 10 total = totl + 10.0 END DO The compiler would give an error for the code fragment above since all variables have to be declared explicitly, and totl was not. Limiting scope of of declarations extends to headers files that are included in C/C++. It is recommended not to include files that are not required. Not only will it pollute the namespace with clutter, but it will also increase build times. In C++, you can importing everything defined in a namespace, e.g., using namespace std; Although it saves on typing, it is better to either use the namespace prefix explicitly, or use only what is required, e.g., using std::cout; using std::endl; In Fortran it is also possible to restrict what to use from modules, e.g., use, intrinsic :: iso_fortran_env, only : REAL64, INT32 The only keyword ensures that only the parameters REAL64 and INT32 are imported from the iso_fortran_env module. Note that the intrinsic keyword is used to ensure that the compiler supplied module is used, and not a module with the same name defined by you. When developing multi-threaded C/C++ programs using OpenMP, limiting the scope of variables to parallel regions makes those variables thread-private, hence reducing the risk of data races. We will discuss this in more detail in a later section. Unfortunately, the semantics for the Fortran block statement in an OpenMP do loop is not defined, at least up to the OpenMP 4.5 specification. Although gfortran accepts such code constructs, and seems to generate code with the expected behavior, it should be avoided since Intel Fortran compiler will report an error for such code. This recommendation is mentioned in the C++ core guidelines.","title":"Limit scope"},{"location":"CodingBestPractices/coding_best_practices/#be-explicit-about-constants","text":"If a variable's value is not supposed to change during the run time of a program, declare it as a constant, so that the compiler will warn you if you inadvertently modify its value. In C/C++, use the const qualifier, in Fortran, use PARAMETER . If arguments passed to function should be read-only, use const in C/C++ code, and INTENT(IN) in Fortran. Although Fortran doesn't require that you state the intent of arguments passed to procedures, it is nevertheless wise to do so. The compiler will catch at least some programming mistakes if you do. However, this is not quite watertight, in fact, one can still change the value of a variable that is declared as a constant in C. Compile and run the following program, and see what happens. #include <stdio.h> void do_mischief(int *n) { *n = 42; } int main(void) { const int n = 5; printf(\"originally, n = %d\\n\", n); do_mischief((int *) &n); printf(\"mischief accomplished, n = %d\\n\", n); return 0; } In fact, this is explicitly mentioned in the C++ core guidelines.","title":"Be explicit about constants"},{"location":"CodingBestPractices/coding_best_practices/#control-access","text":"When defining classes in C++ and Fortran, some attention should be paid to accessibility of object attributes. An object's state is determined by its attributes' values, so allowing unrestricted access to these attributes may leave the object in an inconsistent state. In C++, object attributes and methods are private by default, while structure fields and methods are public. For Fortran, fields in user defined types and procedures defined in modules are public by default. Regardless of the defaults, it is useful to specify the access restrictions explicitly. It is good practice to specify private access as the default, and public as the exception to that rule. Interestingly, both Fortran and C++ have the keyword protected , albeit with very different semantics. In Fortran, protected means that a variable defined in a module can be read by the compilation unit that uses it, but not modified. In the module where it is defined, it can be modified though. In C++, an attribute or a method that is declared protected can be accessed from derived classes as well as the class that defines it. However, like attributes and methods declared private , it can not be accessed elsewhere. This is another example where getting confused about the semantics can lead to interesting bugs. In summary: access modifier C++ Fortran private access restricted to class/struct access restricted to module protected access restricted to class/struct and derived variables: modify access restricted to module, read everywhere public attributes and methods can be accessed from everywhere variables, types and procedures can be accessed from everywhere none class: private, struct: public public","title":"Control access"},{"location":"CodingBestPractices/coding_best_practices/#variable-initialisation","text":"The specifications for Fortran, C and C++ do not define the value an uninitialized variable will have. So you should always initialise variables explicitly, otherwise your code will have undefined, and potentially non-deterministic behavior. When you forget to initialise a variable, the compilers will typically let you get away with it. However, most compilers have optional flags that catch expressions involving uninitialised variables. We will discuss these and other compiler flags in a later section. When initialising or, more generally, assigning a value to a variable that involves constants, your code will be easier to understand when those values indicate the intended type. For example, using 1.0 rather than 1 for floating point is more explicit. This may also avoid needless conversions. This also prevents arithmetic bugs since 1/2 will evaluate to 0 in C, C++ as well as Fortran. Perhaps even more subtly, 1.25 + 1/2 will also evaluate to 1.25 , since the division will be computed using integer values, evaluating to 0 , which is subsequently converted to the floating point value 0.0 , and added to 1.25 . Specifically for C++, I'd strongly encourage you to use list initialisation, since narrowing conversion would lead to warnings. In the code fragment below, the first local variable n1 will be initialised to 7 without any warnings, while the compiler will generate a warning for the initialisation of n2 . int conversion(double x) { int n1 = x; int n2 {x}; return n1 + n2; }","title":"Variable initialisation"},{"location":"CodingBestPractices/coding_best_practices/#to-comment-or-not-to-comment","text":"Comments should never be a substitute for code that is easy to understand. In almost all circumstances, if your code requires a comment without which it can not be understood, it can be rewritten to be more clear. Obviously, there are exceptions to this rule. Sometimes we have no alternative but to sacrifice a clean coding style for performance, or we have to add an obscure line of code to prevent a problem caused by over-eager compilers. If you need to add a comment, remember that it should be kept up-to-date with the code. All too often, we come across comments that are no longer accurate because the code has evolved, but the corresponding comment didn't. In such situations, the comment is harmful, since it can confuse us about the intentions of the developer, and at the least, it will cost us time to disambiguate. The best strategy is to make sure that the code tells its own story, and requires no comments. A common abuse of comments is to disable code fragments that are no longer required, but that you still want to preserve. This is bad practice. Such comments make reading the code more difficult, and take up valuable screen real estate. Moreover, when you use a version control system such as git or subversion in your development process, you can delete with impunity, in the sure knowledge that you can easily retrieve previous versions of your files. If you don't use a version control system routinely, you really should. See the additional material section for some pointers to information and tutorials.","title":"To comment or not to comment?"},{"location":"CodingBestPractices/coding_best_practices/#stick-to-the-standard","text":"The official syntax and semantics of languages like C, C++ and Fortran is defined in official specifications. All compilers that claim compliance with these standards have to implement these specifications. However, over the years, compiler developers have added extensions to the specifications. The Intel Fortran compiler for instance has a very long history that can trace its ancestry back to the DEC compiler, and implements quite a number of Fortran extensions. Similarly, the GCC C++ compiler supports some non-standard features. It goes without saying that your code should not rely on such compiler specific extensions, even if that compiler is mainstream and widely available. There is no guarantee that future releases of that same compiler will still support the extension, and the only official information about that extension would be available in the compiler documentation, not always the most convenient source. Moreover, that implies that even if your code compiles with a specific compiler, that doesn't mean it complies with the official language specification. An other compiler would simply generate error message for the same code, and would fail to compile it. Using language extensions makes code harder to read. As a proficient programmer, you're still not necessarily familiar with language extensions, so you may interpret those constructs incorrectly. Hence I'd encourage you strongly to strictly adhere to a specific language specification. For C there are three specifications that are still relevant, C89, C99 and C11. For C++ that would be C++11, C++14 and C++17. The relevant specification for Fortran are those of 2003 and 2008. References to those specifications can be found in the section on additional material. For C, you may be interested to read the MISRA C software development guidelines, a collections of directives and rules specified by the Motor Industry Software Reliability Association (MISRA) aimed at ensuring safer and more reliable software systems in the automotive industry. A reference to this specification is mentioned in the additional material section.","title":"Stick to the standard"},{"location":"CodingBestPractices/coding_best_practices/#copypaste-is-evil","text":"If you find yourself copying and pasting a fragment of code from one file location to another, or from one file to another, you should consider turning it into a function. Apart from making your code easier to understand, it makes it also easier to maintain. Suppose there is a bug in the fragment. If you copy/pasted it, you would have to remember to fix the bug in each instance of that code fragment. If it was encapsulated in a function, you would have to fix the problem in a single spot only.","title":"Copy/paste is evil"},{"location":"CodingBestPractices/coding_best_practices_intro/","text":"Introduction to best coding practices When you employ good coding practices, your code will be easier to maintain, and hence the likelihood of introducing code defects will be reduced. Many bugs tend to slip in when new features are introduced, or when problems are being fixed. It is much easier to spot anomalies in code that follows the appropriate conventions, than it is in code that doesn't. Reading through code requires much less brain power when it is familiar than when one has to figure out the semantics of many code fragments. In this section, you'll read about the importance of code style, proper formatting, using appropriate idioms, essentially all conventions that will help to interpret code correctly and efficiently. We will discuss general principles, best practices and tips that help you to write clean code in any programming language.","title":"Introduction"},{"location":"CodingBestPractices/coding_best_practices_intro/#introduction-to-best-coding-practices","text":"When you employ good coding practices, your code will be easier to maintain, and hence the likelihood of introducing code defects will be reduced. Many bugs tend to slip in when new features are introduced, or when problems are being fixed. It is much easier to spot anomalies in code that follows the appropriate conventions, than it is in code that doesn't. Reading through code requires much less brain power when it is familiar than when one has to figure out the semantics of many code fragments. In this section, you'll read about the importance of code style, proper formatting, using appropriate idioms, essentially all conventions that will help to interpret code correctly and efficiently. We will discuss general principles, best practices and tips that help you to write clean code in any programming language.","title":"Introduction to best coding practices"},{"location":"CodingBestPractices/references/","text":"References Here you will find a number of references to additional reading material and software. Reading material Clean code: a handbook of agile software craftsmanship , Robert C. Martin, Prentice Hall, 2008 Write clean code and get rid of code smells with real life examples Design patterns: elements of reusable object-oriented software , Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, Addison-Wesley, 1994 Refactoring: improving the design of existing code , Martin Fowler, Addison-Wesley, 1999 Exception handling in Fortran , Arjen Markus, Newsletter ACM SIGPLAN Fortran Forum, volume 32, issue 2, p. 7\u201213, 2013 11 signs you\u2019re writing great code , Andrew C. Oliver, InfoWorld, April 2018 MISRA C : Motor Industry Software Reliability Association (MISRA) guidelines for software development. The seven deadly sins of programming Software stack GCC Intel Fortran Intel C/C++ Cppcheck Fonts for editors","title":"References"},{"location":"CodingBestPractices/references/#references","text":"Here you will find a number of references to additional reading material and software.","title":"References"},{"location":"CodingBestPractices/references/#reading-material","text":"Clean code: a handbook of agile software craftsmanship , Robert C. Martin, Prentice Hall, 2008 Write clean code and get rid of code smells with real life examples Design patterns: elements of reusable object-oriented software , Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, Addison-Wesley, 1994 Refactoring: improving the design of existing code , Martin Fowler, Addison-Wesley, 1999 Exception handling in Fortran , Arjen Markus, Newsletter ACM SIGPLAN Fortran Forum, volume 32, issue 2, p. 7\u201213, 2013 11 signs you\u2019re writing great code , Andrew C. Oliver, InfoWorld, April 2018 MISRA C : Motor Industry Software Reliability Association (MISRA) guidelines for software development. The seven deadly sins of programming","title":"Reading material"},{"location":"CodingBestPractices/references/#software-stack","text":"GCC Intel Fortran Intel C/C++ Cppcheck Fonts for editors","title":"Software stack"},{"location":"CodingBestPractices/ErrorHandling/assertions/","text":"Assertions C/C++ support assertions. These are macros defined in assert.h that test a Boolean condition, and will terminate the application when that condition evaluates to false. Assertions can be used, e.g., to check preconditions, invariants and postconditions of functions. Fortran doesn't have an assert mechanism, but using the preprocessor, you can build your own. Assert statements Below you see an implementation of the factorial function. It has some issues, but that is not relevant here. int fac(int n) { assert(n >= 0); int result = 1; for (int i = 2; i <= n; i++) { result *= i; assert(result > 1); } return result; } In the code fragment above the precondition on the argument n is that it should be positive, since the factorial of a strictly negative integer is not defined. This condition is verified by the assert statement. At any given time, the variable result should be larger than 1. This condition could be violated in case integer overflow occurs. The invariant is checked by the second assert statement. By construction, it also verifies the postcondition, i.e., the result of the factorial should always be larger than 1. When you run an application that has assertions enabled, and that calls this function, you would get the following behaviour for a negative argument. $ ./assertions.exe -5 assertions.exe: assertions.c:23: fac: Assertion `n >= 0' failed. Aborted (core dumped) For an argument that would cause result to overflow, the result is: $ ./assertions.exe 30 assertions.exe: assertions.c:27: fac: Assertion `result > 1' failed. Aborted (core dumped) Although this may seem like a neat way to handle errors, it really isn't. The feedback the user of your application gets is very low-level. Especially for the second failure, it would be quite hard, if not impossible to figure out what went wrong without inspecting the source code. Use case Since you are not supposed to use assertions to handle errors, you may wonder what purpose they serve. In fact, assertions are quite useful while writing code because they help the developer to formalise expectations on function arguments and output (preconditions and postconditions), and to ensure that conditions that should always hold true actually do (invariants). For production code, it is very easy to switch off assertions by defining the macro variable NDEBUG when building the application. Note that assertions can have a serious impact on performance. In the example above, the assert statement in the iteration would be executed n times, and we may expect the factorial function to be called often. Executing all these tests on the value of result could accumulate to a noticeable fraction of the execution time of the application. Building with or without asserts By way of illustration, the following make file would allow to build for release and for debug. The former has assertions disabled, while the latter has them enabled. CC = gcc CFLAGS = -g -O0 -Wall -Wextra release: CFLAGS += -D NDEBUG release: all debug: all all: assertions.exe %.exe: %.c $(CC) $(CFLAGS) -o $@ $< clean: $(RM) $(wildcard *.exe) $(wildcard *.o) $(RM) $(wildcard core.*) core The build targets release and debug will ensure that the NDEBUG macro variable is either defined or not defined respectively. When using CMake as a build system, it will take care of this based on the value of the CMAKE_BUILD_TYPE variable. When that value is either Release or RelWithDebInfo , assertions will be disabled. Also note that depending on the level of optimisation, assertions may actually be optimised away. In the factorial example above, for instance, the assert in the iteration would be optimised out at optimisation level -O2 when using GCC, while the one that checks the precondition would remain. Hence it may be wise to use -O0 for debug builds. Asserts and testing Although assertions can be pressed into service to implement software tests, it is better to use a unit testing framework for this purpose.","title":"Assertions"},{"location":"CodingBestPractices/ErrorHandling/assertions/#assertions","text":"C/C++ support assertions. These are macros defined in assert.h that test a Boolean condition, and will terminate the application when that condition evaluates to false. Assertions can be used, e.g., to check preconditions, invariants and postconditions of functions. Fortran doesn't have an assert mechanism, but using the preprocessor, you can build your own.","title":"Assertions"},{"location":"CodingBestPractices/ErrorHandling/assertions/#assert-statements","text":"Below you see an implementation of the factorial function. It has some issues, but that is not relevant here. int fac(int n) { assert(n >= 0); int result = 1; for (int i = 2; i <= n; i++) { result *= i; assert(result > 1); } return result; } In the code fragment above the precondition on the argument n is that it should be positive, since the factorial of a strictly negative integer is not defined. This condition is verified by the assert statement. At any given time, the variable result should be larger than 1. This condition could be violated in case integer overflow occurs. The invariant is checked by the second assert statement. By construction, it also verifies the postcondition, i.e., the result of the factorial should always be larger than 1. When you run an application that has assertions enabled, and that calls this function, you would get the following behaviour for a negative argument. $ ./assertions.exe -5 assertions.exe: assertions.c:23: fac: Assertion `n >= 0' failed. Aborted (core dumped) For an argument that would cause result to overflow, the result is: $ ./assertions.exe 30 assertions.exe: assertions.c:27: fac: Assertion `result > 1' failed. Aborted (core dumped) Although this may seem like a neat way to handle errors, it really isn't. The feedback the user of your application gets is very low-level. Especially for the second failure, it would be quite hard, if not impossible to figure out what went wrong without inspecting the source code.","title":"Assert statements"},{"location":"CodingBestPractices/ErrorHandling/assertions/#use-case","text":"Since you are not supposed to use assertions to handle errors, you may wonder what purpose they serve. In fact, assertions are quite useful while writing code because they help the developer to formalise expectations on function arguments and output (preconditions and postconditions), and to ensure that conditions that should always hold true actually do (invariants). For production code, it is very easy to switch off assertions by defining the macro variable NDEBUG when building the application. Note that assertions can have a serious impact on performance. In the example above, the assert statement in the iteration would be executed n times, and we may expect the factorial function to be called often. Executing all these tests on the value of result could accumulate to a noticeable fraction of the execution time of the application.","title":"Use case"},{"location":"CodingBestPractices/ErrorHandling/assertions/#building-with-or-without-asserts","text":"By way of illustration, the following make file would allow to build for release and for debug. The former has assertions disabled, while the latter has them enabled. CC = gcc CFLAGS = -g -O0 -Wall -Wextra release: CFLAGS += -D NDEBUG release: all debug: all all: assertions.exe %.exe: %.c $(CC) $(CFLAGS) -o $@ $< clean: $(RM) $(wildcard *.exe) $(wildcard *.o) $(RM) $(wildcard core.*) core The build targets release and debug will ensure that the NDEBUG macro variable is either defined or not defined respectively. When using CMake as a build system, it will take care of this based on the value of the CMAKE_BUILD_TYPE variable. When that value is either Release or RelWithDebInfo , assertions will be disabled. Also note that depending on the level of optimisation, assertions may actually be optimised away. In the factorial example above, for instance, the assert in the iteration would be optimised out at optimisation level -O2 when using GCC, while the one that checks the precondition would remain. Hence it may be wise to use -O0 for debug builds.","title":"Building with or without asserts"},{"location":"CodingBestPractices/ErrorHandling/assertions/#asserts-and-testing","text":"Although assertions can be pressed into service to implement software tests, it is better to use a unit testing framework for this purpose.","title":"Asserts and testing"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/","text":"Error handling in C Quite a number of bugs are introduced due to incorrect or even no handling of error conditions during the execution of an application. This type of defect is especially annoying since the symptoms will occur some time after the actual cause, and manifest themselves in functions that seem to have little to do with that cause. This \"lack of locality\" makes identifying the issue quite hard. A defensive style of programming will help to prevent these situations. Note that proper error handling can be quite complex and increase the size of your code base substantially. Dynamic memory allocation In C, a primary example of non-local issues is the management of dynamic memory, i.e., memory allocated on the heap using malloc or a related function. Consider the following code: #include <stdlib.h> double *create_vector(unsigned long n) { return (double *) calloc(n, sizeof(double)); } ... void daxpy(double alpha, double *x, double *y, unsigned long n) { for (unsigned long i = 0; i < n; i++) x[i] = alpha*x[i] + y[i]; } Allocation functions such as calloc will return a NULL value when there is not enough memory space to accommodate the request. However, since create_vector doesn't check, the application will continue under the assumption that its result is indeed an array with all n elements set to zero. At some point, e.g., in a call to the function daxpy , the double pointer x , or y , or both may in fact contain that NULL , and the application will crash with a segmentation fault. The problem, in this case merely a symptom, will occur in daxpy , while the cause is in fact in create_vector , or, to be more precise, wherever the size of the array was computed. If this is a complex application, it may take you a while to track down the root cause of this crash. You want errors to occur as soon as possible since the closer that happens in space and time to the root cause, the easier it will be to identify and fix the issue. In this particular case, the function create_vector should check whether calloc returns NULL , and if so, generate an error. #include <err.h> #include <stdlib.h> #define MEM_ALLOC_ERR 11 double *create_vector(unsigned long n) { double *v = (double *) calloc(n, sizeof(double)); if (v == NULL) errx(MEM_ALLOC_ERR, \"can't allocate vector of size %lu\", n); return v; } The errx function declared in the err.h will print the error message to standard error and terminate the application with exit status MEM_ALLOC_ERR . This makes it a lot easier to find the problem since you only need to figure out why the value of n is too large. Seasoned C programmers will argue that the above code fragment is not idiomatic and should be written as: double *create_vector(unsigned long n) { double *v; if (!(v = (double *) calloc(n, sizeof(double)))) errx(MEM_ALLOC_ERR, \"can't allocate vector of size %lu\", n); return v; } When this application is run and it fails, this will produce the following output: allocation_error.exe: can't allocate data of size 10000000000 Although this error message describes the issue, it could be more informative by using the values of a few macros: __FILE__ contains the name of the source file it occurs in, __LINE__ contains the line number of the source file it occurs on, __func__ contains the name of the current function (introduced in C99). double *create_vector(unsigned long n) { double *v; if (!(v = (double *) calloc(n, sizeof(double)))) errx(MEM_ALLOC_ERR, \"%s:%d (%s) can't allocate vector of size %lu\", __FILE__, __LINE__, __func__, n); return v; } Now the output would be: ~~~~bash allocation_error.exe: allocation_error.c:12 (create_vector): can't allocate data of size 10000000000 ~~~~ The __LINE__ macro is set to the line number it occurs on in the source file, so it will not actually be the line number on which the error occurs, but it points you in the right direction anyway. Although it is possible to print a backtrace of the current stack, that is probably not worth the effort since this can be handled more easily and conveniently using a debugger. String conversion Often, the functions atoi , atol , and atof are used to convert command line arguments to int , long , and float / double values respectively. However, in general, this is not good practice. When the char array passed to these functions can not be converted to the desired data type, the behaviour is undefined according to the C specification. In other words, it is up to the implementer of the standard library to decide what happens in this case. For instance, consider the following simple program: #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { long n = 5; double a = 3.14; if (argc > 1) n = atol(argv[1]); if (argc > 2) a = atof(argv[2]); printf(\"n = %ld, a = %lf\\n\", n, a); return 0; } When you compile this with either GCC or Intel compilers and run it, you will get the following output: $ ./command_line_args.exe n = 5, a = 3.140000 $ ./command_line_args.exe 15abc 1.43e-2def n = 15, a = 0.014300 $ ./command_line_args.exe 12.73 n = 12, a = 3.140000 $ ./command_line_args.exe abc def n = 0, a = 0.000000 When used as intended, the applications works as expected. However, when the values passed via the command line are not appropriate, the application will run without warnings or errors, but it will most likely produce results you don't expect. This is an argument to avoid atoi and its ilk, and to use functions that are more robust and check for problems. The following code illustrates how to use strtol and strtod . #include <err.h> #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { long n = 5; double a = 3.14; if (argc > 1) { char *end_ptr = argv[1]; n = strtol(argv[1], &end_ptr, 10); if (*end_ptr != '\\0' || end_ptr == argv[1]) warnx(\"'%s' could not be (completely) converted to long\", argv[1]); } if (argc > 2) { char *end_ptr = argv[2]; a = strtod(argv[2], &end_ptr); if (*end_ptr != '\\0' || end_ptr == argv[2]) warnx(\"'%s' could not be (completely) converted to double\", argv[2]); } printf(\"n = %ld, a = %lf\\n\", n, a); return 0; } This application will issue warnings if the command line arguments can not be converted properly. The value of end_ptr is used to detect issues. If *end_ptr != '\\0', then the first part of the argument could be converted to a number, but subsequent characters could not, e.g., 15abc ; end_prt == argv[1] , then either the argument is an empty string or it completely consists of characters that can not be converted to a number. Of course, substituting errx for warnx would terminate the application rather than just print a warning message. Which action is most appropriate depends on the application. Just like errx , warnx is declared in err.h . When you have to deal with non-trivial command line arguments such as options and flags, you should consider using the getopt function declared in unistd.h for that purpose. This is however outside the scope of this course. Alternatively, for C++, you could go with the Boost library's program_options . File I/O When reading or writing files quite a number of things can go wrong. Just like the functions for memory allocation, the fopen function will return a null pointer when the operation fails. If you don't check for that, your application will most likely crash with a segmentation fault as soon as it attempts to read or write. The code fragment below will open a file, read it line by line, and output the length of each line, followed by the line itself. #include <err.h> #include <stdio.h> #include <stdlib.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; char *line = NULL; size_t buffer_length; ssize_t nr_chars; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while ((nr_chars = getline(&line, &buffer_length, fp)) != -1) { printf(\"%3zu: %s\", nr_chars, line); } free(line); fclose(fp); return 0; } The application verifies that the file has been opened successfully, and if not, it uses the err function declared in err.h to report this and terminate the application. The err function is quite similar to errx , but it will also print the error message associated with the failed system call. For instance, when called with a file that doesn't exist, you will get the following error message: $ ./file_error.exe bla file_error.exe: can't open file 'bla' for reading: No such file or directory On the other hand, if it is called with a file that exists, but that you don't have permission to read or write, you would get the following: $ ./file_error.exe test.txt file_error.exe: can't open file 'test.txt' for reading: Permission denied In this case, using err rather than errx improves the quality of the error message and helps the user of your application to figure out what the problem might be. It is also quite useful to check the return value of functions like scanf . This will alert you to problems that may otherwise go unnoticed. Consider the following input file that is used to initialise the coordinates of 3D point: x = 1.1 y = 2.2 z = 3.3 The following application reads that configuration file and prints the coordinates of the point. #include <err.h> #include <stdio.h> #include <string.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 #define VALUE_ERR 3 typedef struct { double x, y, z; } Point; int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; char name[20]; double value; Point point; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while (fscanf(fp, \"%s = %lf\", name, &value) != -1) { if (!strcmp(\"x\", name)) point.x = value; else if (!strcmp(\"y\", name)) point.y = value; else if (!strcmp(\"z\", name)) point.z = value; else errx(VALUE_ERR, \"invalid name '%s'\", name); } fclose(fp); printf(\"x = %lf, y = %lf, z = %lf\\n\", point.x, point.y, point.z); return 0; } Even with an incorrect input file such as the one below, this application will continue to run, most likely producing nonsense results. x = 1.1 y = z = 3.3 The output would be the following, an unintended result is printed, and no errors are reported: $ ./read_error_incorrect.exe input_incomplete.txt x = 1.100000, y = 1.100000, z = 3.300000 The following input would cause an error, although it is a fairly cryptic one: x = 1.1 y = O.5 z = 3.3 This would be the output: ./read_error_incorrect.exe input_nok.txt read_error_incorrect.exe: invalid name 'O.5' Explicitly checking the number of values processed by fscanf will detect the problem and avoid some nasty issues later on. #include <err.h> #include <stdio.h> #include <string.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 #define VALUE_ERR 3 typedef struct { double x, y, z; } Point; int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; int nr_read; int line_nr = 0; char name[20]; double value; Point point; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while ((nr_read = fscanf(fp, \"%s = %lf\", name, &value)) != -1) { line_nr++; if (nr_read != 2) errx(VALUE_ERR, \"invalid input on line %d of %s\\n\", line_nr, argv[1]); if (!strcmp(\"x\", name)) point.x = value; else if (!strcmp(\"y\", name)) point.y = value; else if (!strcmp(\"z\", name)) point.z = value; else errx(VALUE_ERR, \"invalid name '%s'\", name); } fclose(fp); printf(\"x = %lf, y = %lf, z = %lf\\n\", point.x, point.y, point.z); return 0; } When the input is invalid, you get an error: $ ./read_error.exe input_nok.txt read_error.exe: invalid input on line 2 of input_nok.txt Note that keeping track of the line number in the input file and reporting it in case of an error will again help the user of this application to identify the problem. Overly defensive programming Grace Hopper is credited with the quote It's easier to ask forgiveness than it is to get permission. Before even attempting to open a file with a given name, you could check whether something with that name exists, it is actually a file, you have permission to open it. Doing those checks is like asking permission in an administrative matter. It is a lengthy process, it is tedious and boring. The alternative is to simply attempt to open the file, and if that fails, simply tell the user why. Thanks to functions such as err and warn that will pick up the message associated with the most recent error, chances are that your application will write error messages that are as informative as the ones you'd handcraft by checking for all conceivable error conditions manually. Your code will be more concise, simpler, and hence the probability of having bugs in your error handling code is reduced. Error context At which level do you report an error? This is a non-trivial question. Suppose you are developing an application that reads some parameters from a configuration file, it creates data structures, initialises them, and starts to compute. One of the configuration parameters is the size of the vectors your computation uses, and those are dynamically allocated. Now you already know that your should check the result of malloc to ensure that the allocation succeeded. Failing to do so will most likely result in a segmentation fault. However, the user of your application (potentially you) enters a vector size in the configuration file that is too large to be allocated. No problem though, your application handles error conditions and reports to the user. You could report the error and terminate execution in the function where it actually occurs, the create_vector function you defined in one of the previous sections. This would inform the user that some data structure can not be allocated. However, unless she is familiar with the nuts and bolts of the application, that may in fact be completely uninformative. The function create_vector has no clue about the context in which it is called, and can hardly be expected to produce a more meaningful error message. It would be more useful to the user if this error were reported to the calling function, which has more contextual information, and that this function would report an error that has better semantics. At the end of the day, the relevant information is that you should reduce the value of a parameter in your configuration file. Handling errors in the appropriate context is not that easy. It requires careful planning and formulating error messages from the perspective of the user at each layer in your application. In a language such as C, this means that functions should return status information. In the C API for the MPI library for instance, almost all functions return an int exit value that can be used to check whether the function call was executed successfully. In programming languages such as C++ and Python, error handling is simpler since you can use exceptions to propagate status information when a problem occurs and handle it using try ... catch ... statements in C++ or try: ... except ...: ... in Python. Regardless of the programming language you use, proper error handling will be fairly complex. Floating point expectations There is a number of problems that may arise during numerical computations and that go unnoticed or are only noticed late, i.e., when a lot of expensive computations have been performed. The IEEE standard 754 defines five exceptions that can occur as a result of floating point operations: inexact: accuracy is lost; divide by zero; underflow: a value can not be represented and is round to zero; overflow: a value is too large to be represented; and invalid: operations is invalid for the given operands. A divide by zero and an overflow will result in positive or negative infinity, depending on the sign of the operand, while an invalid operation will result in positive or negative NaN (Not a Number). These values will propagate throughout your computations rendering them useless. Note that an underflow will easily go unnoticed, which makes it even more dangerous. The ISO C99 standard defines a number of constants and functions to detect IEEE floating point exceptions, primarily: fetestexcept to test whether an floating point exception occurred, and feclearexcept to reset the exception bits. You can test for the five exceptions using the following predefined constants: FE_INEXACT , FE_DIVBYZERO , FE_UNDERFLOW , FE_OVERFLOW , FE_INVALID , or for all using FE_ALL_EXCEPT . Below is a code sample that shows how to detect invalid and/or overflow in a computation. The relevant declarations are in the header file fenv.h . #include <err.h> #include <fenv.h> #include <math.h> #include <stdio.h> #include <stdlib.h> double sum(int n); int main(int argc, char *argv[]) { int status; int n = 10; if (argc == 2) n = atoi(argv[1]); double result = sum(n); if ((status = fetestexcept(FE_INVALID | FE_OVERFLOW))) { if (status & FE_INVALID) warnx(\"invalid operation detected\"); else if (status & FE_OVERFLOW) warnx(\"overflow detected\"); } printf(\"sum = %le\\n\", result); return 0; } This application would trap any IEEE floating point overflow or invalid exceptions that are raised in the function sum . Alternatively, functions in math.h can be used to check whether a value is normal, e.g., #include <err.h> #include <math.h> #include <stdio.h> #include <stdlib.h> double sum(int n); int main(int argc, char *argv[]) { int status; int n = 10; if (argc == 2) n = atoi(argv[1]); double result = sum(n); if (!isnormal(result)) warnx(\"non-normal result detected\"); printf(\"sum = %le\\n\", result); return 0; } The math.h header defines a number of other functions that may be useful in this context, e.g., isinf , isfinite , isnan .","title":"Error handling in C"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#error-handling-in-c","text":"Quite a number of bugs are introduced due to incorrect or even no handling of error conditions during the execution of an application. This type of defect is especially annoying since the symptoms will occur some time after the actual cause, and manifest themselves in functions that seem to have little to do with that cause. This \"lack of locality\" makes identifying the issue quite hard. A defensive style of programming will help to prevent these situations. Note that proper error handling can be quite complex and increase the size of your code base substantially.","title":"Error handling in C"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#dynamic-memory-allocation","text":"In C, a primary example of non-local issues is the management of dynamic memory, i.e., memory allocated on the heap using malloc or a related function. Consider the following code: #include <stdlib.h> double *create_vector(unsigned long n) { return (double *) calloc(n, sizeof(double)); } ... void daxpy(double alpha, double *x, double *y, unsigned long n) { for (unsigned long i = 0; i < n; i++) x[i] = alpha*x[i] + y[i]; } Allocation functions such as calloc will return a NULL value when there is not enough memory space to accommodate the request. However, since create_vector doesn't check, the application will continue under the assumption that its result is indeed an array with all n elements set to zero. At some point, e.g., in a call to the function daxpy , the double pointer x , or y , or both may in fact contain that NULL , and the application will crash with a segmentation fault. The problem, in this case merely a symptom, will occur in daxpy , while the cause is in fact in create_vector , or, to be more precise, wherever the size of the array was computed. If this is a complex application, it may take you a while to track down the root cause of this crash. You want errors to occur as soon as possible since the closer that happens in space and time to the root cause, the easier it will be to identify and fix the issue. In this particular case, the function create_vector should check whether calloc returns NULL , and if so, generate an error. #include <err.h> #include <stdlib.h> #define MEM_ALLOC_ERR 11 double *create_vector(unsigned long n) { double *v = (double *) calloc(n, sizeof(double)); if (v == NULL) errx(MEM_ALLOC_ERR, \"can't allocate vector of size %lu\", n); return v; } The errx function declared in the err.h will print the error message to standard error and terminate the application with exit status MEM_ALLOC_ERR . This makes it a lot easier to find the problem since you only need to figure out why the value of n is too large. Seasoned C programmers will argue that the above code fragment is not idiomatic and should be written as: double *create_vector(unsigned long n) { double *v; if (!(v = (double *) calloc(n, sizeof(double)))) errx(MEM_ALLOC_ERR, \"can't allocate vector of size %lu\", n); return v; } When this application is run and it fails, this will produce the following output: allocation_error.exe: can't allocate data of size 10000000000 Although this error message describes the issue, it could be more informative by using the values of a few macros: __FILE__ contains the name of the source file it occurs in, __LINE__ contains the line number of the source file it occurs on, __func__ contains the name of the current function (introduced in C99). double *create_vector(unsigned long n) { double *v; if (!(v = (double *) calloc(n, sizeof(double)))) errx(MEM_ALLOC_ERR, \"%s:%d (%s) can't allocate vector of size %lu\", __FILE__, __LINE__, __func__, n); return v; } Now the output would be: ~~~~bash allocation_error.exe: allocation_error.c:12 (create_vector): can't allocate data of size 10000000000 ~~~~ The __LINE__ macro is set to the line number it occurs on in the source file, so it will not actually be the line number on which the error occurs, but it points you in the right direction anyway. Although it is possible to print a backtrace of the current stack, that is probably not worth the effort since this can be handled more easily and conveniently using a debugger.","title":"Dynamic memory allocation"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#string-conversion","text":"Often, the functions atoi , atol , and atof are used to convert command line arguments to int , long , and float / double values respectively. However, in general, this is not good practice. When the char array passed to these functions can not be converted to the desired data type, the behaviour is undefined according to the C specification. In other words, it is up to the implementer of the standard library to decide what happens in this case. For instance, consider the following simple program: #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { long n = 5; double a = 3.14; if (argc > 1) n = atol(argv[1]); if (argc > 2) a = atof(argv[2]); printf(\"n = %ld, a = %lf\\n\", n, a); return 0; } When you compile this with either GCC or Intel compilers and run it, you will get the following output: $ ./command_line_args.exe n = 5, a = 3.140000 $ ./command_line_args.exe 15abc 1.43e-2def n = 15, a = 0.014300 $ ./command_line_args.exe 12.73 n = 12, a = 3.140000 $ ./command_line_args.exe abc def n = 0, a = 0.000000 When used as intended, the applications works as expected. However, when the values passed via the command line are not appropriate, the application will run without warnings or errors, but it will most likely produce results you don't expect. This is an argument to avoid atoi and its ilk, and to use functions that are more robust and check for problems. The following code illustrates how to use strtol and strtod . #include <err.h> #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { long n = 5; double a = 3.14; if (argc > 1) { char *end_ptr = argv[1]; n = strtol(argv[1], &end_ptr, 10); if (*end_ptr != '\\0' || end_ptr == argv[1]) warnx(\"'%s' could not be (completely) converted to long\", argv[1]); } if (argc > 2) { char *end_ptr = argv[2]; a = strtod(argv[2], &end_ptr); if (*end_ptr != '\\0' || end_ptr == argv[2]) warnx(\"'%s' could not be (completely) converted to double\", argv[2]); } printf(\"n = %ld, a = %lf\\n\", n, a); return 0; } This application will issue warnings if the command line arguments can not be converted properly. The value of end_ptr is used to detect issues. If *end_ptr != '\\0', then the first part of the argument could be converted to a number, but subsequent characters could not, e.g., 15abc ; end_prt == argv[1] , then either the argument is an empty string or it completely consists of characters that can not be converted to a number. Of course, substituting errx for warnx would terminate the application rather than just print a warning message. Which action is most appropriate depends on the application. Just like errx , warnx is declared in err.h . When you have to deal with non-trivial command line arguments such as options and flags, you should consider using the getopt function declared in unistd.h for that purpose. This is however outside the scope of this course. Alternatively, for C++, you could go with the Boost library's program_options .","title":"String conversion"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#file-io","text":"When reading or writing files quite a number of things can go wrong. Just like the functions for memory allocation, the fopen function will return a null pointer when the operation fails. If you don't check for that, your application will most likely crash with a segmentation fault as soon as it attempts to read or write. The code fragment below will open a file, read it line by line, and output the length of each line, followed by the line itself. #include <err.h> #include <stdio.h> #include <stdlib.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; char *line = NULL; size_t buffer_length; ssize_t nr_chars; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while ((nr_chars = getline(&line, &buffer_length, fp)) != -1) { printf(\"%3zu: %s\", nr_chars, line); } free(line); fclose(fp); return 0; } The application verifies that the file has been opened successfully, and if not, it uses the err function declared in err.h to report this and terminate the application. The err function is quite similar to errx , but it will also print the error message associated with the failed system call. For instance, when called with a file that doesn't exist, you will get the following error message: $ ./file_error.exe bla file_error.exe: can't open file 'bla' for reading: No such file or directory On the other hand, if it is called with a file that exists, but that you don't have permission to read or write, you would get the following: $ ./file_error.exe test.txt file_error.exe: can't open file 'test.txt' for reading: Permission denied In this case, using err rather than errx improves the quality of the error message and helps the user of your application to figure out what the problem might be. It is also quite useful to check the return value of functions like scanf . This will alert you to problems that may otherwise go unnoticed. Consider the following input file that is used to initialise the coordinates of 3D point: x = 1.1 y = 2.2 z = 3.3 The following application reads that configuration file and prints the coordinates of the point. #include <err.h> #include <stdio.h> #include <string.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 #define VALUE_ERR 3 typedef struct { double x, y, z; } Point; int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; char name[20]; double value; Point point; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while (fscanf(fp, \"%s = %lf\", name, &value) != -1) { if (!strcmp(\"x\", name)) point.x = value; else if (!strcmp(\"y\", name)) point.y = value; else if (!strcmp(\"z\", name)) point.z = value; else errx(VALUE_ERR, \"invalid name '%s'\", name); } fclose(fp); printf(\"x = %lf, y = %lf, z = %lf\\n\", point.x, point.y, point.z); return 0; } Even with an incorrect input file such as the one below, this application will continue to run, most likely producing nonsense results. x = 1.1 y = z = 3.3 The output would be the following, an unintended result is printed, and no errors are reported: $ ./read_error_incorrect.exe input_incomplete.txt x = 1.100000, y = 1.100000, z = 3.300000 The following input would cause an error, although it is a fairly cryptic one: x = 1.1 y = O.5 z = 3.3 This would be the output: ./read_error_incorrect.exe input_nok.txt read_error_incorrect.exe: invalid name 'O.5' Explicitly checking the number of values processed by fscanf will detect the problem and avoid some nasty issues later on. #include <err.h> #include <stdio.h> #include <string.h> #define ARG_ERR 1 #define FILE_OPEN_ERR 2 #define VALUE_ERR 3 typedef struct { double x, y, z; } Point; int main(int argc, char *argv[]) { if (argc == 1) errx(ARG_ERR, \"no file name specified\"); FILE *fp; int nr_read; int line_nr = 0; char name[20]; double value; Point point; if (!(fp = fopen(argv[1], \"r\"))) err(FILE_OPEN_ERR, \"can't open file '%s' for reading\", argv[1]); while ((nr_read = fscanf(fp, \"%s = %lf\", name, &value)) != -1) { line_nr++; if (nr_read != 2) errx(VALUE_ERR, \"invalid input on line %d of %s\\n\", line_nr, argv[1]); if (!strcmp(\"x\", name)) point.x = value; else if (!strcmp(\"y\", name)) point.y = value; else if (!strcmp(\"z\", name)) point.z = value; else errx(VALUE_ERR, \"invalid name '%s'\", name); } fclose(fp); printf(\"x = %lf, y = %lf, z = %lf\\n\", point.x, point.y, point.z); return 0; } When the input is invalid, you get an error: $ ./read_error.exe input_nok.txt read_error.exe: invalid input on line 2 of input_nok.txt Note that keeping track of the line number in the input file and reporting it in case of an error will again help the user of this application to identify the problem.","title":"File I/O"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#overly-defensive-programming","text":"Grace Hopper is credited with the quote It's easier to ask forgiveness than it is to get permission. Before even attempting to open a file with a given name, you could check whether something with that name exists, it is actually a file, you have permission to open it. Doing those checks is like asking permission in an administrative matter. It is a lengthy process, it is tedious and boring. The alternative is to simply attempt to open the file, and if that fails, simply tell the user why. Thanks to functions such as err and warn that will pick up the message associated with the most recent error, chances are that your application will write error messages that are as informative as the ones you'd handcraft by checking for all conceivable error conditions manually. Your code will be more concise, simpler, and hence the probability of having bugs in your error handling code is reduced.","title":"Overly defensive programming"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#error-context","text":"At which level do you report an error? This is a non-trivial question. Suppose you are developing an application that reads some parameters from a configuration file, it creates data structures, initialises them, and starts to compute. One of the configuration parameters is the size of the vectors your computation uses, and those are dynamically allocated. Now you already know that your should check the result of malloc to ensure that the allocation succeeded. Failing to do so will most likely result in a segmentation fault. However, the user of your application (potentially you) enters a vector size in the configuration file that is too large to be allocated. No problem though, your application handles error conditions and reports to the user. You could report the error and terminate execution in the function where it actually occurs, the create_vector function you defined in one of the previous sections. This would inform the user that some data structure can not be allocated. However, unless she is familiar with the nuts and bolts of the application, that may in fact be completely uninformative. The function create_vector has no clue about the context in which it is called, and can hardly be expected to produce a more meaningful error message. It would be more useful to the user if this error were reported to the calling function, which has more contextual information, and that this function would report an error that has better semantics. At the end of the day, the relevant information is that you should reduce the value of a parameter in your configuration file. Handling errors in the appropriate context is not that easy. It requires careful planning and formulating error messages from the perspective of the user at each layer in your application. In a language such as C, this means that functions should return status information. In the C API for the MPI library for instance, almost all functions return an int exit value that can be used to check whether the function call was executed successfully. In programming languages such as C++ and Python, error handling is simpler since you can use exceptions to propagate status information when a problem occurs and handle it using try ... catch ... statements in C++ or try: ... except ...: ... in Python. Regardless of the programming language you use, proper error handling will be fairly complex.","title":"Error context"},{"location":"CodingBestPractices/ErrorHandling/error_handling_c/#floating-point-expectations","text":"There is a number of problems that may arise during numerical computations and that go unnoticed or are only noticed late, i.e., when a lot of expensive computations have been performed. The IEEE standard 754 defines five exceptions that can occur as a result of floating point operations: inexact: accuracy is lost; divide by zero; underflow: a value can not be represented and is round to zero; overflow: a value is too large to be represented; and invalid: operations is invalid for the given operands. A divide by zero and an overflow will result in positive or negative infinity, depending on the sign of the operand, while an invalid operation will result in positive or negative NaN (Not a Number). These values will propagate throughout your computations rendering them useless. Note that an underflow will easily go unnoticed, which makes it even more dangerous. The ISO C99 standard defines a number of constants and functions to detect IEEE floating point exceptions, primarily: fetestexcept to test whether an floating point exception occurred, and feclearexcept to reset the exception bits. You can test for the five exceptions using the following predefined constants: FE_INEXACT , FE_DIVBYZERO , FE_UNDERFLOW , FE_OVERFLOW , FE_INVALID , or for all using FE_ALL_EXCEPT . Below is a code sample that shows how to detect invalid and/or overflow in a computation. The relevant declarations are in the header file fenv.h . #include <err.h> #include <fenv.h> #include <math.h> #include <stdio.h> #include <stdlib.h> double sum(int n); int main(int argc, char *argv[]) { int status; int n = 10; if (argc == 2) n = atoi(argv[1]); double result = sum(n); if ((status = fetestexcept(FE_INVALID | FE_OVERFLOW))) { if (status & FE_INVALID) warnx(\"invalid operation detected\"); else if (status & FE_OVERFLOW) warnx(\"overflow detected\"); } printf(\"sum = %le\\n\", result); return 0; } This application would trap any IEEE floating point overflow or invalid exceptions that are raised in the function sum . Alternatively, functions in math.h can be used to check whether a value is normal, e.g., #include <err.h> #include <math.h> #include <stdio.h> #include <stdlib.h> double sum(int n); int main(int argc, char *argv[]) { int status; int n = 10; if (argc == 2) n = atoi(argv[1]); double result = sum(n); if (!isnormal(result)) warnx(\"non-normal result detected\"); printf(\"sum = %le\\n\", result); return 0; } The math.h header defines a number of other functions that may be useful in this context, e.g., isinf , isfinite , isnan .","title":"Floating point expectations"},{"location":"CodingBestPractices/ErrorHandling/error_handling_cpp/","text":"Error handling in C++ All advise given on error handling in C is of course also relevant for C++. However, C++ adds exception handling as a way to deal with run-time errors, and propagate these through your application. To accommodate this, C++ has a few keywords: throw , try and catch . When an error has been detected, an exception can be thrown, e.g., #include <stdexcept> int fac(int n) { if (n < 0) { std::string msg {\"fac received \"}; msg += std::to_string(n) + \", argument must be positive\"; throw std::domain_error {msg}; } else { int value = 1; for (int i = 2; i <= n; i++) value *= i; return value; } } The throw statement will transfer control to the calling context of the fac function. The destructor of all objects on the stack will be called since they go out of scope. The exception domain_error used here is declared in the stdexcept header. This header declares some standard exceptions that cover many cases, e.g., logic_error , invalid_argument , out_of_range , and so on. In the calling context, the exception can be caught, and handled appropriately using a try ... catch ... statement, e.g., ... try { std::cout << fac(n) << std::endl; catch (std::domain_error& e) { std::cerr << \"math function called with argument not in its domain: \" << e.what() << std::endl; ... } ... In the code fragment above, the exception is caught and handled in the immediate calling context. If that is not the case, the exception percolates up the call stack, for each function calling the destructors for the stack variables that go out of scope. Using exceptions makes it easier to handle exception in context to provide the user of your application with relevant feedback. What can be done at that point depends on the exception safety level. Generally, four levels of exception safety are recognised: Nothrow exception guarantee: the function or method never throws an exception. This is expected from destructors. Strong exception guarantee: the state of the program is rolled to the state just before the exceptional state occurred, e.g., failed operations on STL containers. Basic exceptions guarantee: clean-up may be required, but the application is in a valid state. No exceptions guarantee: the application is not in a valid state, e.g., invariants are violated, or resource leaks may have occurred. For the last level, no exception guarantee, recovery will be dangerous, while handling exceptions gets easier on each higher level. Often, it can be convenient to define application-specific exceptions. These can be derived from the std::exception class. It is good practice to define a base class, perhaps abstract, that is the ancestor to all application-specific exceptions. It is worth noting that C++ has no finally block as other programming languages such as Java and Python have. In those languages, the finally block is used to ensure that resources are managed properly both in case of normal behavior as well as failure. In C++, this is not required if resource management follows the RAII principle (Resource Allocation Is Initialization), which essentially means that the destructor is responsible for proper resource deallocation. Another interesting point is rethrowing of exceptions. If you want to rethrow an exception preserving its polymorphic type, throw; will do that, e.g., try { ... } catch (std::exception& e) { std::cerr << \"Oops!\" << std::endl; throw; } In the calling context of this fragment of code, e will still have its original polymorphic type.","title":"Exceptions in C++"},{"location":"CodingBestPractices/ErrorHandling/error_handling_cpp/#error-handling-in-c","text":"All advise given on error handling in C is of course also relevant for C++. However, C++ adds exception handling as a way to deal with run-time errors, and propagate these through your application. To accommodate this, C++ has a few keywords: throw , try and catch . When an error has been detected, an exception can be thrown, e.g., #include <stdexcept> int fac(int n) { if (n < 0) { std::string msg {\"fac received \"}; msg += std::to_string(n) + \", argument must be positive\"; throw std::domain_error {msg}; } else { int value = 1; for (int i = 2; i <= n; i++) value *= i; return value; } } The throw statement will transfer control to the calling context of the fac function. The destructor of all objects on the stack will be called since they go out of scope. The exception domain_error used here is declared in the stdexcept header. This header declares some standard exceptions that cover many cases, e.g., logic_error , invalid_argument , out_of_range , and so on. In the calling context, the exception can be caught, and handled appropriately using a try ... catch ... statement, e.g., ... try { std::cout << fac(n) << std::endl; catch (std::domain_error& e) { std::cerr << \"math function called with argument not in its domain: \" << e.what() << std::endl; ... } ... In the code fragment above, the exception is caught and handled in the immediate calling context. If that is not the case, the exception percolates up the call stack, for each function calling the destructors for the stack variables that go out of scope. Using exceptions makes it easier to handle exception in context to provide the user of your application with relevant feedback. What can be done at that point depends on the exception safety level. Generally, four levels of exception safety are recognised: Nothrow exception guarantee: the function or method never throws an exception. This is expected from destructors. Strong exception guarantee: the state of the program is rolled to the state just before the exceptional state occurred, e.g., failed operations on STL containers. Basic exceptions guarantee: clean-up may be required, but the application is in a valid state. No exceptions guarantee: the application is not in a valid state, e.g., invariants are violated, or resource leaks may have occurred. For the last level, no exception guarantee, recovery will be dangerous, while handling exceptions gets easier on each higher level. Often, it can be convenient to define application-specific exceptions. These can be derived from the std::exception class. It is good practice to define a base class, perhaps abstract, that is the ancestor to all application-specific exceptions. It is worth noting that C++ has no finally block as other programming languages such as Java and Python have. In those languages, the finally block is used to ensure that resources are managed properly both in case of normal behavior as well as failure. In C++, this is not required if resource management follows the RAII principle (Resource Allocation Is Initialization), which essentially means that the destructor is responsible for proper resource deallocation. Another interesting point is rethrowing of exceptions. If you want to rethrow an exception preserving its polymorphic type, throw; will do that, e.g., try { ... } catch (std::exception& e) { std::cerr << \"Oops!\" << std::endl; throw; } In the calling context of this fragment of code, e will still have its original polymorphic type.","title":"Error handling in C++"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/","text":"Error handling in Fortran Quite a number of bugs are introduced due to incorrect or even no handling of error conditions during the execution of an application. This type of defect is especially annoying since the symptoms will occur some time after the actual cause, and manifest themselves in functions that seem to have little to do with that cause. This \"lack of locality\" makes identifying the issue quite hard. A defensive style of programming will help to prevent these situations. Note that proper error handling can be quite complex and increase the size of your code base substantially. Dynamic memory allocation In Fortran, a primary example of non-local issues is the management of dynamic memory, i.e., memory allocated on the heap using the allocate statement. Consider the following code, the subroutine takes arrays as arguments that may have been allocated. subroutine create_array(x, n) implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n allocate(x(n)) end subroutine create_array ... subroutine daxpy(alpha, x, y) implicit none real, intent(in) :: alpha real, dimension(:), intent(inout) :: x real, dimension(:), intent(in) :: y x = alpha*x + y end subroutine daxpy The allocate statement may fail when there is not enough memory space to accommodate the request. However, since create_array doesn't check, the application will continue under the assumption that its result is indeed an array with all n elements. At some point, e.g., in a call to the procedure daxpy , the array x , or y , or both may in fact not have been allocated at all, and the application will crash with a segmentation fault. The problem, in this case merely a symptom, will occur in daxpy , while the cause is in fact in create_array , or, to be more precise, wherever the size of the array was computed. If this is a complex application, it may take you a while to track down the root cause of this crash. You want errors to occur as soon as possible since the closer that happens in space and time to the root cause, the easier it will be to identify and fix the issue. In this particular case, the procedure create_array should check whether allocate succeeded, and if not, generate an error. subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I12)') & '### error: can not allocate array of size ', n stop 1 end if end subroutine create_array Note that it is more informative to write a message to standard error, and use a non-zero numerical argument to stop . When stop is used with an error message, the application's exit code would be 0, and it would be harder to detect an error in your workflow. It would certainly be a good idea to define constants in a module that account for various error conditions so that they can be used consistently across the application. module error_status implicit none integer, parameter :: ALLOCATION_ERR = 1, & FILE_OPEN_ERR = 2, & CMD_LINE_NR_ARGS_ERR = 3, & CMD_LINE_ARG_VALUE_ERR = 4, & FILE_VALUE_ERR = 5 end module error_status You can use this in the create_array procedure, i.e., subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit use :: error_status, only : ALLOCATION_ERR implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I12)') & '### error: can not allocate array of size ', n stop ALLOCATION_ERR end if end subroutine create_array When this application is run and it fails, this will produce the following output ### error: can not allocate array of size 10000000000 STOP 1 Although this error message describes the issue, it could be more informative by using the values of a few macros: __FILE__ contains the name of the source file it occurs in, __LINE__ contains the line number of the source file it occurs on. Using these compiler macros, the create_array procedure can be implemented as follows: subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit use :: error_status, only : ALLOCATION_ERR implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I3, A, I10)') & '### error in ' // trim(__FILE__) // ',', __LINE__, & ': can not allocate array of size ', n stop ALLOCATION_ERR end if end subroutine create_array Now the output would be: ### error in allocation_error.f90, 18: can not allocate array of size 2000000000 STOP 1 The __LINE__ macro is set to the line number it occurs on in the source file, so it will not actually be the line number on which the error occurs, but at least it points you in the right direction. Note that in order to compile Fortran code that relies on the preprocessor you should either specify the -cpp option for the compiler, or name the source files with a file name extension in capital letters, e.g., .F90 . String conversion In Fortran, converting a string to a number is done using a read statement with the appropriate format string. When you try to convert a string value such as \"abc\" to an integer, the application will crash with a meaningful error message and a backtrace. If you want to handle the error yourself, you can easily do this by providing the optional arguments iostat and iomsg to the read statement. use :: error_status, only : CMD_LINE_NR_ARGS_ERR, & CMD_LINE_ARG_VALUE_ERR implicit none real :: x integer :: ierr character(len=80) :: buffer, msg if (command_argument_count() /= 1) then write (unit=error_unit, fmt='(A)') & '### error: expecting a real as argument' stop CMD_LINE_NR_ARGS_ERR end if call get_command_argument(1, buffer) read (buffer, fmt='(F25.16)', iostat=ierr, iomsg=msg) x if (ierr /= 0) then write (unit=error_unit, fmt='(4A)') & '### error: can not convert to real: ', trim(buffer), & ', ', trim(msg) stop CMD_LINE_ARG_VALUE_ERR end if If the read statement fails, the ierr variable will be set to a non-zero value, so you can easily test for problems. The msg variable will be set to a relevant error message that can be used to provide feedback to the user. File I/O When reading or writing files quite a number of things can go wrong. Just like the statements for memory allocation, the I/O related statements open , read , write , ..., have optional arguments iostat and iomsg that will provide feedback on the success or failure of the operation. Fail to use them at your own peril. The code fragment below will open a file, read it line by line, and do some unspecified processing. open (unit=read_unit, file=file_name, access='sequential', & action='read', status='old', form='formatted') do read (unit=read_unit, fmt=\"(A10, E25.16)\", & iostat=ierr, iomsg=msg) name, value if (ierr < 0) exit end do close (unit=read_unit) You might hope that when you run the application and file_name doesn't contain the name of an existing file, or the file can not be opened, your application may terminate with an error message. However, that will not happen. The read statement will set ierr to a negative value, indicating the end of the input file, and everyone will live happily ever after, except that it is quite likely your application will produce unexpected results. The user of your application gets no indication that the file she wanted to be read was completely ignored. use :: error_status ... open (unit=read_unit, file=file_name, access='sequential', & action='read', status='old', form='formatted', & iostat=ierr, iomsg=msg) if (ierr /= 0) then write (unit=error_unit, fmt='(3A)') & '### error: can not open file ', trim(file_name), & ', error: ' // trim(msg) stop FILE_OPEN_ERR end if ... The application verifies that the file has been opened successfully, and if not, it writes an appropriate error message to standard error using the message that was set by the open statement in its iomsg argument. For instance, when called with a file that doesn't exist, you will get the following error message: $ ./file_error.exe bla ### error: cannot open file bla, error: Cannot open file 'bla': No such file or directory STOP 2 On the other hand, if it is called with a file that exists, but that you don't have permission to read or write, you would get the following: $ ./file_error.exe test.txt ### error: cannot open file test.txt, error: Cannot open file 'test.txt': Permission denied STOP 2 In this case, using the value set in the iomsg as part of your own improves the quality of the error message, and helps the user of your application to figure out what the problem might be. It is also quite useful to check the iostat value set by the read statement. This will allow you to improve the quality of the feedback, similar to what was mentioned in the section on string conversion. When iostat has a non-zero value, you can give the user specific information on the expected values. For instance, when reading a file such as the one below, some appropriate error messages may help the user a lot. alpha 1.1 beta 3.3 line_nr = 0 do read (unit=read_unit, fmt=\"(A10, E25.16)\", & iostat=ierr, iomsg=msg) name, value line_nr = line_nr + 1 if (ierr < 0) then exit else if (ierr > 0) then write (unit=error_unit, fmt='(A, I3, A)') & '### error during read at line ', line_nr, ': ' // trim(msg) stop else ! do something useful with the name and value end if When the input is invalid, such as below, you get an error: alpha 1.1 beta 3.O5 $ ./file_error.exe input_nok.txt ### error during read at line 2: Bad value during floating point read STOP 5 Note that keeping track of the line number in the input file and reporting it in case of an error will again help the user of this application to identify the problem. Overly defensive programming Grace Hopper is credited with the quote It's easier to ask forgiveness than it is to get permission. Before even attempting to open a file with a given name, you could check whether something with that name exists, it is actually a file, you have permission to open it. Doing those checks is like asking permission in an administrative matter. It is a lengthy process, it is tedious and boring. The alternative is to simply attempt to open the file, and if that fails, simply tell the user why. Thanks to the values assigned to the iomsg argument, chances are that your application will write error messages that are as informative as the ones you'd handcraft by checking for all conceivable error conditions manually. Your code will be more concise, simpler, and hence the probability of having bugs in your error handling code is reduced. Error context At which level do you report an error? This is a non-trivial question. Suppose you are developing an application that reads some parameters from a configuration file, it creates data structures, initialises them, and starts to compute. One of the configuration parameters is the size of the vectors your computation uses, and those are dynamically allocated. Now you already know that your should check the result of allocate to ensure that the allocation succeeded. Failing to do so will most likely result in a segmentation fault. However, the user of your application (potentially you) enters a vector size in the configuration file that is too large to be allocated. No problem though, your application handles error conditions and reports to the user. You could report the error and terminate execution in the procedure where it actually occurs, the create_array subroutine you defined in one of the previous sections. This would inform the user that some array can not be allocated. However, unless she is familiar with the nuts and bolts of the application, that may in fact be completely uninformative. The subroutine create_array has no clue about the context in which it is called, and can hardly be expected to produce a more meaningful error message. It would be more useful to the user if this error were reported to the calling procedure, which has more contextual information, and that this procedure would report an error that has better semantics. At the end of the day, the relevant information is that you should reduce the value of a parameter in your configuration file. Handling errors in the appropriate context is not that easy. It requires careful planning and formulating error messages from the perspective of the user at each layer in your application. In a language such as Fortran, this means that procedures should return status information, typically as an inout argument. In the Fortran API for the MPI library for instance, almost all functions take an error argument of type integer that can be used to check whether the procedure call was executed successfully. Note that forgetting this argument in the procedure calls may lead to very interesting bugs. In the Fortran 2008 API for this library, that argument is optional, so omitting it is no longer a deadly sin when you use this API. Note that proper error handling will be fairly complex and potentially increase the size of your code base. Floating point expectations There are a number of problems that may arise during numerical computations and that go unnoticed, or are only noticed late, i.e., when a lot of expensive computations have been performed. The IEEE standard 754 defines five exceptions that can occur as a result of floating point operations: inexact: accuracy is lost; divide by zero; underflow: a value can not be represented and is round to zero; overflow: a value is too large to be represented; and invalid: operations is invalid for the given operands. A divide by zero and an overflow will result in positive or negative infinity, depending on the sign of the operand, while an invalid operation will result in positive or negative NaN (Not a Number). These values will propagate throughout your computations, making them useless. Note that an underflow will easily go unnoticed, which makes it even more dangerous. Modern Fortran compilers implement the ieee_arithmetic intrinsic module that defines various functions to check whether a numerical value is normal, e.g., ieee_is_finite , ieee_is_normal , ieee_is_nan . The code below shows a trivial example. program overflow_sum use, intrinsic :: ieee_arithmetic, only : ieee_is_normal use, intrinsic :: iso_fortran_env, only : error_unit implicit none integer :: n character(len=80) :: buffer real :: result n = 10 if (command_argument_count() > 0) then call get_command_argument(1, buffer) read (buffer, '(I10)') n end if result = compute_sum(n) if (.not. ieee_is_normal(result)) then write (unit=error_unit, fmt='(A)') 'non-normal number detected' end if print '(A, E12.5)', 'result = ', result contains real function compute_sum(n) implicit none integer, intent(in) :: n integer :: i compute_sum = 0.0 do i = 1, n compute_sum = compute_sum + 10.0**i end do end function compute_sum end program overflow_sum","title":"Error handling in Fortran"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#error-handling-in-fortran","text":"Quite a number of bugs are introduced due to incorrect or even no handling of error conditions during the execution of an application. This type of defect is especially annoying since the symptoms will occur some time after the actual cause, and manifest themselves in functions that seem to have little to do with that cause. This \"lack of locality\" makes identifying the issue quite hard. A defensive style of programming will help to prevent these situations. Note that proper error handling can be quite complex and increase the size of your code base substantially.","title":"Error handling in Fortran"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#dynamic-memory-allocation","text":"In Fortran, a primary example of non-local issues is the management of dynamic memory, i.e., memory allocated on the heap using the allocate statement. Consider the following code, the subroutine takes arrays as arguments that may have been allocated. subroutine create_array(x, n) implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n allocate(x(n)) end subroutine create_array ... subroutine daxpy(alpha, x, y) implicit none real, intent(in) :: alpha real, dimension(:), intent(inout) :: x real, dimension(:), intent(in) :: y x = alpha*x + y end subroutine daxpy The allocate statement may fail when there is not enough memory space to accommodate the request. However, since create_array doesn't check, the application will continue under the assumption that its result is indeed an array with all n elements. At some point, e.g., in a call to the procedure daxpy , the array x , or y , or both may in fact not have been allocated at all, and the application will crash with a segmentation fault. The problem, in this case merely a symptom, will occur in daxpy , while the cause is in fact in create_array , or, to be more precise, wherever the size of the array was computed. If this is a complex application, it may take you a while to track down the root cause of this crash. You want errors to occur as soon as possible since the closer that happens in space and time to the root cause, the easier it will be to identify and fix the issue. In this particular case, the procedure create_array should check whether allocate succeeded, and if not, generate an error. subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I12)') & '### error: can not allocate array of size ', n stop 1 end if end subroutine create_array Note that it is more informative to write a message to standard error, and use a non-zero numerical argument to stop . When stop is used with an error message, the application's exit code would be 0, and it would be harder to detect an error in your workflow. It would certainly be a good idea to define constants in a module that account for various error conditions so that they can be used consistently across the application. module error_status implicit none integer, parameter :: ALLOCATION_ERR = 1, & FILE_OPEN_ERR = 2, & CMD_LINE_NR_ARGS_ERR = 3, & CMD_LINE_ARG_VALUE_ERR = 4, & FILE_VALUE_ERR = 5 end module error_status You can use this in the create_array procedure, i.e., subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit use :: error_status, only : ALLOCATION_ERR implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I12)') & '### error: can not allocate array of size ', n stop ALLOCATION_ERR end if end subroutine create_array When this application is run and it fails, this will produce the following output ### error: can not allocate array of size 10000000000 STOP 1 Although this error message describes the issue, it could be more informative by using the values of a few macros: __FILE__ contains the name of the source file it occurs in, __LINE__ contains the line number of the source file it occurs on. Using these compiler macros, the create_array procedure can be implemented as follows: subroutine create_array(x, n) use, intrinsic :: iso_fortran_env, only: error_unit use :: error_status, only : ALLOCATION_ERR implicit none real, dimension(:), allocatable, intent(inout) :: x integer, intent(in) :: n integer :: istat allocate(x(n), stat=istat) if (istat /= 0) then write (unit=error_unit, fmt='(A, I3, A, I10)') & '### error in ' // trim(__FILE__) // ',', __LINE__, & ': can not allocate array of size ', n stop ALLOCATION_ERR end if end subroutine create_array Now the output would be: ### error in allocation_error.f90, 18: can not allocate array of size 2000000000 STOP 1 The __LINE__ macro is set to the line number it occurs on in the source file, so it will not actually be the line number on which the error occurs, but at least it points you in the right direction. Note that in order to compile Fortran code that relies on the preprocessor you should either specify the -cpp option for the compiler, or name the source files with a file name extension in capital letters, e.g., .F90 .","title":"Dynamic memory allocation"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#string-conversion","text":"In Fortran, converting a string to a number is done using a read statement with the appropriate format string. When you try to convert a string value such as \"abc\" to an integer, the application will crash with a meaningful error message and a backtrace. If you want to handle the error yourself, you can easily do this by providing the optional arguments iostat and iomsg to the read statement. use :: error_status, only : CMD_LINE_NR_ARGS_ERR, & CMD_LINE_ARG_VALUE_ERR implicit none real :: x integer :: ierr character(len=80) :: buffer, msg if (command_argument_count() /= 1) then write (unit=error_unit, fmt='(A)') & '### error: expecting a real as argument' stop CMD_LINE_NR_ARGS_ERR end if call get_command_argument(1, buffer) read (buffer, fmt='(F25.16)', iostat=ierr, iomsg=msg) x if (ierr /= 0) then write (unit=error_unit, fmt='(4A)') & '### error: can not convert to real: ', trim(buffer), & ', ', trim(msg) stop CMD_LINE_ARG_VALUE_ERR end if If the read statement fails, the ierr variable will be set to a non-zero value, so you can easily test for problems. The msg variable will be set to a relevant error message that can be used to provide feedback to the user.","title":"String conversion"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#file-io","text":"When reading or writing files quite a number of things can go wrong. Just like the statements for memory allocation, the I/O related statements open , read , write , ..., have optional arguments iostat and iomsg that will provide feedback on the success or failure of the operation. Fail to use them at your own peril. The code fragment below will open a file, read it line by line, and do some unspecified processing. open (unit=read_unit, file=file_name, access='sequential', & action='read', status='old', form='formatted') do read (unit=read_unit, fmt=\"(A10, E25.16)\", & iostat=ierr, iomsg=msg) name, value if (ierr < 0) exit end do close (unit=read_unit) You might hope that when you run the application and file_name doesn't contain the name of an existing file, or the file can not be opened, your application may terminate with an error message. However, that will not happen. The read statement will set ierr to a negative value, indicating the end of the input file, and everyone will live happily ever after, except that it is quite likely your application will produce unexpected results. The user of your application gets no indication that the file she wanted to be read was completely ignored. use :: error_status ... open (unit=read_unit, file=file_name, access='sequential', & action='read', status='old', form='formatted', & iostat=ierr, iomsg=msg) if (ierr /= 0) then write (unit=error_unit, fmt='(3A)') & '### error: can not open file ', trim(file_name), & ', error: ' // trim(msg) stop FILE_OPEN_ERR end if ... The application verifies that the file has been opened successfully, and if not, it writes an appropriate error message to standard error using the message that was set by the open statement in its iomsg argument. For instance, when called with a file that doesn't exist, you will get the following error message: $ ./file_error.exe bla ### error: cannot open file bla, error: Cannot open file 'bla': No such file or directory STOP 2 On the other hand, if it is called with a file that exists, but that you don't have permission to read or write, you would get the following: $ ./file_error.exe test.txt ### error: cannot open file test.txt, error: Cannot open file 'test.txt': Permission denied STOP 2 In this case, using the value set in the iomsg as part of your own improves the quality of the error message, and helps the user of your application to figure out what the problem might be. It is also quite useful to check the iostat value set by the read statement. This will allow you to improve the quality of the feedback, similar to what was mentioned in the section on string conversion. When iostat has a non-zero value, you can give the user specific information on the expected values. For instance, when reading a file such as the one below, some appropriate error messages may help the user a lot. alpha 1.1 beta 3.3 line_nr = 0 do read (unit=read_unit, fmt=\"(A10, E25.16)\", & iostat=ierr, iomsg=msg) name, value line_nr = line_nr + 1 if (ierr < 0) then exit else if (ierr > 0) then write (unit=error_unit, fmt='(A, I3, A)') & '### error during read at line ', line_nr, ': ' // trim(msg) stop else ! do something useful with the name and value end if When the input is invalid, such as below, you get an error: alpha 1.1 beta 3.O5 $ ./file_error.exe input_nok.txt ### error during read at line 2: Bad value during floating point read STOP 5 Note that keeping track of the line number in the input file and reporting it in case of an error will again help the user of this application to identify the problem.","title":"File I/O"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#overly-defensive-programming","text":"Grace Hopper is credited with the quote It's easier to ask forgiveness than it is to get permission. Before even attempting to open a file with a given name, you could check whether something with that name exists, it is actually a file, you have permission to open it. Doing those checks is like asking permission in an administrative matter. It is a lengthy process, it is tedious and boring. The alternative is to simply attempt to open the file, and if that fails, simply tell the user why. Thanks to the values assigned to the iomsg argument, chances are that your application will write error messages that are as informative as the ones you'd handcraft by checking for all conceivable error conditions manually. Your code will be more concise, simpler, and hence the probability of having bugs in your error handling code is reduced.","title":"Overly defensive programming"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#error-context","text":"At which level do you report an error? This is a non-trivial question. Suppose you are developing an application that reads some parameters from a configuration file, it creates data structures, initialises them, and starts to compute. One of the configuration parameters is the size of the vectors your computation uses, and those are dynamically allocated. Now you already know that your should check the result of allocate to ensure that the allocation succeeded. Failing to do so will most likely result in a segmentation fault. However, the user of your application (potentially you) enters a vector size in the configuration file that is too large to be allocated. No problem though, your application handles error conditions and reports to the user. You could report the error and terminate execution in the procedure where it actually occurs, the create_array subroutine you defined in one of the previous sections. This would inform the user that some array can not be allocated. However, unless she is familiar with the nuts and bolts of the application, that may in fact be completely uninformative. The subroutine create_array has no clue about the context in which it is called, and can hardly be expected to produce a more meaningful error message. It would be more useful to the user if this error were reported to the calling procedure, which has more contextual information, and that this procedure would report an error that has better semantics. At the end of the day, the relevant information is that you should reduce the value of a parameter in your configuration file. Handling errors in the appropriate context is not that easy. It requires careful planning and formulating error messages from the perspective of the user at each layer in your application. In a language such as Fortran, this means that procedures should return status information, typically as an inout argument. In the Fortran API for the MPI library for instance, almost all functions take an error argument of type integer that can be used to check whether the procedure call was executed successfully. Note that forgetting this argument in the procedure calls may lead to very interesting bugs. In the Fortran 2008 API for this library, that argument is optional, so omitting it is no longer a deadly sin when you use this API. Note that proper error handling will be fairly complex and potentially increase the size of your code base.","title":"Error context"},{"location":"CodingBestPractices/ErrorHandling/error_handling_fortran/#floating-point-expectations","text":"There are a number of problems that may arise during numerical computations and that go unnoticed, or are only noticed late, i.e., when a lot of expensive computations have been performed. The IEEE standard 754 defines five exceptions that can occur as a result of floating point operations: inexact: accuracy is lost; divide by zero; underflow: a value can not be represented and is round to zero; overflow: a value is too large to be represented; and invalid: operations is invalid for the given operands. A divide by zero and an overflow will result in positive or negative infinity, depending on the sign of the operand, while an invalid operation will result in positive or negative NaN (Not a Number). These values will propagate throughout your computations, making them useless. Note that an underflow will easily go unnoticed, which makes it even more dangerous. Modern Fortran compilers implement the ieee_arithmetic intrinsic module that defines various functions to check whether a numerical value is normal, e.g., ieee_is_finite , ieee_is_normal , ieee_is_nan . The code below shows a trivial example. program overflow_sum use, intrinsic :: ieee_arithmetic, only : ieee_is_normal use, intrinsic :: iso_fortran_env, only : error_unit implicit none integer :: n character(len=80) :: buffer real :: result n = 10 if (command_argument_count() > 0) then call get_command_argument(1, buffer) read (buffer, '(I10)') n end if result = compute_sum(n) if (.not. ieee_is_normal(result)) then write (unit=error_unit, fmt='(A)') 'non-normal number detected' end if print '(A, E12.5)', 'result = ', result contains real function compute_sum(n) implicit none integer, intent(in) :: n integer :: i compute_sum = 0.0 do i = 1, n compute_sum = compute_sum + 10.0**i end do end function compute_sum end program overflow_sum","title":"Floating point expectations"},{"location":"Documentation/documentation_best_practices/","text":"Documentation best practices: reading material When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If we want to deliver high quality software, the documentation is a very important aspect of the development process. We distinguish between two types of documentation: tutorial style and reference documentation. Types of documentation Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former. Pitfalls When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly like writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each class or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects. What to document? The first question to answer is, what do you need to document since a software project has many artefacts. Documenting functions Consider the documentation of a function or a method for instance. You would write a description of what the function does, which argument it takes, and what the return type is. However, you would also add the assumptions the function makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celcius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the function and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfil all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfils all the conditions if she calls it correctly. When a function has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by using const declarations in C/C++, or INTENT(IN) in Fortran. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding to immediately provide the documentation when writing a function, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? Will it potentially throw an exception? What is the semantics of these errors or exceptions? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately. Documenting user defined types or classes For user defined types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each member, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming in C++ or Fortran, you should give the same information for object and class attributes. The documentation of object and class methods is similar to that of functions. Documenting modules Code is typically aggregated according to functionality in Fortran modules, or C/C++ files for separate compilation. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initialises a data structure, a few that manipulate such data structures, and a finalisation function that releases the resources in that data structure, then the module documentation should probably mention that you should 1. call the initialisation function to get a valid data structure; 1. call functions that use the data structure; and 1. call the finalisation function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the functions, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though. Documenting projects This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation. Tools In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material. Doxygen Some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation. MkDocs MkDocs is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the Read the Docs service . It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project.","title":"Best practices"},{"location":"Documentation/documentation_best_practices/#documentation-best-practices-reading-material","text":"When you consider using a new software library, you probably like to start by looking at some example code. You'll do a few experiments of your own, based on those examples. Once you start using the new library for non-trivial applications, you will most likely have to refer to the reference guide. All this documentation is a great help if its quality is good. If we want to deliver high quality software, the documentation is a very important aspect of the development process. We distinguish between two types of documentation: tutorial style and reference documentation.","title":"Documentation best practices: reading material"},{"location":"Documentation/documentation_best_practices/#types-of-documentation","text":"Broadly speaking, we distinguish two types of documentation, tutorials and reference guides. They serve different purposes, and hence are complementary. Tutorials will typically illustrate a number of use cases, presented as a narrative. It is more of a high-level overview of the software project. Such documentation is typically written separately from the source code, and a convenient format is markdown. We will discuss MkDocs, a tools to generate nice looking documentation based on this format. A reference guide serves a different purpose. It is intended to get low-level information on the contents of the library. It should be easy to navigate, so that one can view the definition of a datatype by clicking it when it occurs in the signature of a function. Hence this type of documentation integrates closely with the source code itself. If the latter changes, so should the former.","title":"Types of documentation"},{"location":"Documentation/documentation_best_practices/#pitfalls","text":"When working on a software project, one of the main pitfalls is neglecting documentation. Many developers don't particularly like writing documentation, and hence postpone it. The more classes and functions to be documented, the worse the tasks seems to be, and the larger the probability that it will never happen. Getting into the habit of immediately writing the reference documentation for each class or function as you code is certainly a \"best practice\". Although it may seem to slow you down while coding, that is not necessarily a bad thing. However, even when documentation is diligently written while coding, it has to be kept up to date when code changes. It is all too easy to forget to update the documentation when the signature of a function is modified. Inaccurate documentation is almost worse than no documentation since it is likely to cause serious software defects.","title":"Pitfalls"},{"location":"Documentation/documentation_best_practices/#what-to-document","text":"The first question to answer is, what do you need to document since a software project has many artefacts.","title":"What to document?"},{"location":"Documentation/documentation_best_practices/#documenting-functions","text":"Consider the documentation of a function or a method for instance. You would write a description of what the function does, which argument it takes, and what the return type is. However, you would also add the assumptions the function makes about the values its arguments can have. Such assumptions are called preconditions. By way of example, consider a function that takes a temperature as an argument. An important aspect is the units it expects the temperature to be specified in. Will that be degrees Celsius, or Fahrenheit, or Kelvin? In case it is degrees Celcius, it doesn't make sense to pass a value to the function that is less than -273.15 degrees Celsius. On the other hand, if it is Kelvin, the value should be positive. Needless to say that if you pass a temperature expressed in Kelvin to a function that expects degrees Celsius as a unit, the results will be wrong. This example illustrates an important point: the function documentation should clearly specify its expectation about the arguments that you pass to it. In our example, that would be the units of the temperature, but also the valid range of values. These restrictions are often called preconditions, and they constitute a contract between the user of the function and its author. The contract, i.e., the precondition, says that if the user of the function supplies arguments that fulfil all the preconditions, then the author guarantees that the function will return proper results. When considering values returned by a function, you have to consider similar restrictions. What are the units of the return value and what is the range of values that would make sense? This amounts to postconditions, again a contract between the function's author and its user. The contract, i.e., the postcondition, specifies that the user of the function can rely on it to return a value that fulfils all the conditions if she calls it correctly. When a function has side effects, i.e., it modifies one of its arguments, this should be stated as well. The \"principle of least astonishment\" is very useful in software development. Conversely, when passing an argument to a function that is not changed at all, that constitutes an invariant. Although it may be useful to state this in the documentation, it is far better practice to make that explicit by using const declarations in C/C++, or INTENT(IN) in Fortran. The concepts of preconditions, postconditions, and invariants were introduced in the \"design by contract\" paradigm, pioneered by Bertrand Meyer in 1986. If you make it part of your coding to immediately provide the documentation when writing a function, it will actually help you since you have to make your assumptions explicit, and hence may discover potential flaws. Documenting failure is very important as well. Can the function generate an error? Will it potentially throw an exception? What is the semantics of these errors or exceptions? Again, this is an application of the \"principle of least astonishment\". The user of your functions will be aware of potential failure, and can code to deal with it appropriately.","title":"Documenting functions"},{"location":"Documentation/documentation_best_practices/#documenting-user-defined-types-or-classes","text":"For user defined types, the semantics of the type should be documented. What does a variable of the type represent? Of course, the type name should be chosen so that this is clear, but it doesn't hurt to explain this more formally in the documentation as well. For each member, you should document its type, semantics, and, if applicable, the units that are expected. When doing object oriented programming in C++ or Fortran, you should give the same information for object and class attributes. The documentation of object and class methods is similar to that of functions.","title":"Documenting user defined types or classes"},{"location":"Documentation/documentation_best_practices/#documenting-modules","text":"Code is typically aggregated according to functionality in Fortran modules, or C/C++ files for separate compilation. Hence this aggregation has a purpose, and you should document that as well. What is the overall purpose of the module? Another question that you can answer in module documentation is the interaction between various components. How can the output of one function be used as the input for another, are the various functions independent, or should they be called in a certain order? As an example, if you have a function that initialises a data structure, a few that manipulate such data structures, and a finalisation function that releases the resources in that data structure, then the module documentation should probably mention that you should 1. call the initialisation function to get a valid data structure; 1. call functions that use the data structure; and 1. call the finalisation function to avoid memory leaks. At this level, you may want to add some example code of using the software components in that module. How do you call the functions, how can you use the return values, what are the use cases? This type of documentation could also be maintained at the project level, though.","title":"Documenting modules"},{"location":"Documentation/documentation_best_practices/#documenting-projects","text":"This is typically high-level documentation in the form of a tutorial with use cases and code samples. It should of course contain a link to the reference documentation.","title":"Documenting projects"},{"location":"Documentation/documentation_best_practices/#tools","text":"In this session, we will discuss two tools for creating attractive documentation, Doxygen and MkDocs. The former is best suited for reference guides, while the latter is excellent for tutorial-style material.","title":"Tools"},{"location":"Documentation/documentation_best_practices/#doxygen","text":"Some programming languages such as Java and Python provide support for documentation as part of their specification. The languages we use most frequently in an HPC context, C, C++, and Fortran, have no such provisions. However, Doxygen generates reference documentation out of comment blocks for a wide variety of programming languages, including those of interest to us. This documentation is fully hyperlinked. For instance, clicking the type of a function's argument will bring you to the type's documentation.","title":"Doxygen"},{"location":"Documentation/documentation_best_practices/#mkdocs","text":"MkDocs is a very convenient tool for generating nice looking documentation that can be viewed standalone as HTML pages, or that can be served from the Read the Docs service . It automatically generates a navigation panel and adds search functionality. You can also define a GitHub trigger that will automatically push your project's documentation to Read the Docs each time you do a release. Documentation of previous software versions remain available. In that scenario, MkDocs will provide useful previews before you make a release of your code project.","title":"MkDocs"},{"location":"Documentation/documentation_intro/","text":"Introduction to documentation Documentation is a very important, and often sadly neglected part of a software development project. When you are developing an application, you should write documentation for the user of your application to help her correctly use the software and its features, and interpret output, warnings and errors correctly. As part of that application, you create new classes and functions that form a library, and that can potentially be reused. Or, alternatively, the deliverable of your project might be a library as such, to be used by others. This library and its contents should be documented as well. Incorrect use of APIs (Application Programming Interfaces) or misconceptions about the semantics of functions is one of the major sources of bugs. Good quality and up-to-date documentation can help a lot to cut down on the number of the resulting code defects. We will discuss doxygen , a tool for generating nicely formatted API documentation, and that supports a wide range of programming languages. For application documentation, we take a look at mkdocs which can be integrated into your development process using GitHub and ReadTheDocs. Additionally, we will briefly discuss the difference between documentation and comments in code.","title":"Introduction"},{"location":"Documentation/documentation_intro/#introduction-to-documentation","text":"Documentation is a very important, and often sadly neglected part of a software development project. When you are developing an application, you should write documentation for the user of your application to help her correctly use the software and its features, and interpret output, warnings and errors correctly. As part of that application, you create new classes and functions that form a library, and that can potentially be reused. Or, alternatively, the deliverable of your project might be a library as such, to be used by others. This library and its contents should be documented as well. Incorrect use of APIs (Application Programming Interfaces) or misconceptions about the semantics of functions is one of the major sources of bugs. Good quality and up-to-date documentation can help a lot to cut down on the number of the resulting code defects. We will discuss doxygen , a tool for generating nicely formatted API documentation, and that supports a wide range of programming languages. For application documentation, we take a look at mkdocs which can be integrated into your development process using GitHub and ReadTheDocs. Additionally, we will briefly discuss the difference between documentation and comments in code.","title":"Introduction to documentation"},{"location":"Documentation/mkdocs/","text":"MkDocs MkDocs complements Doxygen nicely, since it is very useful for tutorial-style documentation. The format is MarkDown, a very simple text mark-up format that is ubiquitous nowadays. It is used in many online tools such as blogging software, but also in development tools such as GitHub and Jupyter notebooks. initialisation Suppose we have a project in a directory my_project that has a source directory and some files. Initially, the directory looks like this. my_project/ \u2514\u2500\u2500 src \u251c\u2500\u2500 bye.c \u251c\u2500\u2500 hello.c \u2514\u2500\u2500 Makefile When you want to start to document this project, you can set it up using MkDocs' new command, e.g., $ mkdocs new my_project The directory will now look as follows: my_project/ \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 src \u251c\u2500\u2500 bye.c \u251c\u2500\u2500 hello.c \u2514\u2500\u2500 Makefile A configuration file mkdocs.yml has been created, as well as a docs directory that will contain the MarkDown pages with the documentation, and an index.md file in that directory that serves as the entry point into your documentation. Configuration file The configuration file is in YAML format, and initially looks as follows: site_name: My Docs This will have to be customised to your requirements. The site name can be changed to, e.g., the name of your project. Several other meta-data items can be added, e.g., site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs You also have to specify where to create the documentation, e.g., in a subdirectory html_docs of my_project directory. site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs site_dir: html_docs/worker Lastly, you can specify the titles of the pages of your documentation, and the MarkDown files. site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs site_dir: html_docs/worker pages: - Introduction and motivation: 'index.md' - Step by step: 'steps.md' Of course the index.md file should be edited, and the steps.md file created. Documentation pages: MarkDown Individual pages are written as MarkDown text. Below is an example for steps.md . ## High-level description This project supports a workflow in several steps. 1. First you say hello, 1. then you say bye. ## Usage There are two applications to achieve this, `hello` and `bye` respectively. You can call `hello` as ```bash $ hello gjb ``` Here, `gjb` is the name of the person you want to say hello to. _Caution:_ if you use a name that doesn't exists, you will get an error. As you can see, it is almost a normal text, with some special annotation to define style elements. For instance, * # Heading 1 * ## Heading 2 * ### Heading 3 * \u2026 produce headings at various levels. Unordered lists can be created by using a two-spaced indentation, followed by * for each item, e.g., * First item * Second item Similarly, an ordered list can be created as follows, 1. First item 1. Second item Note that the numbering is automatically done for you, so reordering items is no issue. Text can have styles such as * emphasized : _emphasized_ * bold : __bold__ * ~~strike-through~~: ~~strike-through~~ * code : code Code snippets can be rendered with syntax highlighting as ```fortran IF (a < b) THEN a = a - b END IF ``` Hyperlinks can be added as [text](url) . Incidentally, this text has been written in MarkDown as well. Rendering documentation Once you are done, you can build the documentation by executing $ mkdocs build A very nice feature of MkDocs is that it can offer a live preview of your work in progress, which will shorten your turn-around time while writing documentation. It contains a web server that runs locally, and that you can point your browser to. $ mkdocs serve INFO - Building documentation... [I 180820 14:52:04 server:271] Serving on http://127.0.0.1:8000 [I 180820 14:52:04 handlers:58] Start watching changes You can view the documentation by entering the URL http://127.0.0.1:8000 in your web browser. When you edit the documentation, the pages in your web browser are automatically updated. The main page of MkDocs generated documentation for atools looks like this.","title":"MkDocs for application documentation"},{"location":"Documentation/mkdocs/#mkdocs","text":"MkDocs complements Doxygen nicely, since it is very useful for tutorial-style documentation. The format is MarkDown, a very simple text mark-up format that is ubiquitous nowadays. It is used in many online tools such as blogging software, but also in development tools such as GitHub and Jupyter notebooks.","title":"MkDocs"},{"location":"Documentation/mkdocs/#initialisation","text":"Suppose we have a project in a directory my_project that has a source directory and some files. Initially, the directory looks like this. my_project/ \u2514\u2500\u2500 src \u251c\u2500\u2500 bye.c \u251c\u2500\u2500 hello.c \u2514\u2500\u2500 Makefile When you want to start to document this project, you can set it up using MkDocs' new command, e.g., $ mkdocs new my_project The directory will now look as follows: my_project/ \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 src \u251c\u2500\u2500 bye.c \u251c\u2500\u2500 hello.c \u2514\u2500\u2500 Makefile A configuration file mkdocs.yml has been created, as well as a docs directory that will contain the MarkDown pages with the documentation, and an index.md file in that directory that serves as the entry point into your documentation.","title":"initialisation"},{"location":"Documentation/mkdocs/#configuration-file","text":"The configuration file is in YAML format, and initially looks as follows: site_name: My Docs This will have to be customised to your requirements. The site name can be changed to, e.g., the name of your project. Several other meta-data items can be added, e.g., site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs You also have to specify where to create the documentation, e.g., in a subdirectory html_docs of my_project directory. site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs site_dir: html_docs/worker Lastly, you can specify the titles of the pages of your documentation, and the MarkDown files. site_name: my_project documentation site_description: this is a project that implements some brilliant software. site_author: Geert Jan Bex theme: readthedocs site_dir: html_docs/worker pages: - Introduction and motivation: 'index.md' - Step by step: 'steps.md' Of course the index.md file should be edited, and the steps.md file created.","title":"Configuration file"},{"location":"Documentation/mkdocs/#documentation-pages-markdown","text":"Individual pages are written as MarkDown text. Below is an example for steps.md . ## High-level description This project supports a workflow in several steps. 1. First you say hello, 1. then you say bye. ## Usage There are two applications to achieve this, `hello` and `bye` respectively. You can call `hello` as ```bash $ hello gjb ``` Here, `gjb` is the name of the person you want to say hello to. _Caution:_ if you use a name that doesn't exists, you will get an error. As you can see, it is almost a normal text, with some special annotation to define style elements. For instance, * # Heading 1 * ## Heading 2 * ### Heading 3 * \u2026 produce headings at various levels. Unordered lists can be created by using a two-spaced indentation, followed by * for each item, e.g., * First item * Second item Similarly, an ordered list can be created as follows, 1. First item 1. Second item Note that the numbering is automatically done for you, so reordering items is no issue. Text can have styles such as * emphasized : _emphasized_ * bold : __bold__ * ~~strike-through~~: ~~strike-through~~ * code : code Code snippets can be rendered with syntax highlighting as ```fortran IF (a < b) THEN a = a - b END IF ``` Hyperlinks can be added as [text](url) . Incidentally, this text has been written in MarkDown as well.","title":"Documentation pages: MarkDown"},{"location":"Documentation/mkdocs/#rendering-documentation","text":"Once you are done, you can build the documentation by executing $ mkdocs build A very nice feature of MkDocs is that it can offer a live preview of your work in progress, which will shorten your turn-around time while writing documentation. It contains a web server that runs locally, and that you can point your browser to. $ mkdocs serve INFO - Building documentation... [I 180820 14:52:04 server:271] Serving on http://127.0.0.1:8000 [I 180820 14:52:04 handlers:58] Start watching changes You can view the documentation by entering the URL http://127.0.0.1:8000 in your web browser. When you edit the documentation, the pages in your web browser are automatically updated. The main page of MkDocs generated documentation for atools looks like this.","title":"Rendering documentation"},{"location":"Documentation/references/","text":"References Here you will find a number of references to additional reading material and software. Reading material Documenting Python code: a complete guide 10 software documentation best practices Core practices for agile/lean documentation Putting comments in code: the good, the bad, and the ugly by Bill Sourour Software stack Doxygen MkDocs Alternative to mkdocs: AsciiDoc , Sphinx .","title":"References"},{"location":"Documentation/references/#references","text":"Here you will find a number of references to additional reading material and software.","title":"References"},{"location":"Documentation/references/#reading-material","text":"Documenting Python code: a complete guide 10 software documentation best practices Core practices for agile/lean documentation Putting comments in code: the good, the bad, and the ugly by Bill Sourour","title":"Reading material"},{"location":"Documentation/references/#software-stack","text":"Doxygen MkDocs Alternative to mkdocs: AsciiDoc , Sphinx .","title":"Software stack"},{"location":"Intro/economics_of_bugs/","text":"Impact of bugs Economic impact You may wonder how much impact bugs have on our economy. Trust me, they cost a lot. In a recent study , researchers of Cambridge University, Undo and RogueWave found that the economic cost of software bugs can be estimated at US$ 312 billion annually. This is more than the GDP of some European countries. Some high-profile examples come to mind that cost an astounding amount of money. As far back as 1962, the Mariner-1 space probe mission failed due to a missing hyphen in the source code. This bug caused the guidance system to malfunction, and mission control had to activate the auto-destruct mechanism. Fortunately, the latter functioned correctly, but the incident cost the tax payer US$ 18 million. More recently, in 2009, a bug in the anti-lock-break software installed in Toyota Lexus cars resulted in the recall of 9 million cars, a financial loss of US$ 3 billion, and, most tragically, the death of four people. However, not only the effects of bugs come at a price. The time developers spend on finding and fixing bugs is also considerable. The same study quoted earlier also estimates that an average developer spends 50 % of her time debugging. It is important to catch bugs as early as possible in the development process. A study by the Systems Science Institute (IBM) estimates that the cost of fixing a bug in the quality assurance phase of a project is 15 times that of one found during development. A bug found when the software is already in production is 100 times as expensive. The damage to a company's reputation as a consequence of defective products can be translated directly into the value of its shares on the stock market as the recent meltdown and spectre bugs illustrated, although long-term consequences may be even more severe. Impact on science Although the cost of bugs in scientific software is much harder to determine, it should be clear that they can have a significant impact. Some researchers have expressed concerns about this particular topic, e.g., David Soergel who published an article with the title \"Rampant software errors may undermine scientific results\" . Perhaps Soergel overstates the problem, but it is at least embarrassing when a paper has to be retracted due to bugs in the software used to obtain the results. A Nature news features discusses leaked emails on climate research in which one of the researchers repeatedly refers to problems in his software as Yup, my awful programming strikes again. The article calls for professional training for scientists who develop software packages. In short, good coding practices and debugging skills are as important to scientists as they are to commercial software developers.","title":"Motivation"},{"location":"Intro/economics_of_bugs/#impact-of-bugs","text":"","title":"Impact of bugs"},{"location":"Intro/economics_of_bugs/#economic-impact","text":"You may wonder how much impact bugs have on our economy. Trust me, they cost a lot. In a recent study , researchers of Cambridge University, Undo and RogueWave found that the economic cost of software bugs can be estimated at US$ 312 billion annually. This is more than the GDP of some European countries. Some high-profile examples come to mind that cost an astounding amount of money. As far back as 1962, the Mariner-1 space probe mission failed due to a missing hyphen in the source code. This bug caused the guidance system to malfunction, and mission control had to activate the auto-destruct mechanism. Fortunately, the latter functioned correctly, but the incident cost the tax payer US$ 18 million. More recently, in 2009, a bug in the anti-lock-break software installed in Toyota Lexus cars resulted in the recall of 9 million cars, a financial loss of US$ 3 billion, and, most tragically, the death of four people. However, not only the effects of bugs come at a price. The time developers spend on finding and fixing bugs is also considerable. The same study quoted earlier also estimates that an average developer spends 50 % of her time debugging. It is important to catch bugs as early as possible in the development process. A study by the Systems Science Institute (IBM) estimates that the cost of fixing a bug in the quality assurance phase of a project is 15 times that of one found during development. A bug found when the software is already in production is 100 times as expensive. The damage to a company's reputation as a consequence of defective products can be translated directly into the value of its shares on the stock market as the recent meltdown and spectre bugs illustrated, although long-term consequences may be even more severe.","title":"Economic impact"},{"location":"Intro/economics_of_bugs/#impact-on-science","text":"Although the cost of bugs in scientific software is much harder to determine, it should be clear that they can have a significant impact. Some researchers have expressed concerns about this particular topic, e.g., David Soergel who published an article with the title \"Rampant software errors may undermine scientific results\" . Perhaps Soergel overstates the problem, but it is at least embarrassing when a paper has to be retracted due to bugs in the software used to obtain the results. A Nature news features discusses leaked emails on climate research in which one of the researchers repeatedly refers to problems in his software as Yup, my awful programming strikes again. The article calls for professional training for scientists who develop software packages. In short, good coding practices and debugging skills are as important to scientists as they are to commercial software developers.","title":"Impact on science"},{"location":"Intro/intro_dpd/","text":"Introduction to defensive programming and debugging When writing code, it is important to keep in mind the entire life cycle of the project. In the context of research, the initial scope is often just a single experiment. The quality of your code seems a minor concern since science is your core business, and code is just a means to an end. However, successful research is reproduced and built upon. The original code base will be extended beyond its original scope, either by you or your colleagues. Of course, the same argument also holds for code developed for business applications. It is important that you realize that code is more than just telling a piece of hardware what to do. It is also a means of communication with other researchers or developers, including your future self. Indeed, there will be those who actually read your code, and use it as a building block, or even the foundation for their work. No doubt, you've opened a source file, stared at it for minutes in dismay, wondering \"what the heck?\" Obviously, the author failed to clearly communicate his intentions, his reasoning. Do keep in mind that this author might have been you, a few months previously. Such experiences should serve as motivation to try to convey your intentions and reasoning clearly. In a way, you can view coding as story telling. So source code should be clear, and pleasant to read. That will make it easy to follow the narrative. By sticking to a few simple rules, you can ensure that you write good code. In this section, we will discuss some coding best practices. Another potential source of problems is using someone else's software. Quite likely, you already got frustrated at the lack of documentation or relevant examples. So when you make your own application or library available to others, documentation should be an integral part of the release. Documentation should be clearly written, with ample examples. It should also be up-to-date with the latest version of your code. We will discuss a number of dos and don'ts for writing documentation. We will also introduce some tools to help you generate nicely formatted texts. Donald Knuth once said \"Beware of bugs in the above code; I have only proved it correct, not tried it.\" You could paraphrase that as follows, \"code that has not been tested doesn't work.\" Writing good tests is an important part of any sizable software development project. We will cover both unit testing and integration testing since they target specific and complementary aspects of an application. Both testing best practices and testing frameworks will be discussed.","title":"Introduction to defensive programming and debugging"},{"location":"Intro/intro_dpd/#introduction-to-defensive-programming-and-debugging","text":"When writing code, it is important to keep in mind the entire life cycle of the project. In the context of research, the initial scope is often just a single experiment. The quality of your code seems a minor concern since science is your core business, and code is just a means to an end. However, successful research is reproduced and built upon. The original code base will be extended beyond its original scope, either by you or your colleagues. Of course, the same argument also holds for code developed for business applications. It is important that you realize that code is more than just telling a piece of hardware what to do. It is also a means of communication with other researchers or developers, including your future self. Indeed, there will be those who actually read your code, and use it as a building block, or even the foundation for their work. No doubt, you've opened a source file, stared at it for minutes in dismay, wondering \"what the heck?\" Obviously, the author failed to clearly communicate his intentions, his reasoning. Do keep in mind that this author might have been you, a few months previously. Such experiences should serve as motivation to try to convey your intentions and reasoning clearly. In a way, you can view coding as story telling. So source code should be clear, and pleasant to read. That will make it easy to follow the narrative. By sticking to a few simple rules, you can ensure that you write good code. In this section, we will discuss some coding best practices. Another potential source of problems is using someone else's software. Quite likely, you already got frustrated at the lack of documentation or relevant examples. So when you make your own application or library available to others, documentation should be an integral part of the release. Documentation should be clearly written, with ample examples. It should also be up-to-date with the latest version of your code. We will discuss a number of dos and don'ts for writing documentation. We will also introduce some tools to help you generate nicely formatted texts. Donald Knuth once said \"Beware of bugs in the above code; I have only proved it correct, not tried it.\" You could paraphrase that as follows, \"code that has not been tested doesn't work.\" Writing good tests is an important part of any sizable software development project. We will cover both unit testing and integration testing since they target specific and complementary aspects of an application. Both testing best practices and testing frameworks will be discussed.","title":"Introduction to defensive programming and debugging"},{"location":"Intro/references/","text":"References (In)famous bugs Ariane-5 disaster : a type conversion bug in the inertial reference system caused the rocket to explode shortly after launch. Aliens: colonial marines : a bug in a configuration file completely messed up the behaviour of the aliens in this game. Epic failures: 11 infamous software bugs \"It's not a bug, it's a feature.\" trite---or just right? \"The 44-year-old operating system bug\" They say software will eat the world. Bug bounties Apple's FaceTime bug : Apple pays bounty and scholarship for 14-year-old discoverer. How to earn money as a bug hunter Life as a bug hunter : a struggle just to get paid. Bug bounty programs : everything you thought you knew is wrong. Europe to fund bug bounties for open source software","title":"References"},{"location":"Intro/references/#references","text":"","title":"References"},{"location":"Intro/references/#infamous-bugs","text":"Ariane-5 disaster : a type conversion bug in the inertial reference system caused the rocket to explode shortly after launch. Aliens: colonial marines : a bug in a configuration file completely messed up the behaviour of the aliens in this game. Epic failures: 11 infamous software bugs \"It's not a bug, it's a feature.\" trite---or just right? \"The 44-year-old operating system bug\" They say software will eat the world.","title":"(In)famous bugs"},{"location":"Intro/references/#bug-bounties","text":"Apple's FaceTime bug : Apple pays bounty and scholarship for 14-year-old discoverer. How to earn money as a bug hunter Life as a bug hunter : a struggle just to get paid. Bug bounty programs : everything you thought you knew is wrong. Europe to fund bug bounties for open source software","title":"Bug bounties"},{"location":"Intro/scope/","text":"Scope The primary target audience of this text are developers of scientific software, especially in the context of high performance computing (HPC). On HPC systems, Linux is the dominant operating system, and programming is mostly done in C, C++ or Fortran. We assume that you are familiar with the Linux command line, and are proficient in at least one of these programming languages. Also bear in mind that it is often necessary to work via a terminal only on HPC systems, so although we will discuss GUI applications for debugging and verification, the emphasis will be on terminal based tools.","title":"Scope"},{"location":"Intro/scope/#scope","text":"The primary target audience of this text are developers of scientific software, especially in the context of high performance computing (HPC). On HPC systems, Linux is the dominant operating system, and programming is mostly done in C, C++ or Fortran. We assume that you are familiar with the Linux command line, and are proficient in at least one of these programming languages. Also bear in mind that it is often necessary to work via a terminal only on HPC systems, so although we will discuss GUI applications for debugging and verification, the emphasis will be on terminal based tools.","title":"Scope"},{"location":"Intro/software/","text":"List of software This is a list of the software required to follow along. The version numbers the ones we tested with on Ubuntu 18.04. Most likely everything should just work fine on any other modern Linux distribution. Software that can be installed using a package manager doxygen 1.8.13 graphviz 2.40.1 mkdocs 1.0.4 gcc 7.3.0 g++ 7.3.0 gfortran 7.3.0 cunit 2.1-3 Catch2 2.5.0 Cppcheck 1.82 clang 6.0.0 clang++ 6.0.0 clang-tidy (optional) pkg-config 0.29.1 gdb 8.1.0 valgrind 3.13.0 git 2.17.1 make 4.1 automake 1.15.1 cmake 3.10.2 gnuplot 5.2 patchlevel 2 Software that has to be build pfUnit 3.2.9 Commerical software Arm DDT: a trial license can be obtained from Arm that is valid for a week Intel Cluster edition 2018: a trial license can be obtained from Ingel that is valid for a month icc icpc ifort Intel MPI Inspector ITAC","title":"Software environment"},{"location":"Intro/software/#list-of-software","text":"This is a list of the software required to follow along. The version numbers the ones we tested with on Ubuntu 18.04. Most likely everything should just work fine on any other modern Linux distribution.","title":"List of software"},{"location":"Intro/software/#software-that-can-be-installed-using-a-package-manager","text":"doxygen 1.8.13 graphviz 2.40.1 mkdocs 1.0.4 gcc 7.3.0 g++ 7.3.0 gfortran 7.3.0 cunit 2.1-3 Catch2 2.5.0 Cppcheck 1.82 clang 6.0.0 clang++ 6.0.0 clang-tidy (optional) pkg-config 0.29.1 gdb 8.1.0 valgrind 3.13.0 git 2.17.1 make 4.1 automake 1.15.1 cmake 3.10.2 gnuplot 5.2 patchlevel 2","title":"Software that can be installed using a package manager"},{"location":"Intro/software/#software-that-has-to-be-build","text":"pfUnit 3.2.9","title":"Software that has to be build"},{"location":"Intro/software/#commerical-software","text":"Arm DDT: a trial license can be obtained from Arm that is valid for a week Intel Cluster edition 2018: a trial license can be obtained from Ingel that is valid for a month icc icpc ifort Intel MPI Inspector ITAC","title":"Commerical software"},{"location":"Taxonomy/arithmetic_bugs/","text":"Arithmetic bugs Given that computing numerical results is at the heart of almost all scientific software, this category of bugs is quite important. The primary source of issues is that you will be thinking in mathematical terms, rather than computational terms. Integers In mathematics, the set of integer number is infinite, while in most programming languages, integers are represented by 8, 16, 32 or 64 bits. For simplicity, we will only consider 32 bit integers, but the same argument holds for the other representations. Overflow This means that the largest signed integer that can be represented using 32 bits is 2^31 - 1 . The smallest signed 32 bits integer is -2^31 . Adding 1 to that number causes a numerical overflow, and will result in a negative number, -2^31 . Obviously, all further computations based on that result are meaningless. The situation is of course symmetric, similar problems will arise when subtracting 1 from the smallest integer. Integer numerical overflow can be trapped at runtime using compiler flags. Divide by zero Interestingly, an integer division by zero will result in runtime error due to a floating point exception. The application will crash. Real numbers For real numbers, the situation is more complicated. Again, in mathematics the set of real number is infinite, but there are infinitely many real numbers between any given two real numbers as well. Real numbers are represented as floating point numbers with 16 bits (half precision), 32 bits (single precision), 64 bits (double precision), or 128 bits (quadruple precision). This implies that almost no real number can be represented exactly as a floating point number, which has a number of unpleasant consequences. For simplicity, we will only discuss the 32 bit (single precision) representation, the same arguments hold for all other representations with the corresponding values for the constants involved. Overflow Just like for integers, overflow can be an issue. The largest floating point number is 3.40282347E+38 . When a computation results in a number larger than this value, an overflow occurs, and the result will be Infinity . All further computations will results in either infinity, or NaN (Not a Number). Underflow The smallest strictly positive floating point number that can be represented is 1.17549435E-38 . Computations that result in smaller strictly positive values will be rounded to zero, which is an underflow. This type of problem is of course harder to spot. The result might genuinely be zero, so this situation has to be handled with care if it can arise. Underflow may be the result of multiplying two small floating point numbers. Just like for overflow, underflow illustrates that associativity may not always hold. For example, consider (1.0E20 * 1.0E-20) * (1.0E-20 * 1.0E20) versus 1.0E20 * (1.0E-20 * 1.0E-20) * 1.0E20 The first expression evaluates to 1.0, the second to 0.0, although they are mathematically equivalent. Another source of numerical underflow would be applying the exp function to a large negative number. Round off & loss of precision Round off errors can also have unpleasant consequences. The smallest floating point value that can be added to 1 such that the result is different from 1 is 1.19209290E-07 ., so in single precision, 1.0 + 1.0E-08 would be rounded to 1.0 . Round off also implies that the addition of floating point numbers is not associative, i.e., a + (b + c) is not necessarily equal to (a + b) + c . It is especially important that you realize this when adding many floating point numbers that have different orders of magnitude. As a trivial illustration, consider the following two expressions: (1.0 + 7.0E-08) + 7.0E-08 versus 1.0 + (7.0E-08 + 7.0E-08) From a mathematical point of view, they should yield the same result, however, this is not the case for floating point numbers. Although in general this is fairly innocent, it may lead to entirely wrong results when the numerical algorithm is not robust against this issue. You may solve the issue by choosing a more precise floating point representation, but that will come at a price. Your application will require more memory to store the data, and the performance may be impacted by as much has a factor of 2. Usually, it pays to check whether another algorithm may be a better solution. Divide by zero and invalid operations Dividing a floating point number by zero results in Infinity , and no exception is thrown by default. Division by zero can be caught at runtime when using GCC's sanitizer. Similar, operations that are invalid, e.g., sqrt(-1.0) will result in NaN (Not a Number), while your application will happily continue to compute nonsense.","title":"Arithmetic bugs"},{"location":"Taxonomy/arithmetic_bugs/#arithmetic-bugs","text":"Given that computing numerical results is at the heart of almost all scientific software, this category of bugs is quite important. The primary source of issues is that you will be thinking in mathematical terms, rather than computational terms.","title":"Arithmetic bugs"},{"location":"Taxonomy/arithmetic_bugs/#integers","text":"In mathematics, the set of integer number is infinite, while in most programming languages, integers are represented by 8, 16, 32 or 64 bits. For simplicity, we will only consider 32 bit integers, but the same argument holds for the other representations.","title":"Integers"},{"location":"Taxonomy/arithmetic_bugs/#overflow","text":"This means that the largest signed integer that can be represented using 32 bits is 2^31 - 1 . The smallest signed 32 bits integer is -2^31 . Adding 1 to that number causes a numerical overflow, and will result in a negative number, -2^31 . Obviously, all further computations based on that result are meaningless. The situation is of course symmetric, similar problems will arise when subtracting 1 from the smallest integer. Integer numerical overflow can be trapped at runtime using compiler flags.","title":"Overflow"},{"location":"Taxonomy/arithmetic_bugs/#divide-by-zero","text":"Interestingly, an integer division by zero will result in runtime error due to a floating point exception. The application will crash.","title":"Divide by zero"},{"location":"Taxonomy/arithmetic_bugs/#real-numbers","text":"For real numbers, the situation is more complicated. Again, in mathematics the set of real number is infinite, but there are infinitely many real numbers between any given two real numbers as well. Real numbers are represented as floating point numbers with 16 bits (half precision), 32 bits (single precision), 64 bits (double precision), or 128 bits (quadruple precision). This implies that almost no real number can be represented exactly as a floating point number, which has a number of unpleasant consequences. For simplicity, we will only discuss the 32 bit (single precision) representation, the same arguments hold for all other representations with the corresponding values for the constants involved.","title":"Real numbers"},{"location":"Taxonomy/arithmetic_bugs/#overflow_1","text":"Just like for integers, overflow can be an issue. The largest floating point number is 3.40282347E+38 . When a computation results in a number larger than this value, an overflow occurs, and the result will be Infinity . All further computations will results in either infinity, or NaN (Not a Number).","title":"Overflow"},{"location":"Taxonomy/arithmetic_bugs/#underflow","text":"The smallest strictly positive floating point number that can be represented is 1.17549435E-38 . Computations that result in smaller strictly positive values will be rounded to zero, which is an underflow. This type of problem is of course harder to spot. The result might genuinely be zero, so this situation has to be handled with care if it can arise. Underflow may be the result of multiplying two small floating point numbers. Just like for overflow, underflow illustrates that associativity may not always hold. For example, consider (1.0E20 * 1.0E-20) * (1.0E-20 * 1.0E20) versus 1.0E20 * (1.0E-20 * 1.0E-20) * 1.0E20 The first expression evaluates to 1.0, the second to 0.0, although they are mathematically equivalent. Another source of numerical underflow would be applying the exp function to a large negative number.","title":"Underflow"},{"location":"Taxonomy/arithmetic_bugs/#round-off-loss-of-precision","text":"Round off errors can also have unpleasant consequences. The smallest floating point value that can be added to 1 such that the result is different from 1 is 1.19209290E-07 ., so in single precision, 1.0 + 1.0E-08 would be rounded to 1.0 . Round off also implies that the addition of floating point numbers is not associative, i.e., a + (b + c) is not necessarily equal to (a + b) + c . It is especially important that you realize this when adding many floating point numbers that have different orders of magnitude. As a trivial illustration, consider the following two expressions: (1.0 + 7.0E-08) + 7.0E-08 versus 1.0 + (7.0E-08 + 7.0E-08) From a mathematical point of view, they should yield the same result, however, this is not the case for floating point numbers. Although in general this is fairly innocent, it may lead to entirely wrong results when the numerical algorithm is not robust against this issue. You may solve the issue by choosing a more precise floating point representation, but that will come at a price. Your application will require more memory to store the data, and the performance may be impacted by as much has a factor of 2. Usually, it pays to check whether another algorithm may be a better solution.","title":"Round off &amp; loss of precision"},{"location":"Taxonomy/arithmetic_bugs/#divide-by-zero-and-invalid-operations","text":"Dividing a floating point number by zero results in Infinity , and no exception is thrown by default. Division by zero can be caught at runtime when using GCC's sanitizer. Similar, operations that are invalid, e.g., sqrt(-1.0) will result in NaN (Not a Number), while your application will happily continue to compute nonsense.","title":"Divide by zero and invalid operations"},{"location":"Taxonomy/data_bugs/","text":"Bugs in data Some bugs in this category aren't directly caused by you as a programmer, but rather by the user of your software. Any application requires data as input. This can be as trivial as a few command line arguments, or as complex as reading a data file in some arcane format. If the input data doesn't meet the expectations of your application, the latter may crash, or worse, produce incorrect results. Although there are no actual tools to help out, a few tips may prove valuable. Data conversion is another source of problems, and here careless programming can actually be the problem. Input data Validate your input data. Ensure that when you convert a string representation to, e.g., a number, you don't use functions that silently fail, no errors are raised that you do not handle, and the string is processed completely. Examples were given in the sections on error handling. If there are some requirements on the input data, you would be wise to verify that they are met. For instance, if you expect a number to be positive, check that it is. Input validation is one of the areas where Grace Hopper's maxim doesn't apply. Asking permission is a bit of a bother, and sometimes a lot of work, but far better than having to ask forgiveness. When you define a file format for input data, it is good practice to write a validator. Such an application simply parses the data file, and validates its contents, giving warnings and errors if necessary. The functions for this implementation can be reused in the application(s) that actually process this data. Never write your own code to parse a data format if an off-the-shelf library is available. Even deceivingly simple data formats such as comma separated values files (CSV) are surprisingly hard to parse correctly. There are quite some edge and corner cases to consider, many caused by platform specific issues. Don't even think of implementing your own parser for XML. Fortunately, implementations for parsers that deal with common scientific data formats are available for Fortran, C, and C++. Using those will improve the robustness of your code. If they fail, oh well, at least you can blame someone else. Note that there is an overlap here with the section on bugs in requirements. Carefully specifying the requirements for command line arguments and input data will help you develop accurate validators and more robust code. Data conversion A fairly large number of bugs is caused by inappropriate data conversion. In some cases, you may potentially loose information due to conversion. For instance, converting 64 bits integers to 32 bits values will in general cause problems. Similar, converting a double precision floating point value to single precision will loose precision. These are called 'narrowing conversions'. C/C++ C and C++ compilers will not prevent you from doing narrowing conversions by default. Even with -Wall and -Wextra enabled you will get no warnings. However, adding the -Wconversion flag proves to be quite useful. It will produce warnings for each narrowing conversion in the following code fragment. #include <stdio.h> int main() { long a = 94850485030; long b = 495849853000; int c = a + b; printf(\"c = %d\\n\", c); double x = 1.435e67; double y = 4.394e89; float z = x + y; printf(\"z = %e\\n\", z); int d = x; printf(\"d = %d\\n\", d); return 0; } Fortran The gfortran compiler will give you warnings when you specify -Wall , so it will give three for the following code: program conversions use, intrinsic :: iso_fortran_env, only : & r8 => REAL64, r4 => REAL32, i8 => INT64, i4 => INT32 implicit none real(kind=r8) :: x, y real(kind=r4) :: z integer(kind=i8) :: a, b integer(kind=i4) :: c, d x = 1.394e76_r8 y = 2.37e56_r8 z = x + y print '(E15.5)', z a = 309403103049_i8 b = 49031944903_i8 c = a + b print '(I15)', c d = z print '(I15)', d end program conversions","title":"Bugs in data"},{"location":"Taxonomy/data_bugs/#bugs-in-data","text":"Some bugs in this category aren't directly caused by you as a programmer, but rather by the user of your software. Any application requires data as input. This can be as trivial as a few command line arguments, or as complex as reading a data file in some arcane format. If the input data doesn't meet the expectations of your application, the latter may crash, or worse, produce incorrect results. Although there are no actual tools to help out, a few tips may prove valuable. Data conversion is another source of problems, and here careless programming can actually be the problem.","title":"Bugs in data"},{"location":"Taxonomy/data_bugs/#input-data","text":"Validate your input data. Ensure that when you convert a string representation to, e.g., a number, you don't use functions that silently fail, no errors are raised that you do not handle, and the string is processed completely. Examples were given in the sections on error handling. If there are some requirements on the input data, you would be wise to verify that they are met. For instance, if you expect a number to be positive, check that it is. Input validation is one of the areas where Grace Hopper's maxim doesn't apply. Asking permission is a bit of a bother, and sometimes a lot of work, but far better than having to ask forgiveness. When you define a file format for input data, it is good practice to write a validator. Such an application simply parses the data file, and validates its contents, giving warnings and errors if necessary. The functions for this implementation can be reused in the application(s) that actually process this data. Never write your own code to parse a data format if an off-the-shelf library is available. Even deceivingly simple data formats such as comma separated values files (CSV) are surprisingly hard to parse correctly. There are quite some edge and corner cases to consider, many caused by platform specific issues. Don't even think of implementing your own parser for XML. Fortunately, implementations for parsers that deal with common scientific data formats are available for Fortran, C, and C++. Using those will improve the robustness of your code. If they fail, oh well, at least you can blame someone else. Note that there is an overlap here with the section on bugs in requirements. Carefully specifying the requirements for command line arguments and input data will help you develop accurate validators and more robust code.","title":"Input data"},{"location":"Taxonomy/data_bugs/#data-conversion","text":"A fairly large number of bugs is caused by inappropriate data conversion. In some cases, you may potentially loose information due to conversion. For instance, converting 64 bits integers to 32 bits values will in general cause problems. Similar, converting a double precision floating point value to single precision will loose precision. These are called 'narrowing conversions'.","title":"Data conversion"},{"location":"Taxonomy/data_bugs/#cc","text":"C and C++ compilers will not prevent you from doing narrowing conversions by default. Even with -Wall and -Wextra enabled you will get no warnings. However, adding the -Wconversion flag proves to be quite useful. It will produce warnings for each narrowing conversion in the following code fragment. #include <stdio.h> int main() { long a = 94850485030; long b = 495849853000; int c = a + b; printf(\"c = %d\\n\", c); double x = 1.435e67; double y = 4.394e89; float z = x + y; printf(\"z = %e\\n\", z); int d = x; printf(\"d = %d\\n\", d); return 0; }","title":"C/C++"},{"location":"Taxonomy/data_bugs/#fortran","text":"The gfortran compiler will give you warnings when you specify -Wall , so it will give three for the following code: program conversions use, intrinsic :: iso_fortran_env, only : & r8 => REAL64, r4 => REAL32, i8 => INT64, i4 => INT32 implicit none real(kind=r8) :: x, y real(kind=r4) :: z integer(kind=i8) :: a, b integer(kind=i4) :: c, d x = 1.394e76_r8 y = 2.37e56_r8 z = x + y print '(E15.5)', z a = 309403103049_i8 b = 49031944903_i8 c = a + b print '(I15)', c d = z print '(I15)', d end program conversions","title":"Fortran"},{"location":"Taxonomy/data_races_deadlocks/","text":"Data races and deadlocks Data races and deadlocks are bugs that occur only in the context of parallel programming. Data races Data races can lead to incorrect results of computations. Potentially, this type of bug is subtle, and may go unnoticed for a long time. Formal definition Formally, a data race will occur when two or more threads access the same memory location concurrently; at least one thread accesses that memory location for writing; and no implicit or explicit locks are used to control access. Data races in OpenMP and MPI Both OpenMP and MPI applications may be susceptible to this type of bug. For OpenMP, there are several ways to introduce a data race, e.g., a variable that should be thread-private is shared; a shared variable is used for a reduction operation, but neither a reduction clause, a critical or an atomic directive is used to guarantee atomic updates. an inappropriate nowait clause on a workshare directive. In MPI application, data races can be caused by, e.g., reusing a communication buffer before that was save, e.g., asynchronous point-to-point communication or collectives; inappropriately or missing active target synchronisation when using one-sided communication; inappropriate or missing fences for passive target synchronisation when using one-sided communication; shared memory operations without proper synchronisation. Data races in multi-threaded application can be detected using Valgrind or Intel Inspector. For OpenMP, only the latter will work out of the box. Deadlocks Informally, a deadlock occurs in a concurrent system when each process is waiting for some other process to take action. Formal definition Formally, a deadlock situation occurs when all the following four conditions are met: Mutual exclusion: At least one resource must be held in a non-shareable mode. Otherwise, the processes would not be prevented from using the resource when necessary. Only one process can use the resource at any given instant of time. Hold and wait or resource holding: a process is currently holding at least one resource and requesting additional resources which are being held by other processes. No preemption: a resource can be released only voluntarily by the process holding it. Circular wait: each process must be waiting for a resource which is being held by another process, which in turn is waiting for the first process to release the resource. In general, there is a set of waiting processes, P = {P1, P2, ..., PN}, such that P1 is waiting for a resource held by P2, P2 is waiting for a resource held by P3 and so on until PN is waiting for a resource held by P1. These are known as the Coffman conditions. Example: web shopping As an example, consider Alice and Bob using an online shopping application. Both are a fan of the obscure author Justin Aane who wrote the novel \"Pride\", and its sequel \"Prejudice\". The web shop has only one copy of each left in stock. Alice adds \"Pride\" to her shopping basket, while Bob adds \"Prejudice\" to his. Now Alice would would like to add \"Prejudice\", while Bob would like to add \"Pride\". However, both see that the books are no longer in stock. They decide to keep the novel they already put in their shopping basket, and to wait until the missing book is back in stock to make the purchase. However, both \"Pride\" and \"Prejudice\" are out print, and Alice and Bob find themselves in a deadlock situation. Formally, the books are the resources and our protagonists, Alice and Bob the processes. A resource (book) is held by a process (person) when it is in that persons shopping basket. Since there is just a single copy of each book, having one in a shopping basket is mutually exclusive: either Alice or Bob can have it in their respective basket, but not both simultaneously. (Condition 1: mutual exclusion) Both Alice and Bob hold on to the content of their shopping basket, while waiting to add the missing volume. (Condition 2: hold and wait) Bob can't make Alice remove her \"Pride\" from her shopping basket, and Alice has no influence over Bob's basket either. (Condition 3: no preemption) Alice is waiting for \"Prejudice\", held by Bob, who is in turn waiting for \"Pride\", held by Alice. (Condition 4: circular wait) Deadlocks in OpenMP and MPI For OpenMP application, deadlocks will not occur, unless you are careless when using explicit lock functions from its runtime library. In MPI applications, there are several ways to create a deadlock, e.g., both partners in point-to-point communication do a blocking synchronous send ( MPI_Ssend , or an MPI_Send that does an MPI_Ssend ); not all processes in a communicator participate in a collective communication operation; active target synchronisation for one-sided communication is implemented incorrectly ( MPI_Win_post / MPI_Win_start / MPI_Win_complete / MPI_Win_wait ). Deadlocks in multi-threaded application can be detected using Valgrind or Intel Inspector. For OpenMP, only the latter will work out of the box. Deadlocks in MPI applications can be diagnosed by Intel Trace Analyzer and Collector (ITAC) or MUST.","title":"Data races & deadlocks"},{"location":"Taxonomy/data_races_deadlocks/#data-races-and-deadlocks","text":"Data races and deadlocks are bugs that occur only in the context of parallel programming.","title":"Data races and deadlocks"},{"location":"Taxonomy/data_races_deadlocks/#data-races","text":"Data races can lead to incorrect results of computations. Potentially, this type of bug is subtle, and may go unnoticed for a long time.","title":"Data races"},{"location":"Taxonomy/data_races_deadlocks/#formal-definition","text":"Formally, a data race will occur when two or more threads access the same memory location concurrently; at least one thread accesses that memory location for writing; and no implicit or explicit locks are used to control access.","title":"Formal definition"},{"location":"Taxonomy/data_races_deadlocks/#data-races-in-openmp-and-mpi","text":"Both OpenMP and MPI applications may be susceptible to this type of bug. For OpenMP, there are several ways to introduce a data race, e.g., a variable that should be thread-private is shared; a shared variable is used for a reduction operation, but neither a reduction clause, a critical or an atomic directive is used to guarantee atomic updates. an inappropriate nowait clause on a workshare directive. In MPI application, data races can be caused by, e.g., reusing a communication buffer before that was save, e.g., asynchronous point-to-point communication or collectives; inappropriately or missing active target synchronisation when using one-sided communication; inappropriate or missing fences for passive target synchronisation when using one-sided communication; shared memory operations without proper synchronisation. Data races in multi-threaded application can be detected using Valgrind or Intel Inspector. For OpenMP, only the latter will work out of the box.","title":"Data races in OpenMP and MPI"},{"location":"Taxonomy/data_races_deadlocks/#deadlocks","text":"Informally, a deadlock occurs in a concurrent system when each process is waiting for some other process to take action.","title":"Deadlocks"},{"location":"Taxonomy/data_races_deadlocks/#formal-definition_1","text":"Formally, a deadlock situation occurs when all the following four conditions are met: Mutual exclusion: At least one resource must be held in a non-shareable mode. Otherwise, the processes would not be prevented from using the resource when necessary. Only one process can use the resource at any given instant of time. Hold and wait or resource holding: a process is currently holding at least one resource and requesting additional resources which are being held by other processes. No preemption: a resource can be released only voluntarily by the process holding it. Circular wait: each process must be waiting for a resource which is being held by another process, which in turn is waiting for the first process to release the resource. In general, there is a set of waiting processes, P = {P1, P2, ..., PN}, such that P1 is waiting for a resource held by P2, P2 is waiting for a resource held by P3 and so on until PN is waiting for a resource held by P1. These are known as the Coffman conditions.","title":"Formal definition"},{"location":"Taxonomy/data_races_deadlocks/#example-web-shopping","text":"As an example, consider Alice and Bob using an online shopping application. Both are a fan of the obscure author Justin Aane who wrote the novel \"Pride\", and its sequel \"Prejudice\". The web shop has only one copy of each left in stock. Alice adds \"Pride\" to her shopping basket, while Bob adds \"Prejudice\" to his. Now Alice would would like to add \"Prejudice\", while Bob would like to add \"Pride\". However, both see that the books are no longer in stock. They decide to keep the novel they already put in their shopping basket, and to wait until the missing book is back in stock to make the purchase. However, both \"Pride\" and \"Prejudice\" are out print, and Alice and Bob find themselves in a deadlock situation. Formally, the books are the resources and our protagonists, Alice and Bob the processes. A resource (book) is held by a process (person) when it is in that persons shopping basket. Since there is just a single copy of each book, having one in a shopping basket is mutually exclusive: either Alice or Bob can have it in their respective basket, but not both simultaneously. (Condition 1: mutual exclusion) Both Alice and Bob hold on to the content of their shopping basket, while waiting to add the missing volume. (Condition 2: hold and wait) Bob can't make Alice remove her \"Pride\" from her shopping basket, and Alice has no influence over Bob's basket either. (Condition 3: no preemption) Alice is waiting for \"Prejudice\", held by Bob, who is in turn waiting for \"Pride\", held by Alice. (Condition 4: circular wait)","title":"Example: web shopping"},{"location":"Taxonomy/data_races_deadlocks/#deadlocks-in-openmp-and-mpi","text":"For OpenMP application, deadlocks will not occur, unless you are careless when using explicit lock functions from its runtime library. In MPI applications, there are several ways to create a deadlock, e.g., both partners in point-to-point communication do a blocking synchronous send ( MPI_Ssend , or an MPI_Send that does an MPI_Ssend ); not all processes in a communicator participate in a collective communication operation; active target synchronisation for one-sided communication is implemented incorrectly ( MPI_Win_post / MPI_Win_start / MPI_Win_complete / MPI_Win_wait ). Deadlocks in multi-threaded application can be detected using Valgrind or Intel Inspector. For OpenMP, only the latter will work out of the box. Deadlocks in MPI applications can be diagnosed by Intel Trace Analyzer and Collector (ITAC) or MUST.","title":"Deadlocks in OpenMP and MPI"},{"location":"Taxonomy/requirements_bugs/","text":"Requirements Although projects that develop scientific software rarely start with a formal phase in which requirements are gathered and analysed, every programmer will have an informal list in mind at the start of the project. Actually, you could argue that the more accurate the list of requirements, the better the chances that the project will be successful, and that it will be finished within the planned time. Many project are not delivered on time, or even fail due to problems caused by misconceptions about the requirements. It is definitely worth to invest time and effort in this phase of a project, since it will save you trouble down the road. When drawing up a list, questions you should ask yourself are the following. Is the list complete, what is missing? Are the various requirements consistent, are there contradictions? Is the formulation clear, or is there ambiguity? What are the dependencies between the requirements, are some more important than others? Can all requirements be met, and at what cost? Are all requested features really required? The requirements and these questions about them should be discussed with all the stakeholders for your project. This may not seem to be a cause of bugs as such, since the code does what the programmer intends it to. However, if it is based on flawed requirements, it will not meet the expectations of the user, and this is a defect. Good discussion and examples can be found in the literature on agile development.","title":"Bugs in requirements"},{"location":"Taxonomy/requirements_bugs/#requirements","text":"Although projects that develop scientific software rarely start with a formal phase in which requirements are gathered and analysed, every programmer will have an informal list in mind at the start of the project. Actually, you could argue that the more accurate the list of requirements, the better the chances that the project will be successful, and that it will be finished within the planned time. Many project are not delivered on time, or even fail due to problems caused by misconceptions about the requirements. It is definitely worth to invest time and effort in this phase of a project, since it will save you trouble down the road. When drawing up a list, questions you should ask yourself are the following. Is the list complete, what is missing? Are the various requirements consistent, are there contradictions? Is the formulation clear, or is there ambiguity? What are the dependencies between the requirements, are some more important than others? Can all requirements be met, and at what cost? Are all requested features really required? The requirements and these questions about them should be discussed with all the stakeholders for your project. This may not seem to be a cause of bugs as such, since the code does what the programmer intends it to. However, if it is based on flawed requirements, it will not meet the expectations of the user, and this is a defect. Good discussion and examples can be found in the literature on agile development.","title":"Requirements"},{"location":"Taxonomy/structural_bugs/","text":"Structural bugs These issues are likely the ones that first come to mind when you think of bugs. This is a very broad category that can be divided into a number of subcategories. Control flow Logic Processing Initialisation In the sections below, each type will be discussed, and examples will be given. Control flow Loop termination A prime example of a control flow bug is improper termination of loops. Programmers who switch between 1-based languages (e.g., Fortran, MATLAB, R, ...) and 0-based languages (C, C++, Python) or those just starting to use the latter are prone to this. Consider the following C code fragment: const int n = 5; double data[n]; for (int i = 0; i <= n; i++) data[i] = some_function(i); Although the loop index i starts at 0, which is correct, during the last iteration i will be equal to n , so data[n] will try to access the array out of bounds, since it's last element is at index n - 1 , rather than n . This situation is somewhat less likely to happen in Fortran code. Out of bound array access can be detected using compiler flags and Valgrind. The following example involves a while iteration, and is also quite common. The application reads (a simplified form of) a FASTA file. For each sequence, it counts and prints the sequence ID, followed by the number of A, C, G, and T nucleotides. #include <err.h> #include <stdio.h> #include <stdlib.h> #include <string.h> #define MAX_ID_LEN 80 typedef struct { int a, c, g, t; } NucleotideCounts; void reset_counts(NucleotideCounts *counts); void update_counts(NucleotideCounts *counts, char *sequence); void print_counts(NucleotideCounts counts); int main(int argc, char *argv[]) { if (argc != 2) errx(1, \"no file name given\"); FILE *fp; if (!(fp = fopen(argv[1], \"r\"))) err(2, \"can't open file '%s'\", argv[1]); char *line_ptr = NULL; size_t nr_chars = 0; char current_id[MAX_ID_LEN] = \"\", next_id[MAX_ID_LEN] = \"\"; NucleotideCounts counts; while (getline(&line_ptr, &nr_chars, fp) != -1) { int nr_assigned = sscanf(line_ptr, \"> %s\", next_id); if (nr_assigned == 1) { if (strlen(current_id) > 0) { printf(\"%s: \", current_id); print_counts(counts); } strncpy(current_id, next_id, MAX_ID_LEN); reset_counts(&counts); } else if (nr_chars > 0) { update_counts(&counts, line_ptr); } } free(line_ptr); fclose(fp); return 0; } void reset_counts(NucleotideCounts *counts) { counts->a = counts->c = counts->g = counts->t = 0; } void update_counts(NucleotideCounts *counts, char *sequence) { for (char *nucl = sequence; *nucl != '\\0'; nucl++) switch (*nucl) { case 'A': counts->a++; break; case 'C': counts->c++; break; case 'G': counts->g++; break; case 'T': counts->t++; break; case '\\n': break; case '\\r': break; default: warnx(\"invalid nucleotide symbol '%c'\", *nucl); } } void print_counts(NucleotideCounts counts) { printf(\"A: %d, C = %d, G = %d, T = %d\\n\", counts.a, counts.c, counts.g, counts.t); } However, when you run the application on the following FASTA file, the output is not what you would hope for. > seq01 AACGTACCT CCAGGT > seq02 GGACAGGTT AGGCTAAGC TC > seq03 CGGAC You will see the result for the first two sequences, but not for the last. $ ./count_nucleotides_c.exe seq.fasta seq01: A: 4, C = 5, G = 3, T = 3 seq02: A: 5, C = 4, G = 7, T = 4 The output is correct but incomplete. By now, you will easily find the problem and fix it using a debugger. Unfortunately, there are no tools that can help you detect such problems, except for a set of comprehensive unit and functional tests. Missing code paths Consider the following Fortran function that computes the factorial of a given number. integer function factorial(n) implicit none integer, intent(in) :: n integer :: i factorial = 1 do i = 2, n factorial = factorial(i) end do end function factorial This function has two code paths, although they are implicit since there is no if statement. 1. If n < 2 , the do loop is not executed, only the result is set to 1. 1. If n >= 2 , the result is initialised to 1, and modified in each iteration of the do loop. The function will return a result for each value of its argument (ignoring integer overflow issues for now). However, if the function is called with a negative argument, the return value is 1, which is of course incorrect. A code path to cover this case is not implemented, and hence the factorial function does have a bug. A comprehensive battery of tests may be able to pick up on such issues. Logic problems Negation and double negation Although all programmers are very familiar with propositional logic, surprisingly many bugs are caused by mistakes, especially when negation is involved. Consequences of the following rules are often messed up: not(a and b) == not(a) or not(b) (De Morgan) not(a or b) == not(a) and not(b) (De Morgan) (a and b) or c == (a or c) and (b or c) (distribution) (a or b) and c == (a and c) or (b and c) (distribution) In these simple forms hardly anyone will make a mistake, but matters get more interesting when double negation is involved, e.g., not(a and not(b)) == not(a) or b . In general, humans have a somewhat harder time dealing with double negation. Even if your code contains double negation, and the implementation is in fact correct, it will be harder to understand when someone reads your code. Modifications may lead to bugs. Idiomatic C is quite prone to this, since you'll often see code like the example below. Experienced C programmers have no problem interpreting this code, but the casual ones may stumble. const int n = 10; double *data; if (!(data = (double *) malloc(n*sizeof(double)))) { /* Oops, NULL pointer returned! Handle this! */ } The assignment in C is an expression, not a statement, so it may appear anywhere the syntax allows any expression. The value of the expression is the value that was assigned, so in this case the address returned by the malloc function. This address can be NULL if malloc failed to allocate memory as requested, otherwise it will be a valid memory address. The semantics is that NULL is false, all other addresses are true. So if malloc returns NULL , the Boolean expression in the if statement evaluates to true, so that the problem can be handled. A less idiomatic formulation of this code might actually be beneficial, depending on the experience level of those involved in the project. const int n = 10; double *data= (double *) malloc(n*sizeof(double)); if (data == NULL) { /* Oops, NULL pointer returned! Handle this! */ } Not so exclusive cases Consider the following Fortran subroutine that classifies numbers into categories low, medium, and high. subroutine print_classification(x) implicit none real, intent(in) :: x real, parameter :: low = -5.0, high = 5.0 if (x < low) then print '(A)', 'low' else if (low < x .and. x < high) then print '(A)', 'medium' else print '(A)', 'high' end if end subroutine print_classification The intent is that when the argument is less than -5.0, low should be printed, between -5.0 and 5.0, medium should be printed, and larger than 5.0, high should be printed. This is almost always what happens, except when the argument is -5.0. x < low is false, low < x .and. x < high is also false, so print high . Note that the description of what the subroutine is supposed to do is ambiguous, which illustrates a problem caused by the formulation of the requirements. Again, only proper testing will help detect such issues. Semantics of logic evaluation The semantics of logic evaluation differs among programming languages. C, C++, and Python implement lazy evaluation. For Fortran the situation is more complicated since the standard doesn't specify whether logic evaluation is lazy or not. The following example was already discussed previously. Consider a C fragment that processes a string, but only if it is non-empty. char *text; ... if (strlen(text) && text != NULL) process(&text); Although we test for both conditions, this code will yield a segmentation fault if text is a null pointer. In C, Boolean expressions are evaluated from left to right. Also, evaluation is lazy, so for a logical and, if the left operand evaluates to false, there is no point in evaluating the right operand, since the expression as a whole will evaluate to false, regardless of the right operand's value. char *text; ... if (text != NULL && strlen(text)) process(&text); To summarise the behaviour for lazy evaluation of Boolean expressions: * <expr_1> && <expr_2> : <expr_1> is always evaluated, <expr_2> only when <expr_1> evaluates to true; * <expr_1> || <expr_2> : <expr_1> is always evaluated, <expr_2> only when <expr_1> false. For some programming languages, e.g., Bash, it is considered idiomatic to use lazy evaluation to control the flow of execution. Although you can do the same in C or C++, it is not considered idiomatic, and will make your code hard to understand. char *text; ... !text && strlen(text) && process(&text); Sometimes, a construct such as the one below is used, again relying on lazy evaluation. FILE *fp; (fp = fopen(file_name, \"w\")) || (fp = stdout); fprintf(fp, \"hello\"); fp == stdout || fclose(fp); Please save yourself some problems and don't indulge in this kind of programming. The last type of potential confusion, and hence a source of bugs, is the distinction between logical and bitwise operators. Although they may yield the same results in some circumstances, they will not in others. logical operator bitwise operator semantics || | or && & and ! ~ not For example, 2 & 4 == 0 which is false, while 2 && 4 == 1 which is true, ~1 == -2 which is true, while !1 == 0 which is false. Bitwise operators are not lazy, both operands will always be evaluated. Cppcheck will warn you about potential confusion between & and && , and | and || . For Fortran, the semantics depends on the compiler, since the specification is silent on the matter. The gfortran 8.2 compiler will generate code that does lazy logical evaluation, while for Intel's ifort 2018 compiler this is not the case. Relying on the behavior of a certain compiler is sure to yield non-portable, and hence buggy applications. Processing Bugs Arithmetic bugs are typically classified as processing bugs, but as mentioned in the introduction, they merit their own category. Another type of important processing bugs are resource leaks. A prime example would be memory leaks. Memory is dynamically allocated, but not deallocated. Over time, the memory footprint of the application increases, and the operating system may run out of memory. In the best case, the allocation error is handled gracefully, in the worst, you get a segmentation fault. Memory leaks can be identified using GCC's sanitizer, Cppcheck, Valgrind or Intel Inspector. Other types of resources may leak as well, and this may have unpleasant consequences. A handle to a file that is open for writing but is not closed, may lead to data loss and perhaps even corrupt data files. If file I/O is localised in time, and the granularity of compute and I/O phases is large, it may be prudent to only open the file at the start of the I/O phase and close it at the end. If the program crashes during a compute phase, your data is safe. However, keep in mind that opening and closing files has overhead, so it is not a good idea if a program is continuously doing I/O operations. An operating system command such as lsof can help you identify open files while your program is running. Network connections are another potential source of resource leaks. Opening, but not closing many connections may result in a de facto denial of access attack on a database or another resource. The same considerations as for file I/O apply to network connections. In many HPC systems, file I/O on a compute nodes is essentially network I/O, since this typically is done on shared file systems. The operating system command netstat may be useful to determine network connections. In the context of MPI, resources have to be managed as well. Communicators, derived data types, groups, windows for one-sided communication and so on are created, and must be paired with the corresponding call to free them once they are no longer required. For example, an MPI_Win_free should be paired with an MPI_Win_create . A tool such as Intel Trace Analyser and Collector can help identifying such issues. Initialisation bugs The initialisation of variables, or the lack thereof, is another nice source of problems. As mentioned previously, explicitly initialising variables is good practice, and compilers as well as static code analysers can help you detect the use of uninitialised variables. Attempting to use memory that has not been allocated also falls into this category. The following Fortran code will result in a segmentation fault. program unallocated use, intrinsic :: iso_fortran_env, only : error_unit implicit none integer :: n, i real, dimension(:), allocatable :: data real :: r character(len=80) :: buffer if (command_argument_count() < 1) then write (fmt='(A)', unit=error_unit) & 'missing command argument, positive integer expected' stop 1 end if call get_command_argument(1, buffer) read (buffer, fmt='(I10)') n do i = 1, n call random_number(r) data(i) = r end do print '(A, F15.5)', sum(data) end program unallocated The variable data has been declared allocatable , but the allocate statement is missing. This type of problem can sometimes be detected by the compiler, but certainly by Valgrind.","title":"Structural bugs"},{"location":"Taxonomy/structural_bugs/#structural-bugs","text":"These issues are likely the ones that first come to mind when you think of bugs. This is a very broad category that can be divided into a number of subcategories. Control flow Logic Processing Initialisation In the sections below, each type will be discussed, and examples will be given.","title":"Structural bugs"},{"location":"Taxonomy/structural_bugs/#control-flow","text":"","title":"Control flow"},{"location":"Taxonomy/structural_bugs/#loop-termination","text":"A prime example of a control flow bug is improper termination of loops. Programmers who switch between 1-based languages (e.g., Fortran, MATLAB, R, ...) and 0-based languages (C, C++, Python) or those just starting to use the latter are prone to this. Consider the following C code fragment: const int n = 5; double data[n]; for (int i = 0; i <= n; i++) data[i] = some_function(i); Although the loop index i starts at 0, which is correct, during the last iteration i will be equal to n , so data[n] will try to access the array out of bounds, since it's last element is at index n - 1 , rather than n . This situation is somewhat less likely to happen in Fortran code. Out of bound array access can be detected using compiler flags and Valgrind. The following example involves a while iteration, and is also quite common. The application reads (a simplified form of) a FASTA file. For each sequence, it counts and prints the sequence ID, followed by the number of A, C, G, and T nucleotides. #include <err.h> #include <stdio.h> #include <stdlib.h> #include <string.h> #define MAX_ID_LEN 80 typedef struct { int a, c, g, t; } NucleotideCounts; void reset_counts(NucleotideCounts *counts); void update_counts(NucleotideCounts *counts, char *sequence); void print_counts(NucleotideCounts counts); int main(int argc, char *argv[]) { if (argc != 2) errx(1, \"no file name given\"); FILE *fp; if (!(fp = fopen(argv[1], \"r\"))) err(2, \"can't open file '%s'\", argv[1]); char *line_ptr = NULL; size_t nr_chars = 0; char current_id[MAX_ID_LEN] = \"\", next_id[MAX_ID_LEN] = \"\"; NucleotideCounts counts; while (getline(&line_ptr, &nr_chars, fp) != -1) { int nr_assigned = sscanf(line_ptr, \"> %s\", next_id); if (nr_assigned == 1) { if (strlen(current_id) > 0) { printf(\"%s: \", current_id); print_counts(counts); } strncpy(current_id, next_id, MAX_ID_LEN); reset_counts(&counts); } else if (nr_chars > 0) { update_counts(&counts, line_ptr); } } free(line_ptr); fclose(fp); return 0; } void reset_counts(NucleotideCounts *counts) { counts->a = counts->c = counts->g = counts->t = 0; } void update_counts(NucleotideCounts *counts, char *sequence) { for (char *nucl = sequence; *nucl != '\\0'; nucl++) switch (*nucl) { case 'A': counts->a++; break; case 'C': counts->c++; break; case 'G': counts->g++; break; case 'T': counts->t++; break; case '\\n': break; case '\\r': break; default: warnx(\"invalid nucleotide symbol '%c'\", *nucl); } } void print_counts(NucleotideCounts counts) { printf(\"A: %d, C = %d, G = %d, T = %d\\n\", counts.a, counts.c, counts.g, counts.t); } However, when you run the application on the following FASTA file, the output is not what you would hope for. > seq01 AACGTACCT CCAGGT > seq02 GGACAGGTT AGGCTAAGC TC > seq03 CGGAC You will see the result for the first two sequences, but not for the last. $ ./count_nucleotides_c.exe seq.fasta seq01: A: 4, C = 5, G = 3, T = 3 seq02: A: 5, C = 4, G = 7, T = 4 The output is correct but incomplete. By now, you will easily find the problem and fix it using a debugger. Unfortunately, there are no tools that can help you detect such problems, except for a set of comprehensive unit and functional tests.","title":"Loop termination"},{"location":"Taxonomy/structural_bugs/#missing-code-paths","text":"Consider the following Fortran function that computes the factorial of a given number. integer function factorial(n) implicit none integer, intent(in) :: n integer :: i factorial = 1 do i = 2, n factorial = factorial(i) end do end function factorial This function has two code paths, although they are implicit since there is no if statement. 1. If n < 2 , the do loop is not executed, only the result is set to 1. 1. If n >= 2 , the result is initialised to 1, and modified in each iteration of the do loop. The function will return a result for each value of its argument (ignoring integer overflow issues for now). However, if the function is called with a negative argument, the return value is 1, which is of course incorrect. A code path to cover this case is not implemented, and hence the factorial function does have a bug. A comprehensive battery of tests may be able to pick up on such issues.","title":"Missing code paths"},{"location":"Taxonomy/structural_bugs/#logic-problems","text":"","title":"Logic problems"},{"location":"Taxonomy/structural_bugs/#negation-and-double-negation","text":"Although all programmers are very familiar with propositional logic, surprisingly many bugs are caused by mistakes, especially when negation is involved. Consequences of the following rules are often messed up: not(a and b) == not(a) or not(b) (De Morgan) not(a or b) == not(a) and not(b) (De Morgan) (a and b) or c == (a or c) and (b or c) (distribution) (a or b) and c == (a and c) or (b and c) (distribution) In these simple forms hardly anyone will make a mistake, but matters get more interesting when double negation is involved, e.g., not(a and not(b)) == not(a) or b . In general, humans have a somewhat harder time dealing with double negation. Even if your code contains double negation, and the implementation is in fact correct, it will be harder to understand when someone reads your code. Modifications may lead to bugs. Idiomatic C is quite prone to this, since you'll often see code like the example below. Experienced C programmers have no problem interpreting this code, but the casual ones may stumble. const int n = 10; double *data; if (!(data = (double *) malloc(n*sizeof(double)))) { /* Oops, NULL pointer returned! Handle this! */ } The assignment in C is an expression, not a statement, so it may appear anywhere the syntax allows any expression. The value of the expression is the value that was assigned, so in this case the address returned by the malloc function. This address can be NULL if malloc failed to allocate memory as requested, otherwise it will be a valid memory address. The semantics is that NULL is false, all other addresses are true. So if malloc returns NULL , the Boolean expression in the if statement evaluates to true, so that the problem can be handled. A less idiomatic formulation of this code might actually be beneficial, depending on the experience level of those involved in the project. const int n = 10; double *data= (double *) malloc(n*sizeof(double)); if (data == NULL) { /* Oops, NULL pointer returned! Handle this! */ }","title":"Negation and double negation"},{"location":"Taxonomy/structural_bugs/#not-so-exclusive-cases","text":"Consider the following Fortran subroutine that classifies numbers into categories low, medium, and high. subroutine print_classification(x) implicit none real, intent(in) :: x real, parameter :: low = -5.0, high = 5.0 if (x < low) then print '(A)', 'low' else if (low < x .and. x < high) then print '(A)', 'medium' else print '(A)', 'high' end if end subroutine print_classification The intent is that when the argument is less than -5.0, low should be printed, between -5.0 and 5.0, medium should be printed, and larger than 5.0, high should be printed. This is almost always what happens, except when the argument is -5.0. x < low is false, low < x .and. x < high is also false, so print high . Note that the description of what the subroutine is supposed to do is ambiguous, which illustrates a problem caused by the formulation of the requirements. Again, only proper testing will help detect such issues.","title":"Not so exclusive cases"},{"location":"Taxonomy/structural_bugs/#semantics-of-logic-evaluation","text":"The semantics of logic evaluation differs among programming languages. C, C++, and Python implement lazy evaluation. For Fortran the situation is more complicated since the standard doesn't specify whether logic evaluation is lazy or not. The following example was already discussed previously. Consider a C fragment that processes a string, but only if it is non-empty. char *text; ... if (strlen(text) && text != NULL) process(&text); Although we test for both conditions, this code will yield a segmentation fault if text is a null pointer. In C, Boolean expressions are evaluated from left to right. Also, evaluation is lazy, so for a logical and, if the left operand evaluates to false, there is no point in evaluating the right operand, since the expression as a whole will evaluate to false, regardless of the right operand's value. char *text; ... if (text != NULL && strlen(text)) process(&text); To summarise the behaviour for lazy evaluation of Boolean expressions: * <expr_1> && <expr_2> : <expr_1> is always evaluated, <expr_2> only when <expr_1> evaluates to true; * <expr_1> || <expr_2> : <expr_1> is always evaluated, <expr_2> only when <expr_1> false. For some programming languages, e.g., Bash, it is considered idiomatic to use lazy evaluation to control the flow of execution. Although you can do the same in C or C++, it is not considered idiomatic, and will make your code hard to understand. char *text; ... !text && strlen(text) && process(&text); Sometimes, a construct such as the one below is used, again relying on lazy evaluation. FILE *fp; (fp = fopen(file_name, \"w\")) || (fp = stdout); fprintf(fp, \"hello\"); fp == stdout || fclose(fp); Please save yourself some problems and don't indulge in this kind of programming. The last type of potential confusion, and hence a source of bugs, is the distinction between logical and bitwise operators. Although they may yield the same results in some circumstances, they will not in others. logical operator bitwise operator semantics || | or && & and ! ~ not For example, 2 & 4 == 0 which is false, while 2 && 4 == 1 which is true, ~1 == -2 which is true, while !1 == 0 which is false. Bitwise operators are not lazy, both operands will always be evaluated. Cppcheck will warn you about potential confusion between & and && , and | and || . For Fortran, the semantics depends on the compiler, since the specification is silent on the matter. The gfortran 8.2 compiler will generate code that does lazy logical evaluation, while for Intel's ifort 2018 compiler this is not the case. Relying on the behavior of a certain compiler is sure to yield non-portable, and hence buggy applications.","title":"Semantics of logic evaluation"},{"location":"Taxonomy/structural_bugs/#processing-bugs","text":"Arithmetic bugs are typically classified as processing bugs, but as mentioned in the introduction, they merit their own category. Another type of important processing bugs are resource leaks. A prime example would be memory leaks. Memory is dynamically allocated, but not deallocated. Over time, the memory footprint of the application increases, and the operating system may run out of memory. In the best case, the allocation error is handled gracefully, in the worst, you get a segmentation fault. Memory leaks can be identified using GCC's sanitizer, Cppcheck, Valgrind or Intel Inspector. Other types of resources may leak as well, and this may have unpleasant consequences. A handle to a file that is open for writing but is not closed, may lead to data loss and perhaps even corrupt data files. If file I/O is localised in time, and the granularity of compute and I/O phases is large, it may be prudent to only open the file at the start of the I/O phase and close it at the end. If the program crashes during a compute phase, your data is safe. However, keep in mind that opening and closing files has overhead, so it is not a good idea if a program is continuously doing I/O operations. An operating system command such as lsof can help you identify open files while your program is running. Network connections are another potential source of resource leaks. Opening, but not closing many connections may result in a de facto denial of access attack on a database or another resource. The same considerations as for file I/O apply to network connections. In many HPC systems, file I/O on a compute nodes is essentially network I/O, since this typically is done on shared file systems. The operating system command netstat may be useful to determine network connections. In the context of MPI, resources have to be managed as well. Communicators, derived data types, groups, windows for one-sided communication and so on are created, and must be paired with the corresponding call to free them once they are no longer required. For example, an MPI_Win_free should be paired with an MPI_Win_create . A tool such as Intel Trace Analyser and Collector can help identifying such issues.","title":"Processing Bugs"},{"location":"Taxonomy/structural_bugs/#initialisation-bugs","text":"The initialisation of variables, or the lack thereof, is another nice source of problems. As mentioned previously, explicitly initialising variables is good practice, and compilers as well as static code analysers can help you detect the use of uninitialised variables. Attempting to use memory that has not been allocated also falls into this category. The following Fortran code will result in a segmentation fault. program unallocated use, intrinsic :: iso_fortran_env, only : error_unit implicit none integer :: n, i real, dimension(:), allocatable :: data real :: r character(len=80) :: buffer if (command_argument_count() < 1) then write (fmt='(A)', unit=error_unit) & 'missing command argument, positive integer expected' stop 1 end if call get_command_argument(1, buffer) read (buffer, fmt='(I10)') n do i = 1, n call random_number(r) data(i) = r end do print '(A, F15.5)', sum(data) end program unallocated The variable data has been declared allocatable , but the allocate statement is missing. This type of problem can sometimes be detected by the compiler, but certainly by Valgrind.","title":"Initialisation bugs"},{"location":"Taxonomy/taxonomy_intro/","text":"Taxonomy of bugs: introduction Motivation Consider the work of an entomologist. To identify an insect, a modern day researcher could simply sample some DNA, have it sequenced, do an assembly and a BLAST. It would work, be very accurate, and wildly expensive, even at current prices. It would also take a couple of days, not to mention that most entomologists in the field don't carry a sequencer and a powerful workstation in their backpack. That is the reason biologists typically still learn how to identify species of interest by their morphological characteristics. It just requires a careful look and a good memory (and perhaps a book), a few minutes of quality time with a magnifying glass will be all that is required. Of course, this will not work in all situations. From time to time, identification will be hard, and that's where the heavy machinery comes in. You can also think of the work of a physician. She observes symptoms, and tries to deduce probable causes. These are just hypotheses, so some additional examinations and experiments will be required to narrow down the options, and try to confirm the actual disease. You can imagine that without that initial step, i.e., identifying a number of potential diseases based on the symptoms experienced by the patient, you wouldn't be very successful as a doctor. These two examples illustrate that having a taxonomy in your mind considerably improves your efficiency for many activities. For a biologist, that would be a taxonomy of the organisms she is interested in. For the physician, the diseases that are relevant in her field of specialisation. Although a bug is a bug, there are in fact many types. When you have a taxonomy in mind, it may help you in several ways. If you can map symptoms to potential causes, this will speed up the debugging process considerably. Some types of bugs will occur in specific stages of development, and realising that helps you to pay extra attention to those types when required. You may not be aware of certain issues and the consequences they can have. Identifying them will be much harder in that case. Hopefully, the analogies to the two examples are clear. High-level taxonomy Any taxonomy tends to be a bit fuzzy, and is of course, like everything in science, subject to debate and improvement. You should view the taxonomy of bugs presented here as a guideline, nothing more. Requirements Structural bugs Arithmetic bugs Bugs in data In many classification schemes for bugs, arithmetic bugs are considered structural bugs. However, since this arithmetic bugs are of prime importance in scientific programming, they merit their own category in this course. Each category will be defined and discussed in turn.","title":"Introduction"},{"location":"Taxonomy/taxonomy_intro/#taxonomy-of-bugs-introduction","text":"","title":"Taxonomy of bugs: introduction"},{"location":"Taxonomy/taxonomy_intro/#motivation","text":"Consider the work of an entomologist. To identify an insect, a modern day researcher could simply sample some DNA, have it sequenced, do an assembly and a BLAST. It would work, be very accurate, and wildly expensive, even at current prices. It would also take a couple of days, not to mention that most entomologists in the field don't carry a sequencer and a powerful workstation in their backpack. That is the reason biologists typically still learn how to identify species of interest by their morphological characteristics. It just requires a careful look and a good memory (and perhaps a book), a few minutes of quality time with a magnifying glass will be all that is required. Of course, this will not work in all situations. From time to time, identification will be hard, and that's where the heavy machinery comes in. You can also think of the work of a physician. She observes symptoms, and tries to deduce probable causes. These are just hypotheses, so some additional examinations and experiments will be required to narrow down the options, and try to confirm the actual disease. You can imagine that without that initial step, i.e., identifying a number of potential diseases based on the symptoms experienced by the patient, you wouldn't be very successful as a doctor. These two examples illustrate that having a taxonomy in your mind considerably improves your efficiency for many activities. For a biologist, that would be a taxonomy of the organisms she is interested in. For the physician, the diseases that are relevant in her field of specialisation. Although a bug is a bug, there are in fact many types. When you have a taxonomy in mind, it may help you in several ways. If you can map symptoms to potential causes, this will speed up the debugging process considerably. Some types of bugs will occur in specific stages of development, and realising that helps you to pay extra attention to those types when required. You may not be aware of certain issues and the consequences they can have. Identifying them will be much harder in that case. Hopefully, the analogies to the two examples are clear.","title":"Motivation"},{"location":"Taxonomy/taxonomy_intro/#high-level-taxonomy","text":"Any taxonomy tends to be a bit fuzzy, and is of course, like everything in science, subject to debate and improvement. You should view the taxonomy of bugs presented here as a guideline, nothing more. Requirements Structural bugs Arithmetic bugs Bugs in data In many classification schemes for bugs, arithmetic bugs are considered structural bugs. However, since this arithmetic bugs are of prime importance in scientific programming, they merit their own category in this course. Each category will be defined and discussed in turn.","title":"High-level taxonomy"},{"location":"Testing/testing_as_experiments/","text":"Testing as scientific experiments If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as scientific experiments"},{"location":"Testing/testing_as_experiments/#testing-as-scientific-experiments","text":"If you are a scientist, or at least interested in the subject, you know about the scientific method. Simplified, the process works as follows: 1. You make a number of observations. 1. You formulate a hypothesis that has predictive power. 1. You design experiments to test predictions made by the hypothesis. 1. If an experiment succeeds, your confidence in the hypothesis increases. 1. However, if an experiment fails, you know for certain that there is a problem and you reformulate your hypothesis, possibly gathering some more observations to do so. It is worth pointing out that experiments are actually designed to disprove the hypothesis rather than to confirm it. A quote of the philosopher of science Karl Popper illustrates this. If we are uncritical we shall always find what we want: we shall look for, and find, confirmations, and we shall look away from, and not see, whatever might be dangerous to our pet theories. If you substitute \"pet theories\" in the above by \"pet implementations\", it is quite clear that the same maxims apply to software testing as to scientific research. Another quote from his famous book \"The logic of scientific discovery\" (1934) also translates well to software testing. ...no matter how many instances of white swans we may have observed, this does not justify the conclusion that all swans are white. Paraphrased, you can read that as \"... no matter how many tests your software may pass, it doesn't justify the conclusion that it is correct\".","title":"Testing as scientific experiments"},{"location":"Testing/CodeCoverage/code_coverage_best_practices/","text":"Code coverage best practices Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing. How to enable code coverage? Code coverage is provided by the compiler, and the information gathered during runs of the application can be accessed using a tool that comes with your compiler. GCC To enable code coverage testing for GCC, use the three compiler options -g -fprofile-arcs -ftest-coverage When you run the application, statistics are gathered in files with extensions .gcda and .gcno . This is cumulative, so these files are updated each time you run the application. Those .gcda and .gcno files are binary, and not intended for human consumption. The gcov tool will use the information stored in those files, and created an annotated source file with extension .gcov . For instance, for a source file palindrome.f90 , that is $ gcov palindrome.f90 This will produce palindrome.f90.gcov which you can view using any text editor or even less . Intel To enable code coverage testing for Intel compilers, use the three compiler options -g -prof-gen=srcpos -prof-dir=./profile You can of course choose any directory you like to store the profile information. Note that the directory should be created before compilation. When you run the application, statistics are gathered in files in the profile directory, ./profile in the option above. This is cumulative, so these files are updated each time you run the application. However, these Next, you have to merge the build and runtime information using $ profmerge -prof_dir ./profile Note that the name of the option is not exactly the same as the compiler options. The information contained in the profile directory is not human readable. However, you can generate an HTML overview using ~~~bash $ codecov -dpi ./profile/pgopti.dpi -spi ./profile/pgopti.spi ~~~~ The current working directory now contains a file CODE_COVERAGE.HTML that you can open with a web browser. The output is color-coded, a yellow background means that those lines in a function have not been executed, red is for a function that was called at all.","title":"Best practices"},{"location":"Testing/CodeCoverage/code_coverage_best_practices/#code-coverage-best-practices","text":"Maintaining code over long periods of time is quite expensive. The larger the code base, the more effort has to be spent on keeping code up to date. If some parts of the code are never used, that adds to this burden without return on investment. Updates are an issue as well, since some unused parts of the code may get out of sync with respect to the parts that are executed regularly. If someone starts using the abandoned part of the code, interesting bugs may creep into her code. Hence code that is not used is best removed from the code base. If you use a version control system and informative commit messages, it is quite easy to recover that code later when it is unexpectedly required. An important concern when writing tests for your software project is whether or not all branches in function, and indeed all functions are tested. Figuring out by hand whether that is the case is pretty hard for sizable projects. Fortunately, software tools are available for checking which parts of the code base are executed and which are not. For many programming languages, one has to resort to third party tools, but the compilers for C, C++ and Fortran support this out of the box. The first step in the workflow is to instrument the code with instructions to do the bookkeeping for reporting which lines of code have been executed. Compilers have options to do that automatically, so this can easily be incorporated into the build process by adding a make target specific for a code coverage build. The second step is to execute the software, so that a report is generated. In case you wonder how to run your code to get the most useful report, this depends on your goal. If you want to detect code that is likely not executed in applications, running a number of typical use cases are the best way to go. On the other hand, if you want to verify that you have a comprehensive set of unit tests, execute those. The third step is to inspect that report. Typically, you will get summary information, e.g., the percentage of the code covered in each file. In addition, each individual file can be inspected on a line by line basis. Lines that have not been executed are clearly marked, so that they are easy to spot. Finally, you decide to either weed out the lines if they are dead code, i.e., code that will never be executed in the context of your project, or to create additional unit tests if that code will be executed but is not tested yet. Since code coverage tests produce artifacts, it is best to add rules to remove these artifacts to your make file. This ensures you start with a clean slate when rebuilding. Code coverage assessment is an important tool for delivering good quality code, and goes hand in hand with unit testing and functional testing.","title":"Code coverage best practices"},{"location":"Testing/CodeCoverage/code_coverage_best_practices/#how-to-enable-code-coverage","text":"Code coverage is provided by the compiler, and the information gathered during runs of the application can be accessed using a tool that comes with your compiler.","title":"How to enable code coverage?"},{"location":"Testing/CodeCoverage/code_coverage_best_practices/#gcc","text":"To enable code coverage testing for GCC, use the three compiler options -g -fprofile-arcs -ftest-coverage When you run the application, statistics are gathered in files with extensions .gcda and .gcno . This is cumulative, so these files are updated each time you run the application. Those .gcda and .gcno files are binary, and not intended for human consumption. The gcov tool will use the information stored in those files, and created an annotated source file with extension .gcov . For instance, for a source file palindrome.f90 , that is $ gcov palindrome.f90 This will produce palindrome.f90.gcov which you can view using any text editor or even less .","title":"GCC"},{"location":"Testing/CodeCoverage/code_coverage_best_practices/#intel","text":"To enable code coverage testing for Intel compilers, use the three compiler options -g -prof-gen=srcpos -prof-dir=./profile You can of course choose any directory you like to store the profile information. Note that the directory should be created before compilation. When you run the application, statistics are gathered in files in the profile directory, ./profile in the option above. This is cumulative, so these files are updated each time you run the application. However, these Next, you have to merge the build and runtime information using $ profmerge -prof_dir ./profile Note that the name of the option is not exactly the same as the compiler options. The information contained in the profile directory is not human readable. However, you can generate an HTML overview using ~~~bash $ codecov -dpi ./profile/pgopti.dpi -spi ./profile/pgopti.spi ~~~~ The current working directory now contains a file CODE_COVERAGE.HTML that you can open with a web browser. The output is color-coded, a yellow background means that those lines in a function have not been executed, red is for a function that was called at all.","title":"Intel"},{"location":"Testing/CodeCoverage/code_coverage_intro/","text":"Introduction to code coverage Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite. We will show how to use gcov and codecov , code coverage tools the GCC and Intel compiler suite respectively, and provide a number of examples to illustrate how to use the output to increase the quality of our unit test suite.","title":"Introduction"},{"location":"Testing/CodeCoverage/code_coverage_intro/#introduction-to-code-coverage","text":"Unit tests are very useful to formulate fine-grained test to check the functionality of functions and methods. Unit tests check for edge and corner cases, but also for handling of error conditions such as exceptions being thrown. This is of course very useful in itself, but using a run of the complete unit test suite can also provide a good test to see whether all functions and methods are called, and all codes paths in the code get executed. Code coverage tools will instrument your code, run it, and provide feedback on regions of code that are not executed doing that run. Since we claim that code that is not tested is not correct, coverage provided by running all the unit tests should in fact be (close to) 100 %. If code coverage is insufficient, more unit tests should be added to the test suite. We will show how to use gcov and codecov , code coverage tools the GCC and Intel compiler suite respectively, and provide a number of examples to illustrate how to use the output to increase the quality of our unit test suite.","title":"Introduction to code coverage"},{"location":"Testing/FunctionalTesting/functional_testing_best_practices/","text":"Functional testing best practices Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focussed library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Travis CI is a very nice online continuous integration service that is free to use for open source software projects. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage. We will illustrate the use of shunit2 for functional testing in this section. Full continuous integration is a bit out of scope of this course.","title":"Best practices"},{"location":"Testing/FunctionalTesting/functional_testing_best_practices/#functional-testing-best-practices","text":"Unit testing is an invaluable help for the developer since it catches bugs introduced when the code base changes. Tests can be executed easily and are run frequently. However, unit tests typically concentrate on the low level functionality of the software project. They test whether individual functions behave as expected. This is white box testing, since the tests are developed with access to the \"innards\" of the software under test. In some circumstances, this may be all that is required, e.g., when developing a relatively small or very focussed library. In many cases though, unit testing is best supplemented by functional testing. The point of view of functional testing is opposite to that of unit testing since functional tests will focus on the application as a whole. Are the results for a sophisticated use case reproduced as expected? Does the application's user interface, command line interface (CLI), or graphical user interface (GUI) behave as expected? Are options handled as expected? This is often called black box testing since only the user interface is accessed. Functional testing can also be applied to third party applications that are part of a workflow. For instance, suppose that your application relies on the output of another application not developed by you. If the output format of that application changes from one version to the next, running a functional test will make clear whether there is an impact on your workflow, and you may fix problems by adapting your application. The best way to do functional testing is by using a continuous integration workflow. When the functional tests are run, first a container is prepared with the required operating system and software stack. Next, your software is built within the container, so that the environment is completely controlled. If the build succeeds, tests are run. A report is generated to show failures if they occur. Note that it is possible to set up a matrix of operating system versions and compiler versions to ensure that your code will build and executed cleanly on a wide range of software platforms. Travis CI is a very nice online continuous integration service that is free to use for open source software projects. The question remains how to code the actual tests that will be executed by the continuous integration system. A convenient way is to reuse the unit test paradigm, but now on the level of the shell. In other words, the unit tests will be relatively short shell scripts that invoke your application using various parameters and input data, and verify the results. The shunit2 framework provides a nice framework for this purpose. It provides similar functionality as the unit testing frameworks for specific programming languages. However, from the point of view of the software project this is black box, rather than white box testing. The same concerns as for unit testing apply. For instance, it is important that the tests cover the use cases as well as possible. Here too, code coverage can be a great help to detect which application aspects are tested, and for which additional tests need to be implemented to improve the coverage. We will illustrate the use of shunit2 for functional testing in this section. Full continuous integration is a bit out of scope of this course.","title":"Functional testing best practices"},{"location":"Testing/FunctionalTesting/functional_testing_intro/","text":"Intorudction to functional testing Unit testing is a great help during the development process. It iwll help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Although ideally, functional testing would be done automatically for a release with an online tool such as Travis CI for continuous integration, this would lead us too far. In this section we will discuss how to use shunit2 to test command line applications. We will illustrate how functional testing can detect code defects that would go unnoticed by unit testing, showing that both testing strategies are complementing one another.","title":"Introduction"},{"location":"Testing/FunctionalTesting/functional_testing_intro/#intorudction-to-functional-testing","text":"Unit testing is a great help during the development process. It iwll help us spot problems introduced by code changes immediately after they have been introduced. They help test the functionality at the level of individual functions and methods. However, to test an entire application that is, e.g., run from the command line with various parameters, unit tests are not really the right tool. The literature on software development refers to this type of testing as functional testing. Tests are more coarse grained, and are likely to take longer to run than you are comfortable with during an intensive edit/test/commit development cycle. Hence it is acceptable to run functional tests less often, for instance only when a feature has been added to the software, or a bug has been fixed that may have involved a considerable number of file edits and commits. We rely on unit testing to ensure that this process didn't break low-level integrity of the code. Although ideally, functional testing would be done automatically for a release with an online tool such as Travis CI for continuous integration, this would lead us too far. In this section we will discuss how to use shunit2 to test command line applications. We will illustrate how functional testing can detect code defects that would go unnoticed by unit testing, showing that both testing strategies are complementing one another.","title":"Intorudction to functional testing"},{"location":"Testing/UnitTesting/catch2_cpp/","text":"Catch2: unit testing for C++ Although you could use CUnit for testing C++ code, there are better alternatives. A very nice framework is Catch2 . You can express tests quite naturally using Catch2 so that they resemble a narrative. The framework takes a further step along that path by offering support for Behaviour Driven Development (BDD). Catch2 is a header-only library, so it is trivial to install, and has support for CMake if you're so inclined. The basics The function under test computes the factorial of a given integer, i.e., #include <stdexcept> int fac(int n) { if (n < 0) throw std::domain_error {\"argument must be positive\"}; int result {1}; for (int i = 2; i < n; ++i) result *= i; return result; } Defining the tests Defining tests for Catch2 is quite straightforward using the TEST_CASE macro. It takes one or two arguments, the name of the test that has to be unique, and, optionally, a tag that is used to group tests. In the example below, \"factorials\" is the name of this test, while \"[fac]\" is the tag. #include <catch/catch.hpp> TEST_CASE(\"factorials\", \"[fac]\") { REQUIRE( fac(0) == 1 ); REQUIRE( fac(3) == 6 ); } The code block implements the test, using the REQUIRE macro relying on C++ Boolean expressions to implement the tests. Note that the Boolean expressions should be limited to comparison or a function call. Expressions that include logical operators && , || and ! are too hard to provide meaningful feedback when a test fails. Setting up the tests Setting up the tests is trivial in Catch2. You simply have to define a preprocessor variable in a C++ file, i.e., #define CATCH_CONFIG_MAIN #include <catch/catch.hpp> The main function will be generated automatically. However, that will take the preprocessor/compiler a while, so it is recommended to put the lines above inn their own C++ source file. That way, the code is generated and compiled only once, and not each time you add or make a change to a test. Believe me, you will be grateful for this tip. Building and running The build the tests, the compiler needs to be aware of the location of the Catch2 header files, so you have to specify the appropriate -I flag. The most convenient way is to use the single include file which is in the single_include directory of the distribution. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ test_fac.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- factorials ------------------------------------------------------------------------------- test_fac.cpp:6 ............................................................................... test_fac.cpp:8: FAILED: REQUIRE( fac(3) == 6 ) with expansion: 2 == 6 =============================================================================== test cases: 1 | 1 failed assertions: 2 | 1 passed | 1 failed The report is quite comprehensive, showing you the computed versus the expected value for the test(s) that failed, as well as summary information. In this case, the single test case failed, while of the two assertions, one failed, and one passed. The fac function needs some work. More assertions Although REQUIRE is Catch2's main work horse, a few other macros and features are very useful as well. As was mentioned when discussing best practices, it is important to test for failure, and with Catch2, the REQUIRE_THROWS_AS can be used for this purpose. In the example above, the fac function will throw a domain_error exception when its argument is strictly negative, since the factorial is only defined for positive integers. Testing for this is straightforward. #include <stdexcept> ... REQUIRE_THROWS_AS( fac(-1), std::domain_error ); ... To test on the exception's message, rather than its type, REQUIRE_THROWS can be used, e.g., ... REQUIRE_THROWS_WITH( fac(-1), \"argument must be positive\" ); ... This form is useful to test non-trivial exception messages. Finally, it is also possible to verify that an exception is thrown ( REQUIRE_THROWS ) or not ( REQUIRE_NOTHROW ). Catch2 has no dedicated macro for testing floating point equality, but the Approx class is provided for this purpose. It has two methods to set either the relative ( epsilon ) or absolute ( margin ) accuracy of the comparison. For instance, suppose that the function compute_pi , well, computes the value of pi, then it could be tested whether the result deviates from 3.14 by less than one percent as follows: REQUIRE( compute_pi() == Approx.epsilong(0.01) ); Alternatively, the following test would check whether the computed value is in the interval [3.13, 3.15]: REQUIRE( compute_pi() == Approx.magrin(0.01) ); Another quite useful macro is REQUIRE_THAT . It takes two arguments, the computed value and a matcher. To check whether a string matches a regular expression, the Catch::Matchers::Matches matcher can be used, e.g., suppose the function gen_ip4_address returns strings such as \"127.0.0. \" to represent IP4 addresses, than the following REQUIRE_THAT` macro would verify that. using Catch::Matchers::Matches; ... REQUIRE_THAT( gen_ip4_address(), Matches(R\"(^\\d{1,3}(?:\\.\\d{1,3}){3}$)\") ); ... Besides the Matches matcher, Catch2 also defines StartsWith , EndsWith , Equals and Contains for std::string . For std::vector , three matchers are defined, Contains , ContainsVector (subset) and Equals . Moreover, a generic Predicate matcher can be used to turn a lambda function (or any callable for that matter into a matcher. Note that matchers can be combined into Boolean expressions involving the operators && , || and ! . Finally, a second version for each REQUIRE macro is defined, e.g., CHECK , CHECK_THAT , etc. Unlike the REQUIRE family, execution of the test case doesn't stop when a CHECK fails. Fixtures Fixtures for Catch2 tests are implemented as classes. The constructor will do the set up and, if required, the destructor is responsible for the tear down. TEST_CASE_METHOD is used to define test cases. This macro takes two or three arguments. The first is the class that implements the fixture, the second is the unique name of the test case, and, optionally, the third is the tag. As a somewhat contrived example, consider a stack that is initialized, and integer values are pushed onto it, starting from 0 up to max_value . #include <stack> class VectorFixture { protected: std::stack<int> data; const int max_value {5}; public: VectorFixture() : data() { for (int i = 1; i <= max_value; ++i) data.push(i); }; }; Now tests that use this fixture can be defined as TEST_CASE_METHOD , e.g., #include <catch2/catch.hpp> TEST_CASE_METHOD(VectorFixture, \"sum\", \"[stack]\") { int sum {0}; while (!data.empty()) { sum += data.top(); data.pop(); } REQUIRE( sum == max_value*(max_value + 1)/2 ); } TEST_CASE_METHOD(VectorFixture, \"product\", \"[stack]\") { int prod {1}; while (!data.empty()) { prod *= data.top(); data.pop(); } REQUIRE( prod == fac(max_value) ); } As you can see, for each test case, the stack in the fixtures is emptied, illustrating that the fixture is set up (and teared down) for each individual test case. Alternatively, you can also define the test cases as ordinary object methods in for the fixture class, and register them as such using the METHOD_AS_TEST_CASE macro. Behavior-driven design (BDD) Catch2 also supports a behavior-driven design approach to testing, and in fact, according to the library's author, this is the preferred way to handle fixtures. The tests for the factorial function can be implemented as a scenario, i.e., #include <catch2/catch.hpp> #include <stdexcept> SCENARIO( \"factorial function return values and exceptions\", \"[fac]\" ) { GIVEN( \"factorial function 'fac'\" ) { WHEN( \"argument == 0\" ) { THEN( \"fac(0) == 1\" ) { REQUIRE( fac(0) == 1 ); } } WHEN( \"argument > 0\" ) { THEN( \"fac(n) == n*fac(n-1)\" ) { for (int i = 1; i < 6; i++) REQUIRE( fac(i) == i*fac(i - 1) ); } } WHEN( \"argument < 0\" ) { THEN( \"exception thrown\" ) { REQUIRE_THROWS_AS( fac(-1), std::domain_error ); } } } } When the resulting test application is run with the -s option, it will show the following output. test_fac.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument == 0 Then: fac(0) == 1 ------------------------------------------------------------------------------- test_fac.cpp:9 ............................................................................... test_fac.cpp:10: PASSED: REQUIRE( fac(0) == 1 ) with expansion: 1 == 1 ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument > 0 Then: fac(n) == n*fac(n-1) ------------------------------------------------------------------------------- test_fac.cpp:14 ............................................................................... test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 1 == 1 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 2 == 2 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 6 == 6 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 24 == 24 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 120 == 120 ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument < 0 Then: exception thrown ------------------------------------------------------------------------------- test_fac.cpp:20 ............................................................................... test_fac.cpp:21: PASSED: REQUIRE_THROWS_AS( fac(-1), std::domain_error ) =============================================================================== All tests passed (7 assertions in 1 test case) To illustrate how BDD simplifies creating and working with fixtures, consider the following implementation of the same tests as in the section on fixtures. #define CATCH_CONFIG_MAIN #include <catch2/catch.hpp> #include <stack> int fac(int n) { int result = 1; for (int i = 2; i <= n; ++i) result *= i; return result; } SCENARIO( \"stack test\", \"[stack]\" ) { GIVEN( \"stack with numbers 1 to 5\" ) { const int max_val {5}; std::stack<int> data; for (int i = 1; i <= max_val; ++i) data.push(i); WHEN( \"computing sum\" ) { int sum {0}; while (!data.empty()) { sum += data.top(); data.pop(); } THEN( \"sum == 5*6/2\" ) { REQUIRE( sum == max_val*(max_val + 1)/2 ); } } WHEN( \"computing product\" ) { int prod {1}; while (!data.empty()) { prod *= data.top(); data.pop(); } THEN( \"product == 5!\" ) { REQUIRE( prod == fac(max_val) ); } } } } When the test application is run with the -s flag, the following output is produced. stack_test.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- Scenario: stack test Given: stack with numbers 1 to 5 When: computing sum Then: sum == 5*6/2 ------------------------------------------------------------------------------- stack_test.cpp:19 ............................................................................... stack_test.cpp:25: PASSED: REQUIRE( sum == max_val*(max_val + 1)/2 ) with expansion: 15 == 15 ------------------------------------------------------------------------------- Scenario: stack test Given: stack with numbers 1 to 5 When: computing product Then: product == 5! ------------------------------------------------------------------------------- stack_test.cpp:29 ............................................................................... stack_test.cpp:35: PASSED: REQUIRE( prod == fac(max_val) ) with expansion: 120 == 120 =============================================================================== All tests passed (2 assertions in 1 test case) The code in the GIVEN section of the code is executed before each WHEN case. Arguably, this is a very nice style of formulating tests.","title":"Catch2 for C++"},{"location":"Testing/UnitTesting/catch2_cpp/#catch2-unit-testing-for-c","text":"Although you could use CUnit for testing C++ code, there are better alternatives. A very nice framework is Catch2 . You can express tests quite naturally using Catch2 so that they resemble a narrative. The framework takes a further step along that path by offering support for Behaviour Driven Development (BDD). Catch2 is a header-only library, so it is trivial to install, and has support for CMake if you're so inclined.","title":"Catch2: unit testing for C++"},{"location":"Testing/UnitTesting/catch2_cpp/#the-basics","text":"The function under test computes the factorial of a given integer, i.e., #include <stdexcept> int fac(int n) { if (n < 0) throw std::domain_error {\"argument must be positive\"}; int result {1}; for (int i = 2; i < n; ++i) result *= i; return result; }","title":"The basics"},{"location":"Testing/UnitTesting/catch2_cpp/#defining-the-tests","text":"Defining tests for Catch2 is quite straightforward using the TEST_CASE macro. It takes one or two arguments, the name of the test that has to be unique, and, optionally, a tag that is used to group tests. In the example below, \"factorials\" is the name of this test, while \"[fac]\" is the tag. #include <catch/catch.hpp> TEST_CASE(\"factorials\", \"[fac]\") { REQUIRE( fac(0) == 1 ); REQUIRE( fac(3) == 6 ); } The code block implements the test, using the REQUIRE macro relying on C++ Boolean expressions to implement the tests. Note that the Boolean expressions should be limited to comparison or a function call. Expressions that include logical operators && , || and ! are too hard to provide meaningful feedback when a test fails.","title":"Defining the tests"},{"location":"Testing/UnitTesting/catch2_cpp/#setting-up-the-tests","text":"Setting up the tests is trivial in Catch2. You simply have to define a preprocessor variable in a C++ file, i.e., #define CATCH_CONFIG_MAIN #include <catch/catch.hpp> The main function will be generated automatically. However, that will take the preprocessor/compiler a while, so it is recommended to put the lines above inn their own C++ source file. That way, the code is generated and compiled only once, and not each time you add or make a change to a test. Believe me, you will be grateful for this tip.","title":"Setting up the tests"},{"location":"Testing/UnitTesting/catch2_cpp/#building-and-running","text":"The build the tests, the compiler needs to be aware of the location of the Catch2 header files, so you have to specify the appropriate -I flag. The most convenient way is to use the single include file which is in the single_include directory of the distribution. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ test_fac.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- factorials ------------------------------------------------------------------------------- test_fac.cpp:6 ............................................................................... test_fac.cpp:8: FAILED: REQUIRE( fac(3) == 6 ) with expansion: 2 == 6 =============================================================================== test cases: 1 | 1 failed assertions: 2 | 1 passed | 1 failed The report is quite comprehensive, showing you the computed versus the expected value for the test(s) that failed, as well as summary information. In this case, the single test case failed, while of the two assertions, one failed, and one passed. The fac function needs some work.","title":"Building and running"},{"location":"Testing/UnitTesting/catch2_cpp/#more-assertions","text":"Although REQUIRE is Catch2's main work horse, a few other macros and features are very useful as well. As was mentioned when discussing best practices, it is important to test for failure, and with Catch2, the REQUIRE_THROWS_AS can be used for this purpose. In the example above, the fac function will throw a domain_error exception when its argument is strictly negative, since the factorial is only defined for positive integers. Testing for this is straightforward. #include <stdexcept> ... REQUIRE_THROWS_AS( fac(-1), std::domain_error ); ... To test on the exception's message, rather than its type, REQUIRE_THROWS can be used, e.g., ... REQUIRE_THROWS_WITH( fac(-1), \"argument must be positive\" ); ... This form is useful to test non-trivial exception messages. Finally, it is also possible to verify that an exception is thrown ( REQUIRE_THROWS ) or not ( REQUIRE_NOTHROW ). Catch2 has no dedicated macro for testing floating point equality, but the Approx class is provided for this purpose. It has two methods to set either the relative ( epsilon ) or absolute ( margin ) accuracy of the comparison. For instance, suppose that the function compute_pi , well, computes the value of pi, then it could be tested whether the result deviates from 3.14 by less than one percent as follows: REQUIRE( compute_pi() == Approx.epsilong(0.01) ); Alternatively, the following test would check whether the computed value is in the interval [3.13, 3.15]: REQUIRE( compute_pi() == Approx.magrin(0.01) ); Another quite useful macro is REQUIRE_THAT . It takes two arguments, the computed value and a matcher. To check whether a string matches a regular expression, the Catch::Matchers::Matches matcher can be used, e.g., suppose the function gen_ip4_address returns strings such as \"127.0.0. \" to represent IP4 addresses, than the following REQUIRE_THAT` macro would verify that. using Catch::Matchers::Matches; ... REQUIRE_THAT( gen_ip4_address(), Matches(R\"(^\\d{1,3}(?:\\.\\d{1,3}){3}$)\") ); ... Besides the Matches matcher, Catch2 also defines StartsWith , EndsWith , Equals and Contains for std::string . For std::vector , three matchers are defined, Contains , ContainsVector (subset) and Equals . Moreover, a generic Predicate matcher can be used to turn a lambda function (or any callable for that matter into a matcher. Note that matchers can be combined into Boolean expressions involving the operators && , || and ! . Finally, a second version for each REQUIRE macro is defined, e.g., CHECK , CHECK_THAT , etc. Unlike the REQUIRE family, execution of the test case doesn't stop when a CHECK fails.","title":"More assertions"},{"location":"Testing/UnitTesting/catch2_cpp/#fixtures","text":"Fixtures for Catch2 tests are implemented as classes. The constructor will do the set up and, if required, the destructor is responsible for the tear down. TEST_CASE_METHOD is used to define test cases. This macro takes two or three arguments. The first is the class that implements the fixture, the second is the unique name of the test case, and, optionally, the third is the tag. As a somewhat contrived example, consider a stack that is initialized, and integer values are pushed onto it, starting from 0 up to max_value . #include <stack> class VectorFixture { protected: std::stack<int> data; const int max_value {5}; public: VectorFixture() : data() { for (int i = 1; i <= max_value; ++i) data.push(i); }; }; Now tests that use this fixture can be defined as TEST_CASE_METHOD , e.g., #include <catch2/catch.hpp> TEST_CASE_METHOD(VectorFixture, \"sum\", \"[stack]\") { int sum {0}; while (!data.empty()) { sum += data.top(); data.pop(); } REQUIRE( sum == max_value*(max_value + 1)/2 ); } TEST_CASE_METHOD(VectorFixture, \"product\", \"[stack]\") { int prod {1}; while (!data.empty()) { prod *= data.top(); data.pop(); } REQUIRE( prod == fac(max_value) ); } As you can see, for each test case, the stack in the fixtures is emptied, illustrating that the fixture is set up (and teared down) for each individual test case. Alternatively, you can also define the test cases as ordinary object methods in for the fixture class, and register them as such using the METHOD_AS_TEST_CASE macro.","title":"Fixtures"},{"location":"Testing/UnitTesting/catch2_cpp/#behavior-driven-design-bdd","text":"Catch2 also supports a behavior-driven design approach to testing, and in fact, according to the library's author, this is the preferred way to handle fixtures. The tests for the factorial function can be implemented as a scenario, i.e., #include <catch2/catch.hpp> #include <stdexcept> SCENARIO( \"factorial function return values and exceptions\", \"[fac]\" ) { GIVEN( \"factorial function 'fac'\" ) { WHEN( \"argument == 0\" ) { THEN( \"fac(0) == 1\" ) { REQUIRE( fac(0) == 1 ); } } WHEN( \"argument > 0\" ) { THEN( \"fac(n) == n*fac(n-1)\" ) { for (int i = 1; i < 6; i++) REQUIRE( fac(i) == i*fac(i - 1) ); } } WHEN( \"argument < 0\" ) { THEN( \"exception thrown\" ) { REQUIRE_THROWS_AS( fac(-1), std::domain_error ); } } } } When the resulting test application is run with the -s option, it will show the following output. test_fac.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument == 0 Then: fac(0) == 1 ------------------------------------------------------------------------------- test_fac.cpp:9 ............................................................................... test_fac.cpp:10: PASSED: REQUIRE( fac(0) == 1 ) with expansion: 1 == 1 ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument > 0 Then: fac(n) == n*fac(n-1) ------------------------------------------------------------------------------- test_fac.cpp:14 ............................................................................... test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 1 == 1 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 2 == 2 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 6 == 6 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 24 == 24 test_fac.cpp:16: PASSED: REQUIRE( fac(i) == i*fac(i - 1) ) with expansion: 120 == 120 ------------------------------------------------------------------------------- Scenario: factorial function return values and exceptions Given: factorial function 'fac' When: argument < 0 Then: exception thrown ------------------------------------------------------------------------------- test_fac.cpp:20 ............................................................................... test_fac.cpp:21: PASSED: REQUIRE_THROWS_AS( fac(-1), std::domain_error ) =============================================================================== All tests passed (7 assertions in 1 test case) To illustrate how BDD simplifies creating and working with fixtures, consider the following implementation of the same tests as in the section on fixtures. #define CATCH_CONFIG_MAIN #include <catch2/catch.hpp> #include <stack> int fac(int n) { int result = 1; for (int i = 2; i <= n; ++i) result *= i; return result; } SCENARIO( \"stack test\", \"[stack]\" ) { GIVEN( \"stack with numbers 1 to 5\" ) { const int max_val {5}; std::stack<int> data; for (int i = 1; i <= max_val; ++i) data.push(i); WHEN( \"computing sum\" ) { int sum {0}; while (!data.empty()) { sum += data.top(); data.pop(); } THEN( \"sum == 5*6/2\" ) { REQUIRE( sum == max_val*(max_val + 1)/2 ); } } WHEN( \"computing product\" ) { int prod {1}; while (!data.empty()) { prod *= data.top(); data.pop(); } THEN( \"product == 5!\" ) { REQUIRE( prod == fac(max_val) ); } } } } When the test application is run with the -s flag, the following output is produced. stack_test.exe is a Catch v2.5.0 host application. Run with -? for options ------------------------------------------------------------------------------- Scenario: stack test Given: stack with numbers 1 to 5 When: computing sum Then: sum == 5*6/2 ------------------------------------------------------------------------------- stack_test.cpp:19 ............................................................................... stack_test.cpp:25: PASSED: REQUIRE( sum == max_val*(max_val + 1)/2 ) with expansion: 15 == 15 ------------------------------------------------------------------------------- Scenario: stack test Given: stack with numbers 1 to 5 When: computing product Then: product == 5! ------------------------------------------------------------------------------- stack_test.cpp:29 ............................................................................... stack_test.cpp:35: PASSED: REQUIRE( prod == fac(max_val) ) with expansion: 120 == 120 =============================================================================== All tests passed (2 assertions in 1 test case) The code in the GIVEN section of the code is executed before each WHEN case. Arguably, this is a very nice style of formulating tests.","title":"Behavior-driven design (BDD)"},{"location":"Testing/UnitTesting/cunit_c/","text":"CUnit testing for C CUnit is a very rich framework for developing unit tests for C functions.. The basics The function under test computes the factorial of a given integer, i.e., int fac(int n) { int f = 1; while (n > 1) f *= --n; return f; } Defining the tests Unit tests are functions that are registered as such, and executed by the framework. The signature of these functions is void f(void) , and their body contains at least one CUnit assertions, e.g., #include <CUnit/CUnit.h> void test_fac_0(void) { CU_ASSERT_EQUAL(fac(0), 1); } This test would verify that the computed result, i.e., fac(0) , the factorial of 0, is equal to the expected result 1. Typically, several tests would be defined to cover the paths through the fac function, e.g., void test_fac_3(void) { CU_ASSERT_EQUAL(fac(3), 6); } Setting up the tests When the tests are defined, a test application can be created. The first and last step is to initialise and clean up the test registry, i.e., #include <err.h> #include <CUnit/Basic.h> int main(void) { if (CU_initialize_registry() != CUE_SUCCESS) errx(EXIT_FAILURE, \"can't initialize test registry\"); ... CU_cleanup_registry(); return 0; } Unit tests are grouped into suites, so you need to add at least one suite to the registry once that has been initialized. A suite has a unique name, fac in the code fragment below. For now, don't worry about the second and third argument of the CU_add_suite function, a later section will discuss that. ... CU_pSuite facSuite = CU_add_suite(\"fac\", NULL, NULL); if (CU_get_error() != CUE_SUCCESS) errx(EXIT_FAILURE, \"%s\", CU_get_error_msg()); ... Now the unit test functions can be added to the test suite, i.e., ... CU_add_test(facSuite, \"fac(0)\", test_fac_0); CU_add_test(facSuite, \"fac(3)\", test_fac_3); ... The last step is to ensure that the tests are executed when the application runs. The simplest way to do this is by using the CU_basic_run_tests function. This will execute all the tests in each suite that was added to the registry. This is the complete definition of the main function. int main(void) { if (CU_initialize_registry() != CUE_SUCCESS) errx(EXIT_FAILURE, \"can't initialize test registry\"); CU_pSuite facSuite = CU_add_suite(\"fac\", NULL, NULL); if (CU_get_error() != CUE_SUCCESS) errx(EXIT_FAILURE, \"%s\", CU_get_error_msg()); CU_add_test(facSuite, \"fac(0)\", test_fac_0); CU_add_test(facSuite, \"fac(3)\", test_fac_3); CU_basic_run_tests(); CU_cleanup_registry(); return 0; } Building and running To build the test application, remember to link with the -lcunit flag and other flags or libraries required on your system (use pkg-config to determine those). When you run the test application, you will get a report like the one below CUnit - A unit testing framework for C - Version 2.1-3 http://cunit.sourceforge.net/ Suite fac, Test fac(3) had failures: 1. tests.c:17 - CU_ASSERT_EQUAL(fac(3),6) Run Summary: Type Total Ran Passed Failed Inactive suites 1 1 n/a 0 0 tests 2 2 1 1 0 asserts 2 2 1 1 n/a Elapsed time = 0.000 seconds There is one suite in the registry, and that was run, it had two tests, both were run, one passed, the other failed. In total, there wre two assertions, both ran, one passed, the other failed. The test that failed was fac(3) , clearly, the fac function requires some work. More assertions Besides the CU_ASSERT_EQUAL macro illustrated above, there is a long list of test macros available, e.g., CU_ASSERT_TRUE / CU_ASSERT_FALSE : test Boolean condition; CU_ASSERT_DOUBLE_EQUAL : test floating point equality up to a given tolerance; CU_ASSERT_NSTRING_EQUAL : test string equality; CU_ASSERT_PTR_EQUAL : test whether addresses are equal; CU_PASS / CU_FAIL : test whether code paths are taken. For each EQUAL macro, there is a corresponding NOT_EQUAL version that asserts inequality. At the risk of repeating ourselves, never use CU_ASSERT_EQUAL to compare floating point values! Although all the test macros could be expressed by the generic CU_ASSERT , e.g., CU_ASSERT_EQUAL(a, b) is logically equivalent to CU_ASSERT(a == b) it is good practice to use the most appropriate macro to formulate your test. Doing so will make your intent clear, and may yield failure messages that are more informative. Initialise and clean up In most unit testing frameworks, this is called setup and tear down. In the context of CUnit, the initialisation and cleanup function are provided to a test suite, and they are run before the first test starts, and the last test in that suite completes, respectively. The purpose of the initialisation function is to set up the stage for testing. It may for instance initialise a data structure under test, or initialise a connection to a database. The concept is often called a \"fixture\", since it ensures a consistent state when the tests are run. The clean up function ensures that all resources acquired by the initialisation function are released. So it may free the memory allocated for the data structure, or close the connection to the database. The initialisation and clean up functions take no arguments and are expected to return 0 upon successful completion. They are passed as the optional second and third argument of the CU_add_suite function. Suppose that we want to test functions that perform computations on arrays, we could create an array as a fixture. The initialisation and cleanup function could be implemented as follows. static const int size = 5; static int *array; int initialize() { array = (int *) malloc(size*sizeof(int)); if (!array) return 1; for (int i = 0; i < size; i++) array[i] = i + 1; return 0; } int cleanup() { free(array); return 0; } The tests would be defined below the declaration of the static variables so that they can access them. void test_sum() { int sum = 0; for (int i = 0; i < size; i++) sum += array[i]; CU_ASSERT_EQUAL(sum, 15); } It is of course unfortunate that global variables have to be used as fixtures, although at least their scope is limited to the file since they were declared static. The initialisation and cleanup function can now be assigned to a test suite by passing them as arguments to CU_ . ... CU_pSuite suite = CU_add_suite(\"array\", initialize, cleanup); ... Note that many unit testing frameworks allow more flexibility. They can have setup and tear down functions that are called before and after each individual test.","title":"CUnit for C"},{"location":"Testing/UnitTesting/cunit_c/#cunit-testing-for-c","text":"CUnit is a very rich framework for developing unit tests for C functions..","title":"CUnit testing for C"},{"location":"Testing/UnitTesting/cunit_c/#the-basics","text":"The function under test computes the factorial of a given integer, i.e., int fac(int n) { int f = 1; while (n > 1) f *= --n; return f; }","title":"The basics"},{"location":"Testing/UnitTesting/cunit_c/#defining-the-tests","text":"Unit tests are functions that are registered as such, and executed by the framework. The signature of these functions is void f(void) , and their body contains at least one CUnit assertions, e.g., #include <CUnit/CUnit.h> void test_fac_0(void) { CU_ASSERT_EQUAL(fac(0), 1); } This test would verify that the computed result, i.e., fac(0) , the factorial of 0, is equal to the expected result 1. Typically, several tests would be defined to cover the paths through the fac function, e.g., void test_fac_3(void) { CU_ASSERT_EQUAL(fac(3), 6); }","title":"Defining the tests"},{"location":"Testing/UnitTesting/cunit_c/#setting-up-the-tests","text":"When the tests are defined, a test application can be created. The first and last step is to initialise and clean up the test registry, i.e., #include <err.h> #include <CUnit/Basic.h> int main(void) { if (CU_initialize_registry() != CUE_SUCCESS) errx(EXIT_FAILURE, \"can't initialize test registry\"); ... CU_cleanup_registry(); return 0; } Unit tests are grouped into suites, so you need to add at least one suite to the registry once that has been initialized. A suite has a unique name, fac in the code fragment below. For now, don't worry about the second and third argument of the CU_add_suite function, a later section will discuss that. ... CU_pSuite facSuite = CU_add_suite(\"fac\", NULL, NULL); if (CU_get_error() != CUE_SUCCESS) errx(EXIT_FAILURE, \"%s\", CU_get_error_msg()); ... Now the unit test functions can be added to the test suite, i.e., ... CU_add_test(facSuite, \"fac(0)\", test_fac_0); CU_add_test(facSuite, \"fac(3)\", test_fac_3); ... The last step is to ensure that the tests are executed when the application runs. The simplest way to do this is by using the CU_basic_run_tests function. This will execute all the tests in each suite that was added to the registry. This is the complete definition of the main function. int main(void) { if (CU_initialize_registry() != CUE_SUCCESS) errx(EXIT_FAILURE, \"can't initialize test registry\"); CU_pSuite facSuite = CU_add_suite(\"fac\", NULL, NULL); if (CU_get_error() != CUE_SUCCESS) errx(EXIT_FAILURE, \"%s\", CU_get_error_msg()); CU_add_test(facSuite, \"fac(0)\", test_fac_0); CU_add_test(facSuite, \"fac(3)\", test_fac_3); CU_basic_run_tests(); CU_cleanup_registry(); return 0; }","title":"Setting up the tests"},{"location":"Testing/UnitTesting/cunit_c/#building-and-running","text":"To build the test application, remember to link with the -lcunit flag and other flags or libraries required on your system (use pkg-config to determine those). When you run the test application, you will get a report like the one below CUnit - A unit testing framework for C - Version 2.1-3 http://cunit.sourceforge.net/ Suite fac, Test fac(3) had failures: 1. tests.c:17 - CU_ASSERT_EQUAL(fac(3),6) Run Summary: Type Total Ran Passed Failed Inactive suites 1 1 n/a 0 0 tests 2 2 1 1 0 asserts 2 2 1 1 n/a Elapsed time = 0.000 seconds There is one suite in the registry, and that was run, it had two tests, both were run, one passed, the other failed. In total, there wre two assertions, both ran, one passed, the other failed. The test that failed was fac(3) , clearly, the fac function requires some work.","title":"Building and running"},{"location":"Testing/UnitTesting/cunit_c/#more-assertions","text":"Besides the CU_ASSERT_EQUAL macro illustrated above, there is a long list of test macros available, e.g., CU_ASSERT_TRUE / CU_ASSERT_FALSE : test Boolean condition; CU_ASSERT_DOUBLE_EQUAL : test floating point equality up to a given tolerance; CU_ASSERT_NSTRING_EQUAL : test string equality; CU_ASSERT_PTR_EQUAL : test whether addresses are equal; CU_PASS / CU_FAIL : test whether code paths are taken. For each EQUAL macro, there is a corresponding NOT_EQUAL version that asserts inequality. At the risk of repeating ourselves, never use CU_ASSERT_EQUAL to compare floating point values! Although all the test macros could be expressed by the generic CU_ASSERT , e.g., CU_ASSERT_EQUAL(a, b) is logically equivalent to CU_ASSERT(a == b) it is good practice to use the most appropriate macro to formulate your test. Doing so will make your intent clear, and may yield failure messages that are more informative.","title":"More assertions"},{"location":"Testing/UnitTesting/cunit_c/#initialise-and-clean-up","text":"In most unit testing frameworks, this is called setup and tear down. In the context of CUnit, the initialisation and cleanup function are provided to a test suite, and they are run before the first test starts, and the last test in that suite completes, respectively. The purpose of the initialisation function is to set up the stage for testing. It may for instance initialise a data structure under test, or initialise a connection to a database. The concept is often called a \"fixture\", since it ensures a consistent state when the tests are run. The clean up function ensures that all resources acquired by the initialisation function are released. So it may free the memory allocated for the data structure, or close the connection to the database. The initialisation and clean up functions take no arguments and are expected to return 0 upon successful completion. They are passed as the optional second and third argument of the CU_add_suite function. Suppose that we want to test functions that perform computations on arrays, we could create an array as a fixture. The initialisation and cleanup function could be implemented as follows. static const int size = 5; static int *array; int initialize() { array = (int *) malloc(size*sizeof(int)); if (!array) return 1; for (int i = 0; i < size; i++) array[i] = i + 1; return 0; } int cleanup() { free(array); return 0; } The tests would be defined below the declaration of the static variables so that they can access them. void test_sum() { int sum = 0; for (int i = 0; i < size; i++) sum += array[i]; CU_ASSERT_EQUAL(sum, 15); } It is of course unfortunate that global variables have to be used as fixtures, although at least their scope is limited to the file since they were declared static. The initialisation and cleanup function can now be assigned to a test suite by passing them as arguments to CU_ . ... CU_pSuite suite = CU_add_suite(\"array\", initialize, cleanup); ... Note that many unit testing frameworks allow more flexibility. They can have setup and tear down functions that are called before and after each individual test.","title":"Initialise and clean up"},{"location":"Testing/UnitTesting/pfunit_fortran/","text":"pFUnit additional features pFUnit is a very rich framework for developing unit tests. The screencast presented some of its features, but it is useful to know some more. Code to test Consider the following module defined in fac_mod.f90 under test. module fac_mod use, intrinsic :: iso_fortran_env, only : i32 => INT32 implicit none public :: fac contains integer(kind=i32) function fac(n) implicit none integer(kind=i32), intent(in) :: n integer(kind=i32) :: r = 1_i32 integer :: i do i = 2, n r = r*i end do fac = r end function fac end module fac_mod Unit tests Unit tests for this module reside in a file with extension .pf , e.g., fac_tests.pf , and consist of Fortran code with annotations and macros that are preprocessed by pFUnit. The test below verifies that the factorial of 0 is 1. @test subroutine test_fac_0() use fac_mod use pfunit_mod implicit none @assertEqual(1, fac(0)) end subroutine test_fac_0 A second test in the same file will test whether the factorial of 5 is 120, i.e., @test subroutine test_fac_5() use fac_mod use pfunit_mod implicit none @assertEqual(120, fac(5)) end subroutine test_fac_5 Scaffolding The actual testing program is provided by the pFUnit framework. To ensure that the tests will be included, an include file testSuites.inc is created in which you specify the test suite(s) to take into account. ADD_TEST_SUITE(fac_tests_suite) The .pf files has to be preprocessed first, i.e., tests: tests.exe ifneq ($(BASEMK_INCLUDED),YES) include $(PFUNIT)/include/base.mk endif FC = gfortran FFLAGS += -g -I$(PFUNIT)/mod -I. LIBS = $(PFUNIT)/lib/libpfunit$(LIB_EXT) SRCS = $(wildcard *.pf) OBJS = $(SRCS:.pf=.o) APPL_OBJS = fac_mod.o tests.exe: $(APPL_OBJS) $(OBJS) $(FC) $(FFLAGS) $(FPPFLAGS) -o $@ \\ $(PFUNIT)/include/driver.F90 $(OBJS) $(APPL_OBJS) $(LIBS) testSuites.inc: $(SRCS) %.F90: %.pf $(PFUNIT)/bin/pFUnitParser.py $< $@ fac_mod.o: fac_mod.f90 $(FC) $(FFLAGS) -c $< %.o: %.F90 $(FC) -c $(FFLAGS) $(FPPFLAGS) $< clean: $(RM) *.exe The .pf files will be preprocessed by pFUnitParser.py into .F90 files. Note the .F90 extension which ensure that the Fortran preprocessor will be called prior to compilation. The executable is built using the provided driver.F90 that will use the testSuites.inc` file to determine the test suites to run. To build the tests, the PFUNIT environment variable should be set to the path where pFUnit is installed, and make can be run, i.e., $ export PFUNIT=/path/to/pfunit $ make Running tests When the executable is successfully built, you can run the tests by executing it. $ ./tests.exe The test for the factorial of 5 will fail since the implementation contains a bug. More assertions Besides the @assertEqual macro illustrated above, there is a long list of test macros available, e.g., @assertTrue / @assertFalse : test a Boolean condition; @assertLessThan /.../ @assertGreaterThanOrEqual : test numerical inequalities; @assertAny / @assertAll : test values of logical arrays; @assertSameShape : test shape of (multi-dimensional) arrays; @assertIsNaN / @assertIsFinite : test whether a floating point value is NaN or infinite; @assertIsAssociated : test whether a pointer is associated with a target; @assertFailure : test whether an inappropriate code path is taken. There are negations for some assertions, e.g., @assertNotEqual and @assertNotAssociated . The @assertEqual macro is overloaded, it can test for equality of integer, real and complex numbers, logical values and strings. For assertions on real values, an optional argument can and should be supplied, the tolerance for comparison. At the risk of repeating ourselves, you should never test for exact equality of real numbers. Although almost all the test macros could be expressed by the generic @assertTrue , e.g., @assertNotEqual(a, b) is logically equivalent to @assertTrue(a /= b) it is good practice to use the most appropriate macro to formulate your test. Doing so will make your intent clear, and may yield failure messages that are more informative. Setup and tear down In order to ensure that each test runs in a well-prepared environment, you can use fixtures. These are artefacts that are set up before a test runs, and that are teared down (cleaned up) after the test finishes. A pFUnit definition file can define these setup and teardown subroutine by using the @before and @after annotations respectively. The setup and tear down procedures take no arguments. The purpose of the setup procedure (annotated with @before ) is to, well, set up the stage for testing. It may for instance initialise a data structure under test, or initialise a network connection. The tear down procedure (annotated with @after ) ensures that all resources acquired by the setup procedure are released. So it may deallocate the memory of the data structure, or close the connection. Although this approach is straightforward, it is not ideal since the scope of the fixture objects would have to be global or defined in a separate module. Hence it is more elegant to create a new class derived from pFUnit's TestCase base class. In the new class, the base class methods setUp and tearDown can be overridden to initialise and clean up the fixtures. The tests are also implemented as methods of the new class. Suppose that we want to test procedures that perform computations on arrays, we could create an array as a fixture. It would be a field of a user defined type derived from TestCase , i.e., module tests use pfunit_mod implicit none @testcase type, extends(TestCase) :: tests_type integer :: n = 5 integer, allocatable, dimension(:) :: data_array contains procedure :: setUp procedure :: tearDown end type tests_type contains ... end module tests The definition of the setUp and tearDown procedures are contained in the module. subroutine setUp(this) implicit none class(tests_type), intent(inout) :: this integer :: i allocate(this%data_array(this%n)) do i = 1, this%n this%data_array(i) = i end do end subroutine setUp subroutine tearDown(this) implicit none class(tests_type), intent(inout) :: this deallocate(this%data_array) end subroutine tearDown For the setUp and tearDown methods, no annotation is required since they override TestCase methods. The tests themselves are also contained in the module as methods of the tests_type class, e.g., @test subroutine test_sum(this) use pfunit_mod implicit none class(tests_type), intent(inout) :: this integer :: i, total total = 0 do i = 1, size(this%data_array) total = total + this%data_array(i) end do @assertEqual(15, total) end subroutine test_sum In the test subroutine, you need to use the pfunit_mod module since you will be using assertions such as @assertEqual . Note that many unit testing frameworks allow more flexibility. They can have setup and tear down functions that are called before the first test in a suite starts, and after the last test in a suite ends.","title":"pfUnit for Fortran"},{"location":"Testing/UnitTesting/pfunit_fortran/#pfunit-additional-features","text":"pFUnit is a very rich framework for developing unit tests. The screencast presented some of its features, but it is useful to know some more.","title":"pFUnit additional features"},{"location":"Testing/UnitTesting/pfunit_fortran/#code-to-test","text":"Consider the following module defined in fac_mod.f90 under test. module fac_mod use, intrinsic :: iso_fortran_env, only : i32 => INT32 implicit none public :: fac contains integer(kind=i32) function fac(n) implicit none integer(kind=i32), intent(in) :: n integer(kind=i32) :: r = 1_i32 integer :: i do i = 2, n r = r*i end do fac = r end function fac end module fac_mod","title":"Code to test"},{"location":"Testing/UnitTesting/pfunit_fortran/#unit-tests","text":"Unit tests for this module reside in a file with extension .pf , e.g., fac_tests.pf , and consist of Fortran code with annotations and macros that are preprocessed by pFUnit. The test below verifies that the factorial of 0 is 1. @test subroutine test_fac_0() use fac_mod use pfunit_mod implicit none @assertEqual(1, fac(0)) end subroutine test_fac_0 A second test in the same file will test whether the factorial of 5 is 120, i.e., @test subroutine test_fac_5() use fac_mod use pfunit_mod implicit none @assertEqual(120, fac(5)) end subroutine test_fac_5","title":"Unit tests"},{"location":"Testing/UnitTesting/pfunit_fortran/#scaffolding","text":"The actual testing program is provided by the pFUnit framework. To ensure that the tests will be included, an include file testSuites.inc is created in which you specify the test suite(s) to take into account. ADD_TEST_SUITE(fac_tests_suite) The .pf files has to be preprocessed first, i.e., tests: tests.exe ifneq ($(BASEMK_INCLUDED),YES) include $(PFUNIT)/include/base.mk endif FC = gfortran FFLAGS += -g -I$(PFUNIT)/mod -I. LIBS = $(PFUNIT)/lib/libpfunit$(LIB_EXT) SRCS = $(wildcard *.pf) OBJS = $(SRCS:.pf=.o) APPL_OBJS = fac_mod.o tests.exe: $(APPL_OBJS) $(OBJS) $(FC) $(FFLAGS) $(FPPFLAGS) -o $@ \\ $(PFUNIT)/include/driver.F90 $(OBJS) $(APPL_OBJS) $(LIBS) testSuites.inc: $(SRCS) %.F90: %.pf $(PFUNIT)/bin/pFUnitParser.py $< $@ fac_mod.o: fac_mod.f90 $(FC) $(FFLAGS) -c $< %.o: %.F90 $(FC) -c $(FFLAGS) $(FPPFLAGS) $< clean: $(RM) *.exe The .pf files will be preprocessed by pFUnitParser.py into .F90 files. Note the .F90 extension which ensure that the Fortran preprocessor will be called prior to compilation. The executable is built using the provided driver.F90 that will use the testSuites.inc` file to determine the test suites to run. To build the tests, the PFUNIT environment variable should be set to the path where pFUnit is installed, and make can be run, i.e., $ export PFUNIT=/path/to/pfunit $ make","title":"Scaffolding"},{"location":"Testing/UnitTesting/pfunit_fortran/#running-tests","text":"When the executable is successfully built, you can run the tests by executing it. $ ./tests.exe The test for the factorial of 5 will fail since the implementation contains a bug.","title":"Running tests"},{"location":"Testing/UnitTesting/pfunit_fortran/#more-assertions","text":"Besides the @assertEqual macro illustrated above, there is a long list of test macros available, e.g., @assertTrue / @assertFalse : test a Boolean condition; @assertLessThan /.../ @assertGreaterThanOrEqual : test numerical inequalities; @assertAny / @assertAll : test values of logical arrays; @assertSameShape : test shape of (multi-dimensional) arrays; @assertIsNaN / @assertIsFinite : test whether a floating point value is NaN or infinite; @assertIsAssociated : test whether a pointer is associated with a target; @assertFailure : test whether an inappropriate code path is taken. There are negations for some assertions, e.g., @assertNotEqual and @assertNotAssociated . The @assertEqual macro is overloaded, it can test for equality of integer, real and complex numbers, logical values and strings. For assertions on real values, an optional argument can and should be supplied, the tolerance for comparison. At the risk of repeating ourselves, you should never test for exact equality of real numbers. Although almost all the test macros could be expressed by the generic @assertTrue , e.g., @assertNotEqual(a, b) is logically equivalent to @assertTrue(a /= b) it is good practice to use the most appropriate macro to formulate your test. Doing so will make your intent clear, and may yield failure messages that are more informative.","title":"More assertions"},{"location":"Testing/UnitTesting/pfunit_fortran/#setup-and-tear-down","text":"In order to ensure that each test runs in a well-prepared environment, you can use fixtures. These are artefacts that are set up before a test runs, and that are teared down (cleaned up) after the test finishes. A pFUnit definition file can define these setup and teardown subroutine by using the @before and @after annotations respectively. The setup and tear down procedures take no arguments. The purpose of the setup procedure (annotated with @before ) is to, well, set up the stage for testing. It may for instance initialise a data structure under test, or initialise a network connection. The tear down procedure (annotated with @after ) ensures that all resources acquired by the setup procedure are released. So it may deallocate the memory of the data structure, or close the connection. Although this approach is straightforward, it is not ideal since the scope of the fixture objects would have to be global or defined in a separate module. Hence it is more elegant to create a new class derived from pFUnit's TestCase base class. In the new class, the base class methods setUp and tearDown can be overridden to initialise and clean up the fixtures. The tests are also implemented as methods of the new class. Suppose that we want to test procedures that perform computations on arrays, we could create an array as a fixture. It would be a field of a user defined type derived from TestCase , i.e., module tests use pfunit_mod implicit none @testcase type, extends(TestCase) :: tests_type integer :: n = 5 integer, allocatable, dimension(:) :: data_array contains procedure :: setUp procedure :: tearDown end type tests_type contains ... end module tests The definition of the setUp and tearDown procedures are contained in the module. subroutine setUp(this) implicit none class(tests_type), intent(inout) :: this integer :: i allocate(this%data_array(this%n)) do i = 1, this%n this%data_array(i) = i end do end subroutine setUp subroutine tearDown(this) implicit none class(tests_type), intent(inout) :: this deallocate(this%data_array) end subroutine tearDown For the setUp and tearDown methods, no annotation is required since they override TestCase methods. The tests themselves are also contained in the module as methods of the tests_type class, e.g., @test subroutine test_sum(this) use pfunit_mod implicit none class(tests_type), intent(inout) :: this integer :: i, total total = 0 do i = 1, size(this%data_array) total = total + this%data_array(i) end do @assertEqual(15, total) end subroutine test_sum In the test subroutine, you need to use the pfunit_mod module since you will be using assertions such as @assertEqual . Note that many unit testing frameworks allow more flexibility. They can have setup and tear down functions that are called before the first test in a suite starts, and after the last test in a suite ends.","title":"Setup and tear down"},{"location":"Testing/UnitTesting/unit_testing_best_practices/","text":"Unit testing best practices Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularised by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behaviour of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file. What to test? You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behaviour. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behaviour of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed. How to test? Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarise the characteristics of a good unit test: it is specific: failure of a test localises a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialised, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artefacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organised into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed. We will discuss a unit testing framework for Fortran and one for C , and show how they can be used. As mentioned, many frameworks are available, but once you understand one, it is very easy to start using another since they all operate along similar principles.","title":"Best practices"},{"location":"Testing/UnitTesting/unit_testing_best_practices/#unit-testing-best-practices","text":"Very often code defects are introduced while the code evolves to implement new features or even to fix emerging problems. Without proper testing these new bugs may go unnoticed for a while, even until after the release of a new version of a software project. Hence it is good practice to have a set of tests at hand for regression testing, i.e., testing whether what used to work correctly, still does. It is a very important aspect of a software project with a non-trivial life-cycle. Testing code by running an application and visually inspecting the results is very error-prone. It is all too easy to miss a problem when the output is fairly long, as would be the case for most software projects. Hence a different approach should be taken. Another consideration when planning tests is that they should be easy to run and should complete in a reasonably short time. Such tests offer no excuses to delay running them or to not run them frequently. If those tests can be part of the software build process, and hence automated, that is an additional benefit. From the output of a test failure, it should be very clear what the issue is. If we have tests that check a single concern, or a single use case, this will help to pinpoint the failure's causes much more easily. Fortunately, all these criteria are met by the unit testing paradigm, popularised by the extreme programming developed by Kent Beck and Ron Jeffries around 1998. Quoting from \"The art of unit testing\", Kent Beck defines a unit test as A unit test is an automated piece of code that invokes a unit of work in the system and then checks a single assumption about the behaviour of that unit of work. Many implementations of unit testing frameworks exist, often multiple for the same programming language. They provide a framework that takes care of the scaffolding, i.e., the main function, to run the test, the formatting of the tests output and messages, and summary reporting. As a programmer, you simply implement functions that test for specific features of your software, usually at the level of individual functions. The framework provides functions to compare expected to computed values, the result of Boolean expressions, whether exceptions have been thrown (if expected), and so on. Once the test code is written, building and running it is very easy, and can be added as a target to a make file.","title":"Unit testing best practices"},{"location":"Testing/UnitTesting/unit_testing_best_practices/#what-to-test","text":"You will typically write unit tests that compare the return value of a function to an expected value. Good tests not only check for normal cases, but also for edge or corner cases. By way of example, consider the factorial function. One would of course write tests to verify that its return value for 3 is 6, or for 5 is 120, but the cases where it is easy to make mistakes should also be tested, in this example the input values 0 and 1 (edge cases). If the code under tests has branches, i.e., conditional statements, we should make sure that there is at least one test for each branch. The same applies to iteration statements when the number of iterations is computed, there should be tests in case no iterations have to be executed, a single iteration or multiple iterations. Another important aspect to test for is whether a function throws an exception or generates an error when it is supposed to, since this is the function's declared behaviour. For the factorial example, we should verify that an error occurs when you pass to it a strictly negative argument value. Apart from providing some confidence that changes don't break our software, writing tests alongside code will actually prevent bugs, since you should really think about the behaviour of your code when writing tests. It is quite probable that while doing so, you will catch bugs without even executing the unit tests. This idea is taken to the extreme in Test Driven Development (TDD). A requirement is translated into a number of tests, and only when these are implemented, the actual functions are developed. This approach is kind of satisfying: at first all tests fail, since there is a trivial implementation only. As the development progresses, and the implementation gradually nears completion, more and more tests succeed.","title":"What to test?"},{"location":"Testing/UnitTesting/unit_testing_best_practices/#how-to-test","text":"Note that unit tests should be simple, i.e., a unit test check one particular aspect, so that failure is easy to map to its cause. This means that functions implementing unit tests are typically very short, but that we may have quite many of them. Another very important issue to keep in mind is that tests should be independent. You should be able to run them in any order without altering the results. This implies that unit tests have to be free of side effects. To summarise the characteristics of a good unit test: it is specific: failure of a test localises a defect in a small fragment of code; it is orthogonal to other unit tests: unit tests should be independent of one another. The collection of all tests for your software project should be complete. There should be tests for all functions, but also for all code paths through your code. An aspect that is often forgotten is that you should also test for expected failure. Is an exception actually thrown, or does the exit status of a function reflect a problem? When implementing a new feature or making a change, you should of course develop tests specific to that addition or modification. However, it is not enough to simply run those new tests. Even if they fail, your code might still be broken, since an addition or a modification might introduce a bug for existing code. This situation is called a regression, since the code was correct, but due to changes, it no longer is. Hence it is important to run all tests, to ensure that there are no regressions. This practice is referred to as regression testing. If unit testing is done well, regression testing is always performed. Of course, many tests require a context to run in. Variables and data structures must have been initialised, a file should be present, and so on. Unit testing frameworks provide setup and teardown functions that can be run before and after each individual test respectively to set, or clean up the stage. These temporary artefacts are often referred to as fixtures. Unit testing frameworks typically also provide the means to group tests into so called suites, and run setup and teardown functions before running the first and after running the last test of that suite. Units tests for large projects are typically organised into multiple suites that deal with specific subsets of the code or functionality. Below you can see when various setup and teardown functions are called when test suites are executed. We will discuss a unit testing framework for Fortran and one for C , and show how they can be used. As mentioned, many frameworks are available, but once you understand one, it is very easy to start using another since they all operate along similar principles.","title":"How to test?"},{"location":"Testing/UnitTesting/unit_testing_intro/","text":"Introduction to unit testing Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, or Catch2 for C++ will take care of the \"bookkeeping\" and ensure that running tests is effortless. In this section, we will discuss how to write effective unit tests using these frameworks, and how to integrate them into your development process.","title":"Introduction"},{"location":"Testing/UnitTesting/unit_testing_intro/#introduction-to-unit-testing","text":"Testing is obviously a way to ensure that at least part of the functionality behaves as expected. However, good tests can provide more than that, they can help ensure that changes to the code base don't introduce defects. A code base evolves dynamically, potentially over a long period of time. Adding new features to software is typically quite error prone, and might inadvertently break some use cases. In order to minimize this risk should have a sizable collection of tests available that check whether results are as expected, and be able to run those easily and frequently as part of your development cycle. Unit tests are an excellent approach. They consist of many small fragments of code that each test very specific aspects of the functionality of a library. Using frameworks such as pFUnit for Fortran, CUnit for C, or Catch2 for C++ will take care of the \"bookkeeping\" and ensure that running tests is effortless. In this section, we will discuss how to write effective unit tests using these frameworks, and how to integrate them into your development process.","title":"Introduction to unit testing"}]}